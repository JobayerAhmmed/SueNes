#+TITLE: Dataset Design Doc
* TODOs

** TODO test tf.data.Dataset shuffle performance for tfrec data
** TODO integrate the sentence embedding into the model?
** TODO integrate the negative sampling into the model?
** TODO integrate the mutation into the model?
** TODO integrate the negative sampling into the batch generation?

* TODO sequential embedding data serialization
with index file probably

The old way is to use dictionary as data structure for storing the
data files, including stories, mutations, and sentence embedding
results. The hash of the story (the filename) is used as the key. 
1. It is easy for referencing the same stories
2. Easy to track which stories are processed. This is very useful when
   my machine cannot process everything at once.
3. The order of the stories need not be matched among stories,
   mutations, and different embedding schema. In case some stories
   cannot be processed, this is very helpful.


However, the downside is also obvious.
1. I have to load everything to know what are the processed stories
2. Since it is dict, I cannot really make the serialization serial.

I need to use a list for this, and use TFRecord to load the
files. Hopefully this method will read the data on-demand, and close
it on-time so that I don't enter memory issues. The dataset is huge.

Thus I need to pay special attention to the order of the stories to be
exactly the same.

** Format of TFREC
Each file would have the following structure:
story:
#+BEGIN_EXAMPLE
'key': _bytes_feature(key.encode('utf-8')),
'article': _bytes_feature(article.encode('utf-8')),
'summary': _bytes_feature(summary.encode('utf-8'))
#+END_EXAMPLE

where each field:
- key: bytes string
- article: bytes string
- summary: bytes string

sentence embedding:
#+BEGIN_EXAMPLE
'key': _bytes_feature(key),
'article': _bytes_feature(pickle.dumps(article)),
'summary': _bytes_feature(pickle.dumps(summary))
#+END_EXAMPLE

where each article and summary is:
- article: pickle.dumps(numpy of shape (#sent, 512/4096))
- article: pickle.dumps(numpy of shape (#sent, 512/4096))


** Story shuffle and splitting

The first step is to read the stories, shuffle them, and split them
into training, testing, validating. It must be done first, because I
don't want any potential overlapping impact among the training and
testing set.

#+BEGIN_EXAMPLE
proto/story/train/1.tfrec
proto/story/validate/1.tfrec
proto/story/test/1.tfrec
#+END_EXAMPLE

(HEBI: TODO) I will probably need to make sure the shuffle API of
tf.data.Dataset works for tfrec data. But this is not that important,
as the stories can be hold in memory, and I only need to shuffle the
stories.

** Sentence Embedding

File System Hierarchy. I would really want to maintain the file
correspondence of splits. I'm testing 1,000 stories per file here, but
eventually I will use 10,000.

Note: InferSent embedding is significantly larger (10x).

#+BEGIN_EXAMPLE
proto/USE/train/1.tfrec
proto/USE-Large/validate/1.tfrec
proto/InferSent/test/1.tfrec
#+END_EXAMPLE

** negative sampling

I basically have three approaches to generate negative samples:
1. pre-generate all negative samples. This is also the previous
   approach. However this approach has significant drawbacks:
   - It is not scalable.
   - The negative samples are fixed across epochs. This is not
     general enough.
2. generate negative samples when providing the batch. This is easy to
   implement, and if the entire dataset is shuffled every epochs, it
   can achieve pretty good generalization.
3. In the model, or in the loss function. This is what
   =tf.nn.nce_loss= is doing for word2vec.  However, =tf.nn.nce_loss=
   is not suitable either, it is special for processing word. As far
   as I can see, it expects the words to be indexed, and a vocabulary
   size (say 50000) is give, and sampling is done in the range of
   [0,50000]. This cannot be used for sentence.

** TODO Mutation
This is tricky. I would mutate the article and summary on text
level. After that, I would need to ...



mutation.pickle (I'm going to drop this because I need to perform
separate sentence embedding for it.)

| key          | add          | add label     | delete & label | replace & label |
|--------------+--------------+---------------+----------------+-----------------|
| 0001d1afc246 | [10] of text | [10] of float | [10]           | [10]            |
| ...          |              |               |                |                 |

negative.pickle (I probably don't need this at all.)

| key | neg |
|-----+-----|
|     |     |

** Data Consuming

1. load data depending on the task
   - story (100,000)
   - USE/Large embedding
   - InferSent embedding
   Each contains three fields:
   - key
   - article
   - summary
2. shuffle the entire dataset for each epoch (HEBI: test the performance)
2. fetch 10 batches (100 each):
   - pair each batch with other 9 batches as negative samples (10x9)
   - plus each batch, we got (10x9+10) batch size data
   - concatenate together as one batch
3. train on batch, and continue



* Email

** Experiment Design

First and foremost, I'm adding the following three comparisons:
1. with ROUGE score as baseline
2. with human judgement
3. with [1], which is kind of the manual feature engineering counterpart
of our approach

We will use DUC/TAC data for it, as it has ground truth (human judgement label).

However, there are two potential problems of using DUC and TAC data:

1. The DUC/TA tasks are for extractive summarization, which works well
with ROUGE. There seems to be no such human data for extractive
summarization. Conducting human study ourselves would seem to be too
expensive for now.

2. This dataset is much smaller (thousands of samples each year), because
the summarizations are manually assessed. This may not be sufficient for
a deep learning based training.

To handle the small data set size problem, there's actually an
interesting transfer-learning experiment: training on CNN/DM and NYT
dataset and fine-tune the model on DUC/TAC.

Finally, in terms of other datasets, previous experiment used 30000
stories from CNN/DM. That's 1/10 of the total data. I can use all of
them. I also have New York time corpus in hand, thus adding that as
well.

I would prioritize the experiments as follows, with descending importance:
1. comparison with ROUGE, Human Judgement, and [1]
2. transfer-learning experiment
3. All CNN/DM data and NYT corpus
4. human survey for collecting abstractive summarization assessment data

Let me know if you have any suggestions.


[1] Louis, Annie, and Ani Nenkova. "Automatically assessing machine
summary content without a gold standard." Computational Linguistics 39.2
(2013): 267-300.

** Directly use embedding coverage

I think the reviewers' comments and your suggestion of improvements
makes a lot of sense to me.

But at another direction, I am also very interested in if we can also
use some unsupervised method to measure the summary quality. Say, the
goal of a summary is to preserve the semantic information from a
document. Can we use the sentence embedding from document and sentence
embeddings from abstract, to measure the semantic coverage. (PS. ROUGE
is basiclly a word coverage)

If we can propose two approaches, one supervised method and one
unsupervised, I think the story is complete. what do you think?

On Sat, Feb 23, 2019 at 2:25 AM yinfei yang <yangyin7@gmail.com> wrote: 

 Can we use the sentence embedding from document and sentence
 embeddings from abstract, to measure the semantic
 coverage. (PS. ROUGE is basiclly a word coverage)

Isn't this what we are doing in the paper now? And to measure the
quality (in terms of semantic coverage) of the summary, we use the
ratio of mutation and negative sampling.

I meant to use the unsupervised approach, without any training. 

I remember our current approach includes a training procedure, do we ?
I may have a wrong memory.

Yes, we do have a training procedure. Current approach basically uses
sentence embedding as pretraining, and train a discriminative model on
top.

That's a good idea. I'll do an unsupervised coverage test on the
embedding directly.

