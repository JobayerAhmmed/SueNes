#+TITLE: Anti Rouge

* Setup and running
** Datasets

Get the [[https://cs.nyu.edu/~kcho/DMQA/][CNN/DM benchmark]]. Unzip
and you will get the =cnn= and =dm= directories containing =stories=
folder. Each story is =<hash>.story=, and each contains the article
and several "@highlight xxx" lines as summaries.

** Python packages
Install:
- matplotlib (tkinter)
- numpy
- =tensorflow-gpu=, =tensorflow-hub=
- pytorch, torchvision
- nltk and =nltk.download('punkt')= (used by InferSent)

** Stanford CoreNLP (used for tokenization)
http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip
- put the jar file to =~/bin=. Set classpath 

#+BEGIN_EXAMPLE
export CLASSPATH=/home/hebi/bin/stanford-corenlp-3.9.2.jar
#+END_EXAMPLE

check whether CoreNLP install correctly:
#+BEGIN_EXAMPLE
echo "Please tokenize this text." | java edu.stanford.nlp.process.PTBTokenizer
#+END_EXAMPLE

** Tokenize the corpus

The data preprocessing is partly adapted from
https://github.com/abisee/cnn-dailymail. Note that this repo contains
url_lists, each can be hashed to produce the hashed story file names
below. I might use this for a better reference.

Tokenize the stories. The command:

#+BEGIN_EXAMPLE
java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines mapping.txt
#+END_EXAMPLE

The =mapping.txt= shall be a list of 

#+BEGIN_EXAMPLE
input-file output-file
input-file output-file
input-file output-file
...
#+END_EXAMPLE

Use =tokenize.py= to tokenize the corpus.

** preprocessing the data
From now on, all the configurations, including data file paths, model
hyperparameters are set in =config.py=. I don't like to use command
line to pass parameters.

The story files contains article and summaries. The first step is to
split the stories into story and summaries. I'm going to use a pickle
binary to for that instead of plain text. Use
=preprocess_story_pickle()= in =preprocess.py= for it. This produce
=story.pickle=.

The next step is to generate negative samples. Two approaches:
- =preprocess_word_mutated()=: this produces =word-mutated.pickle=
- =preprocess_negative_sampling()=: this produces
  =negative-sampling.pickle=

** Embedding layer
Word embedding is done inside the model, using an embedding layer
loaded with pretrained matrix. It is essentially a lookup table.

[[https://nlp.stanford.edu/projects/glove/][Glove website]] have the
file at http://nlp.stanford.edu/data/glove.6B.zip.


* TodoList
** Data sets with ground truth
*** NYC annotated corpus: https://catalog.ldc.upenn.edu/LDC2008T19
*** DUC 01/02
** Final
*** TODO LSTM 50 problem
*** TODO InferSent 30000 data
*** TODO early stopping vs not
Now, early stopping seems to give worse results. That is because when
loss stops increasing, accuracy is still increasing. So may be I can
try early stop on accuracy? Or both?
*** DONE plot CNN article summary sent number
    CLOSED: [2018-12-09 Sun 02:28]
** TODO 
** Experiment
*** TODO use all CNN data
*** TODO use DM data
*** TODO [#B] use the model learned from CNN data to test on DM data (transfer)
*** DONE [#A] use fake summary and difference as loss function, instead of mutation and score
    CLOSED: [2018-12-09 Sun 02:28]
*** DONE [#A] use only mutated summary, without article
    CLOSED: [2018-11-30 Fri 15:07]
*** DONE when generating mutation, do not change sentence separator
    CLOSED: [2018-12-09 Sun 02:28]
*** DONE adjust hyper parameters and network architecture
    CLOSED: [2018-12-09 Sun 02:28]
*** DONE try LSTM and attention, MLP instead of current CNN
    CLOSED: [2018-12-09 Sun 02:28]

** New Experiment

All with 4 settings:
- UAE
- UAE-Large
- Glove
- Glove-SO
*** DONE FIXME word mutate avoid sepator
    CLOSED: [2018-12-09 Sun 02:29]
*** DONE [#A] sigmoid on regression problem
    CLOSED: [2018-12-09 Sun 02:29]
*** DONE [#A] classification, use hinge loss
    CLOSED: [2018-12-09 Sun 02:29]
*** DONE LSTM model train more epochs
    CLOSED: [2018-12-09 Sun 02:29]
*** TODO sentence-level mutate

*** DONE mutation with deletion only
    CLOSED: [2018-12-09 Sun 02:31]
good
*** DONE negative sampling with CNN/DM
    CLOSED: [2018-12-09 Sun 02:31]
N/A

*** TODO negative sampling with NYT
*** TODO sentence level mutation with NYT

*** DONE 2nd negative sampling
    CLOSED: [2018-12-09 Sun 02:31]
For reference summary, shuffle the order of words, so that the
sentence don't make sense. Expect the word embedding (glove) based
model to have no impact, but sentence embedding method should observe
a major drop, since the sentence does not make sense.

*** DONE softmax instead of sigmoid
    CLOSED: [2018-12-05 Wed 12:41]
*** DONE negative sampling difference loss function
    CLOSED: [2018-12-09 Sun 02:27]
*** DONE validation and test dataset
    CLOSED: [2018-12-09 Sun 02:27]
*** DONE regularizers (L1, L2, dropout)
    CLOSED: [2018-12-09 Sun 02:27]
*** DONE USE-Large
    CLOSED: [2018-12-04 Tue 21:03]
*** DONE CNN architecture change
    CLOSED: [2018-12-04 Tue 21:58]
- smaller number of CNN
- dropout
- conv2d
*** TODO separate architecture

*** DONE automatic early stop keras
    CLOSED: [2018-12-09 Sun 02:28]

** Paper writing
*** DONE write method
    CLOSED: [2018-12-09 Sun 02:31]
*** DONE figures
    CLOSED: [2018-12-09 Sun 02:31]
*** DONE plot results
    CLOSED: [2018-12-09 Sun 02:31]

* Results

** 5000 articles, 1 fake samples, total 10000 data points

Negative Sampling (Accuracy):

|      | Glove | USE-DAN | USE-Transformer | InferSent |
|------+-------+---------+-----------------+-----------|
| FC   | 0.551 |   0.803 |           0.892 |     0.677 |
| CNN  | 0.701 |   0.789 |           0.844 |     0.689 |
| LSTM |   0.5 |   0.781 |           0.801 |     0.718 |

Mutation (PCC): add/delete/replace

|      | Glove          | USE-DAN        | USE-Transformer | InferSent      |
|------+----------------+----------------+-----------------+----------------|
| FC   | 0.42/0.88/0.75 | 0.85/0.75/0.86 | 0.91/0.84/0.93  | 0.76/0.91/0.84 |
| CNN  | 0.42/0.25/0.49 | 0.88/0.75/0.92 | 0.92/0.87/0.95  | 0.81/0.95/0.89 |
| LSTM | 0.02/0.5/0.5   | 0.88/0.70/0.92 | 0.94/0.88/0.95  | 0.82/0.96/0.89 |


** 30000 articles, 1 fake samples

Negative Sampling (Accuracy):

|           | Glove | USE-DAN | USE-Trans | InferSent |
|-----------+-------+---------+-----------+-----------|
| FC-only   |  80.6 |    88.2 |      92.8 | N/A       |
| CNN       |  73.0 |    85.7 |      89.5 | N/A       |
| LSTM      |  50.0 |    89.0 |      92.6 | N/A       |
|-----------+-------+---------+-----------+-----------|
| FC-only   |  70.2 |    84.1 |      92.0 | 12/9      |
| CNN       |  72.5 |    86.8 |      91.7 |           |
| LSTM      |  50.0 |    85.2 |      87.8 |           |
|-----------+-------+---------+-----------+-----------|
| FC-only   |  62.0 |    83.2 |      93.5 | 96.2      |
| CNN       |  73.0 |    79.8 |      91.9 | 93.7      |
| CNN-patch |  72.2 |    83.8 |      90.6 | 91.1      |
| LSTM      |  48.9 |    85.0 |      88.2 | 95.6      |

Mutation (PCC): add/delete/replace

|           | Glove              | USE-DAN            | USE-Transformer    | InferSent          | Comment                   |
|-----------+--------------------+--------------------+--------------------+--------------------+---------------------------|
| FC        | 81.1 / 89.1 / 88.5 | 86.9 / 79.9 / 90.8 | 92.7 / 89.3 / 95.2 | N/A                |                           |
| CNN       | 82.9 / 44.0 / 84.5 | 88.1 / 82.1 / 92.0 | 93.7 / 90.5 / 95.6 | N/A                |                           |
| LSTM      | 50.0 / 50.0 / 50.0 | 90.4 / 85.7 / 93.6 | 95.6 / 93.5 / 96.9 | N/A                |                           |
|-----------+--------------------+--------------------+--------------------+--------------------+---------------------------|
| FC        | 79.0 / 89.7 / 88.1 | 84.8 / 74.4 / 89.2 | 91.5 / 85.4 / 94.2 | N/A                | 12/9                      |
| CNN       | 78.7 / 50.4 / 85.8 | 89.4 / 81.3 / 93.3 | 94.7 / 90.4 / 96.4 | N/A                | change padding length     |
| LSTM      | 92.3 / 93.4 / 95.6 | 91.8 / 84.4 / 93.5 | 95.4 / 92.1 / 97.0 | N/A                | using early stopping      |
|-----------+--------------------+--------------------+--------------------+--------------------+---------------------------|
| FC        | 79.4 / 90.0 / 87.7 | 85.1 / 75.3 / 88.7 | 91.5 / 85.6 / 94.2 | 56.2 / 90.7 / 79.1 | 12/10 LSTM remove dropout |
| CNN       | 79.5 / 48.8 / 86.3 | 89.5 / 79.6 / 93.0 | 94.4 / 90.2 / 96.2 | 64.5 / 91.7 / 83.3 |                           |
| CNN-patch | 76.5 / 46.4 / 82.2 | 88.6 / 89.2 / 92.7 | 94.2 / 89.2 / 96.2 | 66.2 / 91.4 / 84.2 |                           |
| LSTM      | 90.7 / 93.7 / 95.9 | 91.2 / 85.1 / 93.7 | 95.5 / 93.0 / 96.9 | 69.8 / 93.6 / 87.4 |                           |



| data augment method                  | comment        | Do? |
|--------------------------------------+----------------+-----|
| word-mutate (add + delete)           | regression     | Y   |
| word-mutate (add + delete + replace) |                |     |
| word-mutate (delete only)            |                | Y   |
|--------------------------------------+----------------+-----|
| sentence-mutate (add + delete)       | regression     |     |
| sentence-mutate (delete only)        |                |     |
|--------------------------------------+----------------+-----|
| negative sampling (1 sample)         | classification | Y   |
| negative sampling (5 samples)        |                | Y   |

Model

| model              | comment            |
|--------------------+--------------------|
| glove              | word embedding     |
| glove summary only |                    |
|--------------------+--------------------|
| USE-DAN            | sentence embedding |
| USE-Transformer    |                    |
|--------------------+--------------------|
| InferSent          | sentence embedding |

Network Architecture

| Architecture |
|--------------|
| CNN          |
| LSTM         |
| Dense        |


* Code structure and usage instruction

- =model.py=: models
- =embedding.py=: load glove embedding and USE sentence embedding
- =data.py=: prepare data
- =preprocessing.py=
- =config.py=: hyper parameters

=main.py= glue the data and model together: load data, build model,
train and validate results. Various of experiments are defined in the
following functions.
- =glove_main=
- =use_vector_main=
- =glove_summary_main=

See [[file:log.org][log.org]] for the experiment log.

* Mutation operator

    MODE can be add, delete, mutate (TODO). Generate 10 for each mode.
    
    I need to generate random mutation to the summary. Save it to a
    file so that I use the same generated data. For each summary, I
    generate several data:
        
    1. generate 10 random float numbers [0,1] as ratios
    2. for each ratio, do:
    2.1 deletion: select ratio percent of words to remove
    2.2 addition: add ratio percent of new words (from vocab.txt) to
    random places

    Issues:
    
    - should I add better, regularized noise, e.g. gaussian noise? How
      to do that?
    - should I check if the sentence is really modified?
    - should we use the text from original article?
    - should we treat sentences? should we maintain the sentence
      separator period?
