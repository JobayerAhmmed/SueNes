#+TITLE: Experiment log

* <2018-11-26 Mon> glove model

In [132]: model.fit(x_train, y_train,
     ...:           epochs=40, batch_size=128,
     ...:           validation_data=(x_val, y_val), verbose=1)
Train on 189000 samples, validate on 21000 samples
Epoch 1/40
2018-11-26 17:36:28.507505: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-26 17:36:28.603822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-26 17:36:28.604295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.58GiB
2018-11-26 17:36:28.604312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-26 17:36:28.763021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-26 17:36:28.763055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-26 17:36:28.763060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-26 17:36:28.763187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2276 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
188672/189000 [============================>.] - ETA: 0s - loss: 0.0567 - mean_absolute_error: 0.18242018-11-26 17:37:02.137188: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-26 17:37:02.153934: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
189000/189000 [==============================] - 35s 186us/step - loss: 0.0566 - mean_absolute_error: 0.1824 - val_loss: 0.0379 - val_mean_absolute_error: 0.1496
Epoch 2/40
189000/189000 [==============================] - 34s 182us/step - loss: 0.0355 - mean_absolute_error: 0.1427 - val_loss: 0.0473 - val_mean_absolute_error: 0.1656
Epoch 3/40
189000/189000 [==============================] - 35s 188us/step - loss: 0.0307 - mean_absolute_error: 0.1320 - val_loss: 0.0323 - val_mean_absolute_error: 0.1346
Epoch 4/40
189000/189000 [==============================] - 36s 192us/step - loss: 0.0278 - mean_absolute_error: 0.1252 - val_loss: 0.0340 - val_mean_absolute_error: 0.1359
Epoch 5/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0257 - mean_absolute_error: 0.1200 - val_loss: 0.0324 - val_mean_absolute_error: 0.1320
Epoch 6/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0238 - mean_absolute_error: 0.1155 - val_loss: 0.0303 - val_mean_absolute_error: 0.1272
Epoch 7/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0222 - mean_absolute_error: 0.1111 - val_loss: 0.0303 - val_mean_absolute_error: 0.1266
Epoch 8/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0209 - mean_absolute_error: 0.1077 - val_loss: 0.0311 - val_mean_absolute_error: 0.1280
Epoch 9/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0197 - mean_absolute_error: 0.1042 - val_loss: 0.0315 - val_mean_absolute_error: 0.1290
Epoch 10/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0186 - mean_absolute_error: 0.1014 - val_loss: 0.0312 - val_mean_absolute_error: 0.1310
Epoch 11/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0175 - mean_absolute_error: 0.0984 - val_loss: 0.0315 - val_mean_absolute_error: 0.1267
Epoch 12/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0166 - mean_absolute_error: 0.0956 - val_loss: 0.0303 - val_mean_absolute_error: 0.1249
Epoch 13/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0158 - mean_absolute_error: 0.0932 - val_loss: 0.0306 - val_mean_absolute_error: 0.1235
Epoch 14/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0149 - mean_absolute_error: 0.0907 - val_loss: 0.0311 - val_mean_absolute_error: 0.1256
Epoch 15/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0143 - mean_absolute_error: 0.0887 - val_loss: 0.0319 - val_mean_absolute_error: 0.1253
Epoch 16/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0137 - mean_absolute_error: 0.0867 - val_loss: 0.0308 - val_mean_absolute_error: 0.1255
Epoch 17/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0130 - mean_absolute_error: 0.0848 - val_loss: 0.0310 - val_mean_absolute_error: 0.1248
Epoch 18/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0124 - mean_absolute_error: 0.0829 - val_loss: 0.0327 - val_mean_absolute_error: 0.1266
Epoch 19/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0120 - mean_absolute_error: 0.0812 - val_loss: 0.0332 - val_mean_absolute_error: 0.1252
Epoch 20/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0114 - mean_absolute_error: 0.0793 - val_loss: 0.0331 - val_mean_absolute_error: 0.1254
Epoch 21/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0110 - mean_absolute_error: 0.0778 - val_loss: 0.0317 - val_mean_absolute_error: 0.1247
Epoch 22/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0316 - val_mean_absolute_error: 0.1239
Epoch 23/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0102 - mean_absolute_error: 0.0750 - val_loss: 0.0321 - val_mean_absolute_error: 0.1248
Epoch 24/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0098 - mean_absolute_error: 0.0737 - val_loss: 0.0338 - val_mean_absolute_error: 0.1262
Epoch 25/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0095 - mean_absolute_error: 0.0725 - val_loss: 0.0325 - val_mean_absolute_error: 0.1255
Epoch 26/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0092 - mean_absolute_error: 0.0714 - val_loss: 0.0328 - val_mean_absolute_error: 0.1258
Epoch 27/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0089 - mean_absolute_error: 0.0704 - val_loss: 0.0322 - val_mean_absolute_error: 0.1258
Epoch 28/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0086 - mean_absolute_error: 0.0691 - val_loss: 0.0328 - val_mean_absolute_error: 0.1241
Epoch 29/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0084 - mean_absolute_error: 0.0683 - val_loss: 0.0340 - val_mean_absolute_error: 0.1301
Epoch 30/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0081 - mean_absolute_error: 0.0673 - val_loss: 0.0349 - val_mean_absolute_error: 0.1277
Epoch 31/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0079 - mean_absolute_error: 0.0663 - val_loss: 0.0331 - val_mean_absolute_error: 0.1249
Epoch 32/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0078 - mean_absolute_error: 0.0658 - val_loss: 0.0335 - val_mean_absolute_error: 0.1265
Epoch 33/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0075 - mean_absolute_error: 0.0647 - val_loss: 0.0342 - val_mean_absolute_error: 0.1280
Epoch 34/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0073 - mean_absolute_error: 0.0639 - val_loss: 0.0346 - val_mean_absolute_error: 0.1256
Epoch 35/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0072 - mean_absolute_error: 0.0632 - val_loss: 0.0322 - val_mean_absolute_error: 0.1244
Epoch 36/40
189000/189000 [==============================] - 37s 193us/step - loss: 0.0070 - mean_absolute_error: 0.0625 - val_loss: 0.0326 - val_mean_absolute_error: 0.1241
Epoch 37/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0068 - mean_absolute_error: 0.0619 - val_loss: 0.0326 - val_mean_absolute_error: 0.1246
Epoch 38/40
189000/189000 [==============================] - 36s 193us/step - loss: 0.0067 - mean_absolute_error: 0.0613 - val_loss: 0.0352 - val_mean_absolute_error: 0.1254
Epoch 39/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0065 - mean_absolute_error: 0.0606 - val_loss: 0.0327 - val_mean_absolute_error: 0.1244
Epoch 40/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0064 - mean_absolute_error: 0.0600 - val_loss: 0.0350 - val_mean_absolute_error: 0.1279
Out[132]: <keras.callbacks.History at 0x7f1f9029ce48>

In [133]: 



* <2018-11-27 Tue> glove model

In [28]: model.fit(x_train, y_train,
    ...:           epochs=40, batch_size=128,
    ...:           validation_data=(x_val, y_val), verbose=1)
Train on 189000 samples, validate on 21000 samples
Epoch 1/40
189000/189000 [==============================] - 35s 188us/step - loss: 0.0323 - mean_absolute_error: 0.1331 - val_loss: 0.0175 - val_mean_absolute_error: 0.1020
Epoch 2/40
189000/189000 [==============================] - 36s 193us/step - loss: 0.0144 - mean_absolute_error: 0.0923 - val_loss: 0.0130 - val_mean_absolute_error: 0.0871
Epoch 3/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0118 - mean_absolute_error: 0.0832 - val_loss: 0.0121 - val_mean_absolute_error: 0.0834
Epoch 4/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0101 - mean_absolute_error: 0.0767 - val_loss: 0.0113 - val_mean_absolute_error: 0.0805
Epoch 5/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0088 - mean_absolute_error: 0.0713 - val_loss: 0.0104 - val_mean_absolute_error: 0.0767
Epoch 6/40
189000/189000 [==============================] - 38s 200us/step - loss: 0.0078 - mean_absolute_error: 0.0669 - val_loss: 0.0096 - val_mean_absolute_error: 0.0735
Epoch 7/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0070 - mean_absolute_error: 0.0636 - val_loss: 0.0135 - val_mean_absolute_error: 0.0896
Epoch 8/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0063 - mean_absolute_error: 0.0606 - val_loss: 0.0090 - val_mean_absolute_error: 0.0704
Epoch 9/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0058 - mean_absolute_error: 0.0580 - val_loss: 0.0093 - val_mean_absolute_error: 0.0718
Epoch 10/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0054 - mean_absolute_error: 0.0558 - val_loss: 0.0089 - val_mean_absolute_error: 0.0698
Epoch 11/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0050 - mean_absolute_error: 0.0538 - val_loss: 0.0093 - val_mean_absolute_error: 0.0720
Epoch 12/40
189000/189000 [==============================] - 37s 197us/step - loss: 0.0046 - mean_absolute_error: 0.0519 - val_loss: 0.0085 - val_mean_absolute_error: 0.0665
Epoch 13/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0044 - mean_absolute_error: 0.0504 - val_loss: 0.0085 - val_mean_absolute_error: 0.0683
Epoch 14/40
189000/189000 [==============================] - 37s 198us/step - loss: 0.0041 - mean_absolute_error: 0.0490 - val_loss: 0.0089 - val_mean_absolute_error: 0.0680
Epoch 15/40
189000/189000 [==============================] - 37s 197us/step - loss: 0.0039 - mean_absolute_error: 0.0476 - val_loss: 0.0088 - val_mean_absolute_error: 0.0676
Epoch 16/40
189000/189000 [==============================] - 38s 201us/step - loss: 0.0037 - mean_absolute_error: 0.0466 - val_loss: 0.0084 - val_mean_absolute_error: 0.0651
Epoch 17/40
189000/189000 [==============================] - 38s 201us/step - loss: 0.0035 - mean_absolute_error: 0.0454 - val_loss: 0.0083 - val_mean_absolute_error: 0.0661
Epoch 18/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0033 - mean_absolute_error: 0.0445 - val_loss: 0.0080 - val_mean_absolute_error: 0.0643
Epoch 19/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0032 - mean_absolute_error: 0.0434 - val_loss: 0.0093 - val_mean_absolute_error: 0.0710
Epoch 20/40
189000/189000 [==============================] - 38s 199us/step - loss: 0.0031 - mean_absolute_error: 0.0425 - val_loss: 0.0083 - val_mean_absolute_error: 0.0655
Epoch 21/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0029 - mean_absolute_error: 0.0417 - val_loss: 0.0079 - val_mean_absolute_error: 0.0638
Epoch 22/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0028 - mean_absolute_error: 0.0409 - val_loss: 0.0078 - val_mean_absolute_error: 0.0629
Epoch 23/40
189000/189000 [==============================] - 38s 199us/step - loss: 0.0027 - mean_absolute_error: 0.0403 - val_loss: 0.0080 - val_mean_absolute_error: 0.0630
Epoch 24/40
189000/189000 [==============================] - 38s 202us/step - loss: 0.0026 - mean_absolute_error: 0.0395 - val_loss: 0.0081 - val_mean_absolute_error: 0.0646
Epoch 25/40
189000/189000 [==============================] - 38s 201us/step - loss: 0.0025 - mean_absolute_error: 0.0388 - val_loss: 0.0084 - val_mean_absolute_error: 0.0655
Epoch 26/40
164608/189000 [=========================>....] - ETA: 4s - loss: 0.0024 - mean_absolute_error: 0.0380---------------------------------------------------------------------------

* <2018-11-30 Fri> sentence encoding (UAE model)

In [11]: train_model(model, data)
Train on 125515 samples, validate on 13946 samples
Epoch 1/40
2018-11-30 13:46:19.878392: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 13:46:19.980973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 13:46:19.981417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.66GiB
2018-11-30 13:46:19.981436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 13:46:20.138845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 13:46:20.138886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 13:46:20.138891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 13:46:20.139265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2358 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
125515/125515 [==============================] - 5s 42us/step - loss: 0.0587 - mean_absolute_error: 0.1884 - val_loss: 0.0325 - val_mean_absolute_error: 0.1409
Epoch 2/40
125515/125515 [==============================] - 4s 34us/step - loss: 0.0302 - mean_absolute_error: 0.1350 - val_loss: 0.0287 - val_mean_absolute_error: 0.1304
Epoch 3/40
125515/125515 [==============================] - 4s 34us/step - loss: 0.0262 - mean_absolute_error: 0.1248 - val_loss: 0.0277 - val_mean_absolute_error: 0.1273
Epoch 4/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0238 - mean_absolute_error: 0.1183 - val_loss: 0.0266 - val_mean_absolute_error: 0.1235
Epoch 5/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0219 - mean_absolute_error: 0.1130 - val_loss: 0.0258 - val_mean_absolute_error: 0.1209
Epoch 6/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0205 - mean_absolute_error: 0.1090 - val_loss: 0.0243 - val_mean_absolute_error: 0.1165
Epoch 7/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0192 - mean_absolute_error: 0.1052 - val_loss: 0.0230 - val_mean_absolute_error: 0.1150
Epoch 8/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0181 - mean_absolute_error: 0.1019 - val_loss: 0.0249 - val_mean_absolute_error: 0.1176
Epoch 9/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0172 - mean_absolute_error: 0.0990 - val_loss: 0.0219 - val_mean_absolute_error: 0.1119
Epoch 10/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0165 - mean_absolute_error: 0.0967 - val_loss: 0.0221 - val_mean_absolute_error: 0.1102
Epoch 11/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0157 - mean_absolute_error: 0.0944 - val_loss: 0.0221 - val_mean_absolute_error: 0.1093
Epoch 12/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0151 - mean_absolute_error: 0.0924 - val_loss: 0.0213 - val_mean_absolute_error: 0.1079
Epoch 13/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0145 - mean_absolute_error: 0.0903 - val_loss: 0.0214 - val_mean_absolute_error: 0.1079
Epoch 14/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0140 - mean_absolute_error: 0.0887 - val_loss: 0.0211 - val_mean_absolute_error: 0.1084
Epoch 15/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0135 - mean_absolute_error: 0.0870 - val_loss: 0.0218 - val_mean_absolute_error: 0.1075
Epoch 16/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0131 - mean_absolute_error: 0.0857 - val_loss: 0.0226 - val_mean_absolute_error: 0.1091
Epoch 17/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0127 - mean_absolute_error: 0.0843 - val_loss: 0.0213 - val_mean_absolute_error: 0.1082
Epoch 18/40
125515/125515 [==============================] - 4s 34us/step - loss: 0.0123 - mean_absolute_error: 0.0830 - val_loss: 0.0217 - val_mean_absolute_error: 0.1094
Epoch 19/40
125515/125515 [==============================] - 4s 34us/step - loss: 0.0120 - mean_absolute_error: 0.0818 - val_loss: 0.0213 - val_mean_absolute_error: 0.1071
Epoch 20/40
125515/125515 [==============================] - 4s 34us/step - loss: 0.0117 - mean_absolute_error: 0.0808 - val_loss: 0.0221 - val_mean_absolute_error: 0.1115
Epoch 21/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0113 - mean_absolute_error: 0.0795 - val_loss: 0.0213 - val_mean_absolute_error: 0.1064
Epoch 22/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0110 - mean_absolute_error: 0.0783 - val_loss: 0.0215 - val_mean_absolute_error: 0.1078
Epoch 23/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0108 - mean_absolute_error: 0.0775 - val_loss: 0.0215 - val_mean_absolute_error: 0.1058
Epoch 24/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0105 - mean_absolute_error: 0.0765 - val_loss: 0.0231 - val_mean_absolute_error: 0.1107
Epoch 25/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0103 - mean_absolute_error: 0.0757 - val_loss: 0.0215 - val_mean_absolute_error: 0.1076
Epoch 26/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0100 - mean_absolute_error: 0.0746 - val_loss: 0.0229 - val_mean_absolute_error: 0.1083
Epoch 27/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0098 - mean_absolute_error: 0.0737 - val_loss: 0.0213 - val_mean_absolute_error: 0.1054
Epoch 28/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0096 - mean_absolute_error: 0.0729 - val_loss: 0.0216 - val_mean_absolute_error: 0.1082
Epoch 29/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0094 - mean_absolute_error: 0.0722 - val_loss: 0.0212 - val_mean_absolute_error: 0.1068
Epoch 30/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0092 - mean_absolute_error: 0.0712 - val_loss: 0.0215 - val_mean_absolute_error: 0.1060
Epoch 31/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0090 - mean_absolute_error: 0.0705 - val_loss: 0.0216 - val_mean_absolute_error: 0.1067
Epoch 32/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0088 - mean_absolute_error: 0.0697 - val_loss: 0.0221 - val_mean_absolute_error: 0.1102
Epoch 33/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0086 - mean_absolute_error: 0.0689 - val_loss: 0.0219 - val_mean_absolute_error: 0.1065
Epoch 34/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0085 - mean_absolute_error: 0.0683 - val_loss: 0.0228 - val_mean_absolute_error: 0.1097
Epoch 35/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0083 - mean_absolute_error: 0.0678 - val_loss: 0.0220 - val_mean_absolute_error: 0.1062
Epoch 36/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0082 - mean_absolute_error: 0.0672 - val_loss: 0.0214 - val_mean_absolute_error: 0.1057
Epoch 37/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0080 - mean_absolute_error: 0.0665 - val_loss: 0.0218 - val_mean_absolute_error: 0.1064
Epoch 38/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0078 - mean_absolute_error: 0.0658 - val_loss: 0.0217 - val_mean_absolute_error: 0.1066
Epoch 39/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0077 - mean_absolute_error: 0.0652 - val_loss: 0.0223 - val_mean_absolute_error: 0.1068
Epoch 40/40
125515/125515 [==============================] - 4s 36us/step - loss: 0.0076 - mean_absolute_error: 0.0645 - val_loss: 0.0232 - val_mean_absolute_error: 0.1081
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________


* <2018-11-30 Fri> Glove model, 1000 sample
In [23]: train_model(model, data)
Train on 18900 samples, validate on 2100 samples
Epoch 1/40
2018-11-30 14:14:00.026530: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 14:14:00.203571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 14:14:00.204047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.60GiB
2018-11-30 14:14:00.204064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 14:14:01.013035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 14:14:01.013068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 14:14:01.013075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 14:14:01.013233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2301 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
18688/18900 [============================>.] - ETA: 0s - loss: 0.1276 - mean_absolute_error: 0.2883 - pearson_correlation_f2: 0.37032018-11-30 14:14:05.928711: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.36GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 14:14:05.949685: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.67GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 14:14:06.169337: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18900/18900 [==============================] - 6s 325us/step - loss: 0.1268 - mean_absolute_error: 0.2872 - pearson_correlation_f2: 0.3742 - val_loss: 0.0490 - val_mean_absolute_error: 0.1774 - val_pearson_correlation_f2: 0.7510
Epoch 2/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0449 - mean_absolute_error: 0.1676 - pearson_correlation_f2: 0.8332 - val_loss: 0.0383 - val_mean_absolute_error: 0.1577 - val_pearson_correlation_f2: 0.8828
Epoch 3/40
18900/18900 [==============================] - 3s 180us/step - loss: 0.0254 - mean_absolute_error: 0.1251 - pearson_correlation_f2: 0.9034 - val_loss: 0.0210 - val_mean_absolute_error: 0.1124 - val_pearson_correlation_f2: 0.9070
Epoch 4/40
18900/18900 [==============================] - 3s 179us/step - loss: 0.0179 - mean_absolute_error: 0.1036 - pearson_correlation_f2: 0.9295 - val_loss: 0.0199 - val_mean_absolute_error: 0.1122 - val_pearson_correlation_f2: 0.9210
Epoch 5/40
18900/18900 [==============================] - 3s 180us/step - loss: 0.0134 - mean_absolute_error: 0.0895 - pearson_correlation_f2: 0.9483 - val_loss: 0.0130 - val_mean_absolute_error: 0.0867 - val_pearson_correlation_f2: 0.9275
Epoch 6/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0104 - mean_absolute_error: 0.0790 - pearson_correlation_f2: 0.9605 - val_loss: 0.0165 - val_mean_absolute_error: 0.0998 - val_pearson_correlation_f2: 0.9294
Epoch 7/40
18900/18900 [==============================] - 3s 185us/step - loss: 0.0084 - mean_absolute_error: 0.0709 - pearson_correlation_f2: 0.9679 - val_loss: 0.0142 - val_mean_absolute_error: 0.0871 - val_pearson_correlation_f2: 0.9321
Epoch 8/40
18900/18900 [==============================] - 4s 189us/step - loss: 0.0069 - mean_absolute_error: 0.0643 - pearson_correlation_f2: 0.9732 - val_loss: 0.0119 - val_mean_absolute_error: 0.0808 - val_pearson_correlation_f2: 0.9348
Epoch 9/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0060 - mean_absolute_error: 0.0604 - pearson_correlation_f2: 0.9765 - val_loss: 0.0123 - val_mean_absolute_error: 0.0839 - val_pearson_correlation_f2: 0.9328
Epoch 10/40
18900/18900 [==============================] - 3s 185us/step - loss: 0.0054 - mean_absolute_error: 0.0568 - pearson_correlation_f2: 0.9792 - val_loss: 0.0134 - val_mean_absolute_error: 0.0821 - val_pearson_correlation_f2: 0.9328
Epoch 11/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0047 - mean_absolute_error: 0.0529 - pearson_correlation_f2: 0.9816 - val_loss: 0.0122 - val_mean_absolute_error: 0.0798 - val_pearson_correlation_f2: 0.9362
Epoch 12/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0042 - mean_absolute_error: 0.0502 - pearson_correlation_f2: 0.9832 - val_loss: 0.0130 - val_mean_absolute_error: 0.0843 - val_pearson_correlation_f2: 0.9381
Epoch 13/40
18900/18900 [==============================] - 4s 189us/step - loss: 0.0041 - mean_absolute_error: 0.0492 - pearson_correlation_f2: 0.9845 - val_loss: 0.0117 - val_mean_absolute_error: 0.0797 - val_pearson_correlation_f2: 0.9384
Epoch 14/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0037 - mean_absolute_error: 0.0472 - pearson_correlation_f2: 0.9854 - val_loss: 0.0127 - val_mean_absolute_error: 0.0814 - val_pearson_correlation_f2: 0.9380
Epoch 15/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0033 - mean_absolute_error: 0.0449 - pearson_correlation_f2: 0.9866 - val_loss: 0.0098 - val_mean_absolute_error: 0.0733 - val_pearson_correlation_f2: 0.9419
Epoch 16/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0033 - mean_absolute_error: 0.0442 - pearson_correlation_f2: 0.9870 - val_loss: 0.0099 - val_mean_absolute_error: 0.0715 - val_pearson_correlation_f2: 0.9421
Epoch 17/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0030 - mean_absolute_error: 0.0427 - pearson_correlation_f2: 0.9883 - val_loss: 0.0097 - val_mean_absolute_error: 0.0712 - val_pearson_correlation_f2: 0.9424
Epoch 18/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0029 - mean_absolute_error: 0.0414 - pearson_correlation_f2: 0.9884 - val_loss: 0.0105 - val_mean_absolute_error: 0.0739 - val_pearson_correlation_f2: 0.9415
Epoch 19/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0027 - mean_absolute_error: 0.0402 - pearson_correlation_f2: 0.9892 - val_loss: 0.0101 - val_mean_absolute_error: 0.0741 - val_pearson_correlation_f2: 0.9430
Epoch 20/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0025 - mean_absolute_error: 0.0383 - pearson_correlation_f2: 0.9900 - val_loss: 0.0101 - val_mean_absolute_error: 0.0710 - val_pearson_correlation_f2: 0.9418
Epoch 21/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0025 - mean_absolute_error: 0.0380 - pearson_correlation_f2: 0.9904 - val_loss: 0.0097 - val_mean_absolute_error: 0.0696 - val_pearson_correlation_f2: 0.9425
Epoch 22/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0023 - mean_absolute_error: 0.0372 - pearson_correlation_f2: 0.9908 - val_loss: 0.0097 - val_mean_absolute_error: 0.0694 - val_pearson_correlation_f2: 0.9437
Epoch 23/40
18900/18900 [==============================] - 3s 185us/step - loss: 0.0023 - mean_absolute_error: 0.0368 - pearson_correlation_f2: 0.9910 - val_loss: 0.0110 - val_mean_absolute_error: 0.0757 - val_pearson_correlation_f2: 0.9385
Epoch 24/40
18900/18900 [==============================] - 4s 188us/step - loss: 0.0021 - mean_absolute_error: 0.0352 - pearson_correlation_f2: 0.9914 - val_loss: 0.0097 - val_mean_absolute_error: 0.0715 - val_pearson_correlation_f2: 0.9426
Epoch 25/40
18900/18900 [==============================] - 4s 188us/step - loss: 0.0021 - mean_absolute_error: 0.0353 - pearson_correlation_f2: 0.9917 - val_loss: 0.0113 - val_mean_absolute_error: 0.0760 - val_pearson_correlation_f2: 0.9404
Epoch 26/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0020 - mean_absolute_error: 0.0343 - pearson_correlation_f2: 0.9919 - val_loss: 0.0105 - val_mean_absolute_error: 0.0720 - val_pearson_correlation_f2: 0.9424
Epoch 27/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0019 - mean_absolute_error: 0.0335 - pearson_correlation_f2: 0.9924 - val_loss: 0.0104 - val_mean_absolute_error: 0.0736 - val_pearson_correlation_f2: 0.9422
Epoch 28/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0018 - mean_absolute_error: 0.0332 - pearson_correlation_f2: 0.9925 - val_loss: 0.0108 - val_mean_absolute_error: 0.0725 - val_pearson_correlation_f2: 0.9402
Epoch 29/40
18900/18900 [==============================] - 4s 194us/step - loss: 0.0018 - mean_absolute_error: 0.0327 - pearson_correlation_f2: 0.9927 - val_loss: 0.0096 - val_mean_absolute_error: 0.0709 - val_pearson_correlation_f2: 0.9438
Epoch 30/40
18900/18900 [==============================] - 4s 195us/step - loss: 0.0018 - mean_absolute_error: 0.0322 - pearson_correlation_f2: 0.9931 - val_loss: 0.0100 - val_mean_absolute_error: 0.0724 - val_pearson_correlation_f2: 0.9431
Epoch 31/40
18900/18900 [==============================] - 4s 199us/step - loss: 0.0017 - mean_absolute_error: 0.0314 - pearson_correlation_f2: 0.9933 - val_loss: 0.0105 - val_mean_absolute_error: 0.0765 - val_pearson_correlation_f2: 0.9396
Epoch 32/40
18900/18900 [==============================] - 4s 197us/step - loss: 0.0017 - mean_absolute_error: 0.0313 - pearson_correlation_f2: 0.9935 - val_loss: 0.0107 - val_mean_absolute_error: 0.0721 - val_pearson_correlation_f2: 0.9412
Epoch 33/40
18900/18900 [==============================] - 4s 194us/step - loss: 0.0016 - mean_absolute_error: 0.0308 - pearson_correlation_f2: 0.9936 - val_loss: 0.0101 - val_mean_absolute_error: 0.0723 - val_pearson_correlation_f2: 0.9433
Epoch 34/40
18900/18900 [==============================] - 4s 191us/step - loss: 0.0016 - mean_absolute_error: 0.0303 - pearson_correlation_f2: 0.9939 - val_loss: 0.0095 - val_mean_absolute_error: 0.0712 - val_pearson_correlation_f2: 0.9448
Epoch 35/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0015 - mean_absolute_error: 0.0297 - pearson_correlation_f2: 0.9939 - val_loss: 0.0097 - val_mean_absolute_error: 0.0691 - val_pearson_correlation_f2: 0.9430
Epoch 36/40
18900/18900 [==============================] - 4s 193us/step - loss: 0.0015 - mean_absolute_error: 0.0296 - pearson_correlation_f2: 0.9941 - val_loss: 0.0101 - val_mean_absolute_error: 0.0726 - val_pearson_correlation_f2: 0.9433
Epoch 37/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0014 - mean_absolute_error: 0.0285 - pearson_correlation_f2: 0.9943 - val_loss: 0.0095 - val_mean_absolute_error: 0.0688 - val_pearson_correlation_f2: 0.9442
Epoch 38/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0013 - mean_absolute_error: 0.0283 - pearson_correlation_f2: 0.9946 - val_loss: 0.0094 - val_mean_absolute_error: 0.0705 - val_pearson_correlation_f2: 0.9442
Epoch 39/40
18900/18900 [==============================] - 4s 197us/step - loss: 0.0014 - mean_absolute_error: 0.0282 - pearson_correlation_f2: 0.9945 - val_loss: 0.0109 - val_mean_absolute_error: 0.0770 - val_pearson_correlation_f2: 0.9457
Epoch 40/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0013 - mean_absolute_error: 0.0281 - pearson_correlation_f2: 0.9948 - val_loss: 0.0095 - val_mean_absolute_error: 0.0684 - val_pearson_correlation_f2: 0.9448
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          10782500  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 11,027,365
Trainable params: 244,865
Non-trainable params: 10,782,500
_________________________________________________________________

* <2018-11-30 Fri> GLove, 10000 sample

In [11]: (x_train, y_train), (x_val, y_val) = data

In [12]: x_train.shape
Out[12]: (189000, 640)

In [13]: y_train.shape
Out[13]: (189000,)

In [14]: x_val.shape
Out[14]: (21000, 640)

In [15]: y_val.shape
Out[15]: (21000,)

In [16]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [17]: model = build_glove_model(embedding_layer)

In [18]: train_model(model, data)
Train on 189000 samples, validate on 21000 samples
Epoch 1/40
2018-11-30 14:21:40.561028: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 14:21:40.651910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 14:21:40.652347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.58GiB
2018-11-30 14:21:40.652362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 14:21:40.807973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 14:21:40.808005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 14:21:40.808011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 14:21:40.808154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2278 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
188800/189000 [============================>.] - ETA: 0s - loss: 0.0336 - mean_absolute_error: 0.1342 - pearson_correlation_f: 0.83012018-11-30 14:22:14.725850: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 14:22:14.742010: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 14:22:16.324400: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.16GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
189000/189000 [==============================] - 36s 189us/step - loss: 0.0336 - mean_absolute_error: 0.1342 - pearson_correlation_f: 0.8302 - val_loss: 0.0161 - val_mean_absolute_error: 0.0994 - val_pearson_correlation_f: 0.9158
Epoch 2/40
189000/189000 [==============================] - 35s 185us/step - loss: 0.0149 - mean_absolute_error: 0.0936 - pearson_correlation_f: 0.9215 - val_loss: 0.0132 - val_mean_absolute_error: 0.0892 - val_pearson_correlation_f: 0.9246
Epoch 3/40
189000/189000 [==============================] - 36s 192us/step - loss: 0.0123 - mean_absolute_error: 0.0846 - pearson_correlation_f: 0.9352 - val_loss: 0.0126 - val_mean_absolute_error: 0.0861 - val_pearson_correlation_f: 0.9292
Epoch 4/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0103 - mean_absolute_error: 0.0774 - pearson_correlation_f: 0.9453 - val_loss: 0.0116 - val_mean_absolute_error: 0.0821 - val_pearson_correlation_f: 0.9346
Epoch 5/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0090 - mean_absolute_error: 0.0721 - pearson_correlation_f: 0.9525 - val_loss: 0.0107 - val_mean_absolute_error: 0.0772 - val_pearson_correlation_f: 0.9404
Epoch 6/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0079 - mean_absolute_error: 0.0673 - pearson_correlation_f: 0.9586 - val_loss: 0.0109 - val_mean_absolute_error: 0.0778 - val_pearson_correlation_f: 0.9396
Epoch 7/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0071 - mean_absolute_error: 0.0637 - pearson_correlation_f: 0.9631 - val_loss: 0.0109 - val_mean_absolute_error: 0.0792 - val_pearson_correlation_f: 0.9402
Epoch 8/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0064 - mean_absolute_error: 0.0607 - pearson_correlation_f: 0.9665 - val_loss: 0.0097 - val_mean_absolute_error: 0.0731 - val_pearson_correlation_f: 0.9452
Epoch 9/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0058 - mean_absolute_error: 0.0580 - pearson_correlation_f: 0.9697 - val_loss: 0.0096 - val_mean_absolute_error: 0.0719 - val_pearson_correlation_f: 0.9472
Epoch 10/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0054 - mean_absolute_error: 0.0557 - pearson_correlation_f: 0.9720 - val_loss: 0.0099 - val_mean_absolute_error: 0.0733 - val_pearson_correlation_f: 0.9465
Epoch 11/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0050 - mean_absolute_error: 0.0537 - pearson_correlation_f: 0.9741 - val_loss: 0.0136 - val_mean_absolute_error: 0.0823 - val_pearson_correlation_f: 0.9402
Epoch 12/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0047 - mean_absolute_error: 0.0520 - pearson_correlation_f: 0.9758 - val_loss: 0.0093 - val_mean_absolute_error: 0.0690 - val_pearson_correlation_f: 0.9484
Epoch 13/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0043 - mean_absolute_error: 0.0502 - pearson_correlation_f: 0.9775 - val_loss: 0.0092 - val_mean_absolute_error: 0.0691 - val_pearson_correlation_f: 0.9501
Epoch 14/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0041 - mean_absolute_error: 0.0489 - pearson_correlation_f: 0.9788 - val_loss: 0.0090 - val_mean_absolute_error: 0.0672 - val_pearson_correlation_f: 0.9509
Epoch 15/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0039 - mean_absolute_error: 0.0476 - pearson_correlation_f: 0.9800 - val_loss: 0.0095 - val_mean_absolute_error: 0.0695 - val_pearson_correlation_f: 0.9467
Epoch 16/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0037 - mean_absolute_error: 0.0465 - pearson_correlation_f: 0.9810 - val_loss: 0.0088 - val_mean_absolute_error: 0.0677 - val_pearson_correlation_f: 0.9509
Epoch 17/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0035 - mean_absolute_error: 0.0453 - pearson_correlation_f: 0.9821 - val_loss: 0.0095 - val_mean_absolute_error: 0.0689 - val_pearson_correlation_f: 0.9514
Epoch 18/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0033 - mean_absolute_error: 0.0443 - pearson_correlation_f: 0.9829 - val_loss: 0.0091 - val_mean_absolute_error: 0.0679 - val_pearson_correlation_f: 0.9518
Epoch 19/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0032 - mean_absolute_error: 0.0433 - pearson_correlation_f: 0.9839 - val_loss: 0.0087 - val_mean_absolute_error: 0.0658 - val_pearson_correlation_f: 0.9514
Epoch 20/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0031 - mean_absolute_error: 0.0426 - pearson_correlation_f: 0.9844 - val_loss: 0.0095 - val_mean_absolute_error: 0.0691 - val_pearson_correlation_f: 0.9494
Epoch 21/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0030 - mean_absolute_error: 0.0418 - pearson_correlation_f: 0.9851 - val_loss: 0.0085 - val_mean_absolute_error: 0.0661 - val_pearson_correlation_f: 0.9526
Epoch 22/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0028 - mean_absolute_error: 0.0410 - pearson_correlation_f: 0.9857 - val_loss: 0.0090 - val_mean_absolute_error: 0.0661 - val_pearson_correlation_f: 0.9513
Epoch 23/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0027 - mean_absolute_error: 0.0404 - pearson_correlation_f: 0.9862 - val_loss: 0.0090 - val_mean_absolute_error: 0.0679 - val_pearson_correlation_f: 0.9520
Epoch 24/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0027 - mean_absolute_error: 0.0398 - pearson_correlation_f: 0.9867 - val_loss: 0.0088 - val_mean_absolute_error: 0.0660 - val_pearson_correlation_f: 0.9516
Epoch 25/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0026 - mean_absolute_error: 0.0392 - pearson_correlation_f: 0.9871 - val_loss: 0.0088 - val_mean_absolute_error: 0.0671 - val_pearson_correlation_f: 0.9512
Epoch 26/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0025 - mean_absolute_error: 0.0386 - pearson_correlation_f: 0.9875 - val_loss: 0.0086 - val_mean_absolute_error: 0.0642 - val_pearson_correlation_f: 0.9518
Epoch 27/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0024 - mean_absolute_error: 0.0381 - pearson_correlation_f: 0.9879 - val_loss: 0.0088 - val_mean_absolute_error: 0.0649 - val_pearson_correlation_f: 0.9511
Epoch 28/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0024 - mean_absolute_error: 0.0376 - pearson_correlation_f: 0.9883 - val_loss: 0.0084 - val_mean_absolute_error: 0.0647 - val_pearson_correlation_f: 0.9529
Epoch 29/40
189000/189000 [==============================] - 37s 194us/step - loss: 0.0023 - mean_absolute_error: 0.0372 - pearson_correlation_f: 0.9887 - val_loss: 0.0094 - val_mean_absolute_error: 0.0685 - val_pearson_correlation_f: 0.9532
Epoch 30/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0022 - mean_absolute_error: 0.0366 - pearson_correlation_f: 0.9890 - val_loss: 0.0087 - val_mean_absolute_error: 0.0656 - val_pearson_correlation_f: 0.9515
Epoch 31/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0022 - mean_absolute_error: 0.0363 - pearson_correlation_f: 0.9892 - val_loss: 0.0089 - val_mean_absolute_error: 0.0679 - val_pearson_correlation_f: 0.9527
Epoch 32/40
189000/189000 [==============================] - 37s 195us/step - loss: 0.0021 - mean_absolute_error: 0.0359 - pearson_correlation_f: 0.9894 - val_loss: 0.0088 - val_mean_absolute_error: 0.0646 - val_pearson_correlation_f: 0.9518
Epoch 33/40
189000/189000 [==============================] - 37s 196us/step - loss: 0.0021 - mean_absolute_error: 0.0356 - pearson_correlation_f: 0.9897 - val_loss: 0.0088 - val_mean_absolute_error: 0.0652 - val_pearson_correlation_f: 0.9509
Epoch 34/40
189000/189000 [==============================] - 37s 197us/step - loss: 0.0020 - mean_absolute_error: 0.0350 - pearson_correlation_f: 0.9900 - val_loss: 0.0094 - val_mean_absolute_error: 0.0690 - val_pearson_correlation_f: 0.9511
Epoch 35/40
189000/189000 [==============================] - 37s 198us/step - loss: 0.0020 - mean_absolute_error: 0.0347 - pearson_correlation_f: 0.9902 - val_loss: 0.0088 - val_mean_absolute_error: 0.0645 - val_pearson_correlation_f: 0.9520
Epoch 36/40
189000/189000 [==============================] - 37s 198us/step - loss: 0.0020 - mean_absolute_error: 0.0344 - pearson_correlation_f: 0.9905 - val_loss: 0.0084 - val_mean_absolute_error: 0.0634 - val_pearson_correlation_f: 0.9530
Epoch 37/40
189000/189000 [==============================] - 37s 198us/step - loss: 0.0019 - mean_absolute_error: 0.0341 - pearson_correlation_f: 0.9907 - val_loss: 0.0093 - val_mean_absolute_error: 0.0674 - val_pearson_correlation_f: 0.9518
Epoch 38/40
189000/189000 [==============================] - 38s 202us/step - loss: 0.0019 - mean_absolute_error: 0.0338 - pearson_correlation_f: 0.9908 - val_loss: 0.0086 - val_mean_absolute_error: 0.0641 - val_pearson_correlation_f: 0.9536
Epoch 39/40
189000/189000 [==============================] - 37s 197us/step - loss: 0.0019 - mean_absolute_error: 0.0334 - pearson_correlation_f: 0.9910 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654 - val_pearson_correlation_f: 0.9540
Epoch 40/40
189000/189000 [==============================] - 37s 198us/step - loss: 0.0018 - mean_absolute_error: 0.0332 - pearson_correlation_f: 0.9912 - val_loss: 0.0087 - val_mean_absolute_error: 0.0650 - val_pearson_correlation_f: 0.9530
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          17878200  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 18,123,065
Trainable params: 244,865
Non-trainable params: 17,878,200
_________________________________________________________________

* <2018-11-30 Fri> Glove summary only, 10000 sample

In [57]: model = build_glove_summary_only_model(embedding_layer)

In [58]: train_model(model, data)
Train on 189000 samples, validate on 21000 samples
Epoch 1/40
2018-11-30 14:53:31.404973: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 14:53:31.500657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 14:53:31.501141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.61GiB
2018-11-30 14:53:31.501157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 14:53:31.672458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 14:53:31.672493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 14:53:31.672498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 14:53:31.672644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2312 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2018-11-30 14:53:32.831414: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 14:53:32.865222: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.62GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
188672/189000 [============================>.] - ETA: 0s - loss: 0.0265 - mean_absolute_error: 0.1227 - pearson_correlation_f: 0.87052018-11-30 14:53:41.808075: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
189000/189000 [==============================] - 11s 58us/step - loss: 0.0265 - mean_absolute_error: 0.1226 - pearson_correlation_f: 0.8706 - val_loss: 0.0187 - val_mean_absolute_error: 0.1066 - val_pearson_correlation_f: 0.9105
Epoch 2/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0161 - mean_absolute_error: 0.0978 - pearson_correlation_f: 0.9171 - val_loss: 0.0148 - val_mean_absolute_error: 0.0953 - val_pearson_correlation_f: 0.9164
Epoch 3/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0140 - mean_absolute_error: 0.0908 - pearson_correlation_f: 0.9280 - val_loss: 0.0132 - val_mean_absolute_error: 0.0885 - val_pearson_correlation_f: 0.9260
Epoch 4/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0125 - mean_absolute_error: 0.0856 - pearson_correlation_f: 0.9355 - val_loss: 0.0121 - val_mean_absolute_error: 0.0833 - val_pearson_correlation_f: 0.9314
Epoch 5/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0115 - mean_absolute_error: 0.0820 - pearson_correlation_f: 0.9406 - val_loss: 0.0121 - val_mean_absolute_error: 0.0835 - val_pearson_correlation_f: 0.9327
Epoch 6/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0107 - mean_absolute_error: 0.0790 - pearson_correlation_f: 0.9449 - val_loss: 0.0133 - val_mean_absolute_error: 0.0879 - val_pearson_correlation_f: 0.9260
Epoch 7/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0100 - mean_absolute_error: 0.0765 - pearson_correlation_f: 0.9482 - val_loss: 0.0148 - val_mean_absolute_error: 0.0899 - val_pearson_correlation_f: 0.9269
Epoch 8/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0095 - mean_absolute_error: 0.0745 - pearson_correlation_f: 0.9509 - val_loss: 0.0128 - val_mean_absolute_error: 0.0868 - val_pearson_correlation_f: 0.9339
Epoch 9/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0090 - mean_absolute_error: 0.0726 - pearson_correlation_f: 0.9533 - val_loss: 0.0144 - val_mean_absolute_error: 0.0928 - val_pearson_correlation_f: 0.9317
Epoch 10/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0085 - mean_absolute_error: 0.0707 - pearson_correlation_f: 0.9558 - val_loss: 0.0128 - val_mean_absolute_error: 0.0841 - val_pearson_correlation_f: 0.9344
Epoch 11/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0082 - mean_absolute_error: 0.0693 - pearson_correlation_f: 0.9577 - val_loss: 0.0115 - val_mean_absolute_error: 0.0811 - val_pearson_correlation_f: 0.9348
Epoch 12/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0078 - mean_absolute_error: 0.0679 - pearson_correlation_f: 0.9595 - val_loss: 0.0118 - val_mean_absolute_error: 0.0814 - val_pearson_correlation_f: 0.9341
Epoch 13/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0075 - mean_absolute_error: 0.0665 - pearson_correlation_f: 0.9611 - val_loss: 0.0121 - val_mean_absolute_error: 0.0813 - val_pearson_correlation_f: 0.9338
Epoch 14/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0072 - mean_absolute_error: 0.0653 - pearson_correlation_f: 0.9626 - val_loss: 0.0131 - val_mean_absolute_error: 0.0841 - val_pearson_correlation_f: 0.9326
Epoch 15/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0070 - mean_absolute_error: 0.0642 - pearson_correlation_f: 0.9640 - val_loss: 0.0145 - val_mean_absolute_error: 0.0876 - val_pearson_correlation_f: 0.9243
Epoch 16/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0067 - mean_absolute_error: 0.0630 - pearson_correlation_f: 0.9652 - val_loss: 0.0121 - val_mean_absolute_error: 0.0815 - val_pearson_correlation_f: 0.9327
Epoch 17/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0065 - mean_absolute_error: 0.0619 - pearson_correlation_f: 0.9667 - val_loss: 0.0135 - val_mean_absolute_error: 0.0891 - val_pearson_correlation_f: 0.9317
Epoch 18/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0063 - mean_absolute_error: 0.0609 - pearson_correlation_f: 0.9676 - val_loss: 0.0127 - val_mean_absolute_error: 0.0828 - val_pearson_correlation_f: 0.9303
Epoch 19/40
189000/189000 [==============================] - 10s 50us/step - loss: 0.0061 - mean_absolute_error: 0.0600 - pearson_correlation_f: 0.9688 - val_loss: 0.0132 - val_mean_absolute_error: 0.0874 - val_pearson_correlation_f: 0.9315
Epoch 20/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0059 - mean_absolute_error: 0.0593 - pearson_correlation_f: 0.9697 - val_loss: 0.0149 - val_mean_absolute_error: 0.0886 - val_pearson_correlation_f: 0.9269
Epoch 21/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0057 - mean_absolute_error: 0.0583 - pearson_correlation_f: 0.9706 - val_loss: 0.0127 - val_mean_absolute_error: 0.0832 - val_pearson_correlation_f: 0.9282
Epoch 22/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0056 - mean_absolute_error: 0.0576 - pearson_correlation_f: 0.9714 - val_loss: 0.0126 - val_mean_absolute_error: 0.0815 - val_pearson_correlation_f: 0.9311
Epoch 23/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0054 - mean_absolute_error: 0.0568 - pearson_correlation_f: 0.9722 - val_loss: 0.0135 - val_mean_absolute_error: 0.0865 - val_pearson_correlation_f: 0.9282
Epoch 24/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0053 - mean_absolute_error: 0.0561 - pearson_correlation_f: 0.9729 - val_loss: 0.0131 - val_mean_absolute_error: 0.0845 - val_pearson_correlation_f: 0.9278
Epoch 25/40
189000/189000 [==============================] - 9s 49us/step - loss: 0.0052 - mean_absolute_error: 0.0554 - pearson_correlation_f: 0.9736 - val_loss: 0.0145 - val_mean_absolute_error: 0.0928 - val_pearson_correlation_f: 0.9268
Epoch 26/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0050 - mean_absolute_error: 0.0548 - pearson_correlation_f: 0.9743 - val_loss: 0.0133 - val_mean_absolute_error: 0.0857 - val_pearson_correlation_f: 0.9273
Epoch 27/40
189000/189000 [==============================] - 10s 50us/step - loss: 0.0049 - mean_absolute_error: 0.0541 - pearson_correlation_f: 0.9750 - val_loss: 0.0137 - val_mean_absolute_error: 0.0855 - val_pearson_correlation_f: 0.9264
Epoch 28/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0048 - mean_absolute_error: 0.0538 - pearson_correlation_f: 0.9754 - val_loss: 0.0141 - val_mean_absolute_error: 0.0861 - val_pearson_correlation_f: 0.9271
Epoch 29/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0047 - mean_absolute_error: 0.0531 - pearson_correlation_f: 0.9760 - val_loss: 0.0135 - val_mean_absolute_error: 0.0848 - val_pearson_correlation_f: 0.9252
Epoch 30/40
189000/189000 [==============================] - 10s 50us/step - loss: 0.0046 - mean_absolute_error: 0.0527 - pearson_correlation_f: 0.9764 - val_loss: 0.0138 - val_mean_absolute_error: 0.0879 - val_pearson_correlation_f: 0.9259
Epoch 31/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0045 - mean_absolute_error: 0.0520 - pearson_correlation_f: 0.9771 - val_loss: 0.0133 - val_mean_absolute_error: 0.0839 - val_pearson_correlation_f: 0.9268
Epoch 32/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0044 - mean_absolute_error: 0.0515 - pearson_correlation_f: 0.9775 - val_loss: 0.0135 - val_mean_absolute_error: 0.0864 - val_pearson_correlation_f: 0.9254
Epoch 33/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0043 - mean_absolute_error: 0.0510 - pearson_correlation_f: 0.9780 - val_loss: 0.0134 - val_mean_absolute_error: 0.0847 - val_pearson_correlation_f: 0.9258
Epoch 34/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0043 - mean_absolute_error: 0.0507 - pearson_correlation_f: 0.9783 - val_loss: 0.0134 - val_mean_absolute_error: 0.0844 - val_pearson_correlation_f: 0.9249
Epoch 35/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0042 - mean_absolute_error: 0.0502 - pearson_correlation_f: 0.9788 - val_loss: 0.0138 - val_mean_absolute_error: 0.0860 - val_pearson_correlation_f: 0.9262
Epoch 36/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0041 - mean_absolute_error: 0.0497 - pearson_correlation_f: 0.9791 - val_loss: 0.0137 - val_mean_absolute_error: 0.0850 - val_pearson_correlation_f: 0.9246
Epoch 37/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0041 - mean_absolute_error: 0.0494 - pearson_correlation_f: 0.9794 - val_loss: 0.0142 - val_mean_absolute_error: 0.0873 - val_pearson_correlation_f: 0.9255
Epoch 38/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0040 - mean_absolute_error: 0.0490 - pearson_correlation_f: 0.9798 - val_loss: 0.0136 - val_mean_absolute_error: 0.0848 - val_pearson_correlation_f: 0.9253
Epoch 39/40
189000/189000 [==============================] - 9s 50us/step - loss: 0.0039 - mean_absolute_error: 0.0487 - pearson_correlation_f: 0.9802 - val_loss: 0.0141 - val_mean_absolute_error: 0.0859 - val_pearson_correlation_f: 0.9240
Epoch 40/40
189000/189000 [==============================] - 10s 50us/step - loss: 0.0039 - mean_absolute_error: 0.0483 - pearson_correlation_f: 0.9805 - val_loss: 0.0147 - val_mean_absolute_error: 0.0894 - val_pearson_correlation_f: 0.9256
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 100)          16428100  
_________________________________________________________________
conv1d_16 (Conv1D)           (None, 126, 128)          38528     
_________________________________________________________________
max_pooling1d_11 (MaxPooling (None, 25, 128)           0         
_________________________________________________________________
conv1d_17 (Conv1D)           (None, 23, 128)           49280     
_________________________________________________________________
max_pooling1d_12 (MaxPooling (None, 4, 128)            0         
_________________________________________________________________
conv1d_18 (Conv1D)           (None, 2, 128)            49280     
_________________________________________________________________
global_max_pooling1d_3 (Glob (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 129       
=================================================================
Total params: 16,581,829
Trainable params: 153,729
Non-trainable params: 16,428,100
_________________________________________________________________

* <2018-11-30 Fri> UAE sentence encoder
In [3]: data = prepare_data_using_use()
loading USE preprocessed stories ..
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [4]: (x_train, y_train), (x_val, y_val) = data
x_train.shape

In [5]: Out[5]: (125515, 13, 512)

In [6]: y_train.shape
x_val.shape
Out[6]: (125515,)

In [7]: Out[7]: (13946, 13, 512)

In [8]: y_val.shape
Out[8]: (13946,)

In [9]: model = build_uae_model()

In [10]: train_model(model, data)
Train on 125515 samples, validate on 13946 samples
Epoch 1/40
2018-11-30 15:29:25.732462: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 15:29:25.865841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 15:29:25.866283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.58GiB
2018-11-30 15:29:25.866299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 15:29:26.649993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 15:29:26.650034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 15:29:26.650043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 15:29:26.650619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2275 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
125515/125515 [==============================] - 7s 54us/step - loss: 0.0603 - mean_absolute_error: 0.1900 - pearson_correlation_f: 0.6197 - val_loss: 0.0338 - val_mean_absolute_error: 0.1440 - val_pearson_correlation_f: 0.8067
Epoch 2/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0300 - mean_absolute_error: 0.1341 - pearson_correlation_f: 0.8289 - val_loss: 0.0300 - val_mean_absolute_error: 0.1352 - val_pearson_correlation_f: 0.8340
Epoch 3/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0260 - mean_absolute_error: 0.1241 - pearson_correlation_f: 0.8523 - val_loss: 0.0262 - val_mean_absolute_error: 0.1261 - val_pearson_correlation_f: 0.8462
Epoch 4/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0236 - mean_absolute_error: 0.1176 - pearson_correlation_f: 0.8668 - val_loss: 0.0255 - val_mean_absolute_error: 0.1208 - val_pearson_correlation_f: 0.8507
Epoch 5/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0217 - mean_absolute_error: 0.1122 - pearson_correlation_f: 0.8777 - val_loss: 0.0237 - val_mean_absolute_error: 0.1166 - val_pearson_correlation_f: 0.8608
Epoch 6/40
125515/125515 [==============================] - 5s 39us/step - loss: 0.0202 - mean_absolute_error: 0.1080 - pearson_correlation_f: 0.8862 - val_loss: 0.0305 - val_mean_absolute_error: 0.1308 - val_pearson_correlation_f: 0.8593
Epoch 7/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0190 - mean_absolute_error: 0.1045 - pearson_correlation_f: 0.8937 - val_loss: 0.0227 - val_mean_absolute_error: 0.1133 - val_pearson_correlation_f: 0.8684
Epoch 8/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0179 - mean_absolute_error: 0.1011 - pearson_correlation_f: 0.8995 - val_loss: 0.0236 - val_mean_absolute_error: 0.1136 - val_pearson_correlation_f: 0.8682
Epoch 9/40
125515/125515 [==============================] - 5s 39us/step - loss: 0.0170 - mean_absolute_error: 0.0986 - pearson_correlation_f: 0.9049 - val_loss: 0.0223 - val_mean_absolute_error: 0.1106 - val_pearson_correlation_f: 0.8728
Epoch 10/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0161 - mean_absolute_error: 0.0956 - pearson_correlation_f: 0.9099 - val_loss: 0.0219 - val_mean_absolute_error: 0.1101 - val_pearson_correlation_f: 0.8721
Epoch 11/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0154 - mean_absolute_error: 0.0936 - pearson_correlation_f: 0.9139 - val_loss: 0.0220 - val_mean_absolute_error: 0.1089 - val_pearson_correlation_f: 0.8757
Epoch 12/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0148 - mean_absolute_error: 0.0916 - pearson_correlation_f: 0.9175 - val_loss: 0.0241 - val_mean_absolute_error: 0.1130 - val_pearson_correlation_f: 0.8743
Epoch 13/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0142 - mean_absolute_error: 0.0895 - pearson_correlation_f: 0.9210 - val_loss: 0.0230 - val_mean_absolute_error: 0.1114 - val_pearson_correlation_f: 0.8772
Epoch 14/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0137 - mean_absolute_error: 0.0880 - pearson_correlation_f: 0.9237 - val_loss: 0.0215 - val_mean_absolute_error: 0.1079 - val_pearson_correlation_f: 0.8781
Epoch 15/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0133 - mean_absolute_error: 0.0863 - pearson_correlation_f: 0.9264 - val_loss: 0.0207 - val_mean_absolute_error: 0.1069 - val_pearson_correlation_f: 0.8811
Epoch 16/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0129 - mean_absolute_error: 0.0850 - pearson_correlation_f: 0.9286 - val_loss: 0.0209 - val_mean_absolute_error: 0.1062 - val_pearson_correlation_f: 0.8800
Epoch 17/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0124 - mean_absolute_error: 0.0835 - pearson_correlation_f: 0.9311 - val_loss: 0.0214 - val_mean_absolute_error: 0.1060 - val_pearson_correlation_f: 0.8766
Epoch 18/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0121 - mean_absolute_error: 0.0822 - pearson_correlation_f: 0.9331 - val_loss: 0.0216 - val_mean_absolute_error: 0.1057 - val_pearson_correlation_f: 0.8801
Epoch 19/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0117 - mean_absolute_error: 0.0809 - pearson_correlation_f: 0.9351 - val_loss: 0.0215 - val_mean_absolute_error: 0.1085 - val_pearson_correlation_f: 0.8761
Epoch 20/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0114 - mean_absolute_error: 0.0798 - pearson_correlation_f: 0.9370 - val_loss: 0.0219 - val_mean_absolute_error: 0.1113 - val_pearson_correlation_f: 0.8779
Epoch 21/40
125515/125515 [==============================] - 5s 39us/step - loss: 0.0111 - mean_absolute_error: 0.0787 - pearson_correlation_f: 0.9386 - val_loss: 0.0204 - val_mean_absolute_error: 0.1043 - val_pearson_correlation_f: 0.8812
Epoch 22/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0109 - mean_absolute_error: 0.0778 - pearson_correlation_f: 0.9401 - val_loss: 0.0205 - val_mean_absolute_error: 0.1046 - val_pearson_correlation_f: 0.8826
Epoch 23/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0106 - mean_absolute_error: 0.0767 - pearson_correlation_f: 0.9419 - val_loss: 0.0220 - val_mean_absolute_error: 0.1065 - val_pearson_correlation_f: 0.8821
Epoch 24/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0103 - mean_absolute_error: 0.0758 - pearson_correlation_f: 0.9433 - val_loss: 0.0219 - val_mean_absolute_error: 0.1072 - val_pearson_correlation_f: 0.8792
Epoch 25/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0101 - mean_absolute_error: 0.0747 - pearson_correlation_f: 0.9446 - val_loss: 0.0209 - val_mean_absolute_error: 0.1044 - val_pearson_correlation_f: 0.8812
Epoch 26/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0098 - mean_absolute_error: 0.0739 - pearson_correlation_f: 0.9460 - val_loss: 0.0211 - val_mean_absolute_error: 0.1042 - val_pearson_correlation_f: 0.8795
Epoch 27/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0096 - mean_absolute_error: 0.0729 - pearson_correlation_f: 0.9472 - val_loss: 0.0212 - val_mean_absolute_error: 0.1083 - val_pearson_correlation_f: 0.8787
Epoch 28/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0094 - mean_absolute_error: 0.0722 - pearson_correlation_f: 0.9485 - val_loss: 0.0209 - val_mean_absolute_error: 0.1038 - val_pearson_correlation_f: 0.8821
Epoch 29/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0092 - mean_absolute_error: 0.0713 - pearson_correlation_f: 0.9495 - val_loss: 0.0215 - val_mean_absolute_error: 0.1055 - val_pearson_correlation_f: 0.8806
Epoch 30/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0090 - mean_absolute_error: 0.0705 - pearson_correlation_f: 0.9506 - val_loss: 0.0223 - val_mean_absolute_error: 0.1082 - val_pearson_correlation_f: 0.8828
Epoch 31/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0088 - mean_absolute_error: 0.0698 - pearson_correlation_f: 0.9517 - val_loss: 0.0214 - val_mean_absolute_error: 0.1044 - val_pearson_correlation_f: 0.8814
Epoch 32/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0086 - mean_absolute_error: 0.0691 - pearson_correlation_f: 0.9528 - val_loss: 0.0234 - val_mean_absolute_error: 0.1100 - val_pearson_correlation_f: 0.8792
Epoch 33/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0085 - mean_absolute_error: 0.0685 - pearson_correlation_f: 0.9535 - val_loss: 0.0213 - val_mean_absolute_error: 0.1041 - val_pearson_correlation_f: 0.8788
Epoch 34/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0083 - mean_absolute_error: 0.0677 - pearson_correlation_f: 0.9544 - val_loss: 0.0208 - val_mean_absolute_error: 0.1036 - val_pearson_correlation_f: 0.8813
Epoch 35/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0082 - mean_absolute_error: 0.0671 - pearson_correlation_f: 0.9554 - val_loss: 0.0213 - val_mean_absolute_error: 0.1048 - val_pearson_correlation_f: 0.8796
Epoch 36/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0080 - mean_absolute_error: 0.0666 - pearson_correlation_f: 0.9564 - val_loss: 0.0211 - val_mean_absolute_error: 0.1046 - val_pearson_correlation_f: 0.8791
Epoch 37/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0078 - mean_absolute_error: 0.0658 - pearson_correlation_f: 0.9572 - val_loss: 0.0223 - val_mean_absolute_error: 0.1058 - val_pearson_correlation_f: 0.8795
Epoch 38/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0077 - mean_absolute_error: 0.0652 - pearson_correlation_f: 0.9580 - val_loss: 0.0208 - val_mean_absolute_error: 0.1034 - val_pearson_correlation_f: 0.8816
Epoch 39/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0076 - mean_absolute_error: 0.0647 - pearson_correlation_f: 0.9587 - val_loss: 0.0213 - val_mean_absolute_error: 0.1049 - val_pearson_correlation_f: 0.8779
Epoch 40/40
125515/125515 [==============================] - 5s 38us/step - loss: 0.0074 - mean_absolute_error: 0.0640 - pearson_correlation_f: 0.9595 - val_loss: 0.0214 - val_mean_absolute_error: 0.1052 - val_pearson_correlation_f: 0.8785
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________

* <2018-11-30 Fri> Glove 1000 sample, article based splitting

In [4]: tokenizer = prepare_tokenizer(articles + summaries)

In [5]: data = prepare_data_using_tokenizer(articles, summaries, scores,
   ...:                                     tokenizer)
article texts to sequences ..
padding ..
summary texts to sequences ..
padding ..
concatenating ..
splitting ..

In [6]: (x_train, y_train), (x_val, y_val) = data

In [7]: x_train.shape
Out[7]: (18900, 640)

In [8]: y_train.shape
Out[8]: (18900,)

In [9]: x_val.shape
Out[9]: (2100, 640)

In [10]: y_val.shape
Out[10]: (2100,)

In [11]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [12]: model = build_glove_model(embedding_layer)

In [13]: train_model(model, data)
Train on 18900 samples, validate on 2100 samples
Epoch 1/40
2018-11-30 18:14:18.437121: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 18:14:18.537290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 18:14:18.537735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.56GiB
2018-11-30 18:14:18.537752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 18:14:18.712242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 18:14:18.712278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 18:14:18.712284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 18:14:18.712431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2258 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
18816/18900 [============================>.] - ETA: 0s - loss: 0.1286 - mean_absolute_error: 0.2852 - pearson_correlation_f: 0.42992018-11-30 18:14:23.062868: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.36GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 18:14:23.083772: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.67GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 18:14:23.299442: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18900/18900 [==============================] - 5s 258us/step - loss: 0.1287 - mean_absolute_error: 0.2853 - pearson_correlation_f: 0.4311 - val_loss: 0.0514 - val_mean_absolute_error: 0.1889 - val_pearson_correlation_f: 0.7057
Epoch 2/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0431 - mean_absolute_error: 0.1642 - pearson_correlation_f: 0.8371 - val_loss: 0.0251 - val_mean_absolute_error: 0.1234 - val_pearson_correlation_f: 0.8635
Epoch 3/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0239 - mean_absolute_error: 0.1215 - pearson_correlation_f: 0.9085 - val_loss: 0.0247 - val_mean_absolute_error: 0.1246 - val_pearson_correlation_f: 0.8768
Epoch 4/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0175 - mean_absolute_error: 0.1038 - pearson_correlation_f: 0.9343 - val_loss: 0.0233 - val_mean_absolute_error: 0.1175 - val_pearson_correlation_f: 0.8771
Epoch 5/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0131 - mean_absolute_error: 0.0892 - pearson_correlation_f: 0.9511 - val_loss: 0.0218 - val_mean_absolute_error: 0.1144 - val_pearson_correlation_f: 0.8746
Epoch 6/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0104 - mean_absolute_error: 0.0795 - pearson_correlation_f: 0.9625 - val_loss: 0.0345 - val_mean_absolute_error: 0.1447 - val_pearson_correlation_f: 0.8658
Epoch 7/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0083 - mean_absolute_error: 0.0712 - pearson_correlation_f: 0.9700 - val_loss: 0.0318 - val_mean_absolute_error: 0.1406 - val_pearson_correlation_f: 0.8670
Epoch 8/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0070 - mean_absolute_error: 0.0651 - pearson_correlation_f: 0.9743 - val_loss: 0.0231 - val_mean_absolute_error: 0.1142 - val_pearson_correlation_f: 0.8722
Epoch 9/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0061 - mean_absolute_error: 0.0606 - pearson_correlation_f: 0.9779 - val_loss: 0.0238 - val_mean_absolute_error: 0.1171 - val_pearson_correlation_f: 0.8649
Epoch 10/40
18900/18900 [==============================] - 3s 181us/step - loss: 0.0053 - mean_absolute_error: 0.0566 - pearson_correlation_f: 0.9803 - val_loss: 0.0241 - val_mean_absolute_error: 0.1184 - val_pearson_correlation_f: 0.8669
Epoch 11/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0048 - mean_absolute_error: 0.0534 - pearson_correlation_f: 0.9820 - val_loss: 0.0254 - val_mean_absolute_error: 0.1238 - val_pearson_correlation_f: 0.8761
Epoch 12/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0043 - mean_absolute_error: 0.0511 - pearson_correlation_f: 0.9840 - val_loss: 0.0259 - val_mean_absolute_error: 0.1237 - val_pearson_correlation_f: 0.8654
Epoch 13/40
18900/18900 [==============================] - 3s 182us/step - loss: 0.0038 - mean_absolute_error: 0.0480 - pearson_correlation_f: 0.9856 - val_loss: 0.0233 - val_mean_absolute_error: 0.1164 - val_pearson_correlation_f: 0.8657
Epoch 14/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0036 - mean_absolute_error: 0.0466 - pearson_correlation_f: 0.9863 - val_loss: 0.0235 - val_mean_absolute_error: 0.1175 - val_pearson_correlation_f: 0.8697
Epoch 15/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0034 - mean_absolute_error: 0.0454 - pearson_correlation_f: 0.9870 - val_loss: 0.0228 - val_mean_absolute_error: 0.1159 - val_pearson_correlation_f: 0.8707
Epoch 16/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0032 - mean_absolute_error: 0.0438 - pearson_correlation_f: 0.9882 - val_loss: 0.0230 - val_mean_absolute_error: 0.1160 - val_pearson_correlation_f: 0.8701
Epoch 17/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0030 - mean_absolute_error: 0.0423 - pearson_correlation_f: 0.9886 - val_loss: 0.0231 - val_mean_absolute_error: 0.1156 - val_pearson_correlation_f: 0.8674
Epoch 18/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0028 - mean_absolute_error: 0.0411 - pearson_correlation_f: 0.9894 - val_loss: 0.0231 - val_mean_absolute_error: 0.1156 - val_pearson_correlation_f: 0.8688
Epoch 19/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0027 - mean_absolute_error: 0.0399 - pearson_correlation_f: 0.9899 - val_loss: 0.0235 - val_mean_absolute_error: 0.1167 - val_pearson_correlation_f: 0.8726
Epoch 20/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0025 - mean_absolute_error: 0.0386 - pearson_correlation_f: 0.9905 - val_loss: 0.0224 - val_mean_absolute_error: 0.1145 - val_pearson_correlation_f: 0.8707
Epoch 21/40
18900/18900 [==============================] - 3s 183us/step - loss: 0.0024 - mean_absolute_error: 0.0376 - pearson_correlation_f: 0.9910 - val_loss: 0.0255 - val_mean_absolute_error: 0.1222 - val_pearson_correlation_f: 0.8626
Epoch 22/40
18900/18900 [==============================] - 3s 184us/step - loss: 0.0023 - mean_absolute_error: 0.0369 - pearson_correlation_f: 0.9912 - val_loss: 0.0263 - val_mean_absolute_error: 0.1247 - val_pearson_correlation_f: 0.8611
Epoch 23/40
18900/18900 [==============================] - 4s 186us/step - loss: 0.0022 - mean_absolute_error: 0.0361 - pearson_correlation_f: 0.9917 - val_loss: 0.0276 - val_mean_absolute_error: 0.1278 - val_pearson_correlation_f: 0.8591
Epoch 24/40
18900/18900 [==============================] - 4s 186us/step - loss: 0.0022 - mean_absolute_error: 0.0360 - pearson_correlation_f: 0.9917 - val_loss: 0.0250 - val_mean_absolute_error: 0.1194 - val_pearson_correlation_f: 0.8558
Epoch 25/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0021 - mean_absolute_error: 0.0352 - pearson_correlation_f: 0.9922 - val_loss: 0.0240 - val_mean_absolute_error: 0.1186 - val_pearson_correlation_f: 0.8629
Epoch 26/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0020 - mean_absolute_error: 0.0340 - pearson_correlation_f: 0.9924 - val_loss: 0.0245 - val_mean_absolute_error: 0.1198 - val_pearson_correlation_f: 0.8647
Epoch 27/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0019 - mean_absolute_error: 0.0335 - pearson_correlation_f: 0.9928 - val_loss: 0.0260 - val_mean_absolute_error: 0.1253 - val_pearson_correlation_f: 0.8662
Epoch 28/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0018 - mean_absolute_error: 0.0333 - pearson_correlation_f: 0.9929 - val_loss: 0.0239 - val_mean_absolute_error: 0.1191 - val_pearson_correlation_f: 0.8614
Epoch 29/40
18900/18900 [==============================] - 4s 190us/step - loss: 0.0018 - mean_absolute_error: 0.0327 - pearson_correlation_f: 0.9931 - val_loss: 0.0233 - val_mean_absolute_error: 0.1160 - val_pearson_correlation_f: 0.8654
Epoch 30/40
18900/18900 [==============================] - 4s 191us/step - loss: 0.0017 - mean_absolute_error: 0.0320 - pearson_correlation_f: 0.9934 - val_loss: 0.0251 - val_mean_absolute_error: 0.1214 - val_pearson_correlation_f: 0.8696
Epoch 31/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0017 - mean_absolute_error: 0.0318 - pearson_correlation_f: 0.9936 - val_loss: 0.0231 - val_mean_absolute_error: 0.1163 - val_pearson_correlation_f: 0.8680
Epoch 32/40
18900/18900 [==============================] - 4s 196us/step - loss: 0.0016 - mean_absolute_error: 0.0310 - pearson_correlation_f: 0.9937 - val_loss: 0.0237 - val_mean_absolute_error: 0.1154 - val_pearson_correlation_f: 0.8620
Epoch 33/40
18900/18900 [==============================] - 4s 193us/step - loss: 0.0016 - mean_absolute_error: 0.0303 - pearson_correlation_f: 0.9940 - val_loss: 0.0241 - val_mean_absolute_error: 0.1183 - val_pearson_correlation_f: 0.8633
Epoch 34/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0015 - mean_absolute_error: 0.0296 - pearson_correlation_f: 0.9942 - val_loss: 0.0254 - val_mean_absolute_error: 0.1213 - val_pearson_correlation_f: 0.8623
Epoch 35/40
18900/18900 [==============================] - 4s 191us/step - loss: 0.0015 - mean_absolute_error: 0.0299 - pearson_correlation_f: 0.9943 - val_loss: 0.0239 - val_mean_absolute_error: 0.1163 - val_pearson_correlation_f: 0.8616
Epoch 36/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0014 - mean_absolute_error: 0.0289 - pearson_correlation_f: 0.9946 - val_loss: 0.0275 - val_mean_absolute_error: 0.1288 - val_pearson_correlation_f: 0.8642
Epoch 37/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0014 - mean_absolute_error: 0.0286 - pearson_correlation_f: 0.9946 - val_loss: 0.0239 - val_mean_absolute_error: 0.1174 - val_pearson_correlation_f: 0.8657
Epoch 38/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0014 - mean_absolute_error: 0.0285 - pearson_correlation_f: 0.9947 - val_loss: 0.0244 - val_mean_absolute_error: 0.1178 - val_pearson_correlation_f: 0.8582
Epoch 39/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0013 - mean_absolute_error: 0.0280 - pearson_correlation_f: 0.9949 - val_loss: 0.0251 - val_mean_absolute_error: 0.1209 - val_pearson_correlation_f: 0.8622
Epoch 40/40
18900/18900 [==============================] - 4s 192us/step - loss: 0.0013 - mean_absolute_error: 0.0280 - pearson_correlation_f: 0.9951 - val_loss: 0.0274 - val_mean_absolute_error: 0.1264 - val_pearson_correlation_f: 0.8598
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          10782500  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 11,027,365
Trainable params: 244,865
Non-trainable params: 10,782,500
_________________________________________________________________

* <2018-11-30 Fri> Glove summary, 1000 sample, article based split
In [6]: (x_train, y_train), (x_val, y_val) = data

In [7]: x_train.shape
Out[7]: (18900, 128)

In [8]: y_train.shape
Out[8]: (18900,)

In [9]: x_val.shape
Out[9]: (2100, 128)

In [10]: y_val.shape
Out[10]: (2100,)

In [11]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [12]: model = build_glove_summary_only_model(embedding_layer)

In [13]: train_model(model, data)
Train on 18900 samples, validate on 2100 samples
Epoch 1/40
2018-11-30 23:00:16.267311: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 23:00:16.365583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 23:00:16.366021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.63GiB
2018-11-30 23:00:16.366037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 23:00:16.538430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 23:00:16.538474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 23:00:16.538479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 23:00:16.538651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2330 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2018-11-30 23:00:17.510656: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 23:00:17.542535: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.62GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18304/18900 [============================>.] - ETA: 0s - loss: 0.0835 - mean_absolute_error: 0.2124 - pearson_correlation_f: 0.70502018-11-30 23:00:18.505189: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 23:00:18.527616: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.38GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-30 23:00:18.598738: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.23GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18900/18900 [==============================] - 2s 124us/step - loss: 0.0821 - mean_absolute_error: 0.2105 - pearson_correlation_f: 0.7083 - val_loss: 0.0568 - val_mean_absolute_error: 0.1814 - val_pearson_correlation_f: 0.7899
Epoch 2/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0355 - mean_absolute_error: 0.1448 - pearson_correlation_f: 0.8604 - val_loss: 0.0203 - val_mean_absolute_error: 0.1100 - val_pearson_correlation_f: 0.8838
Epoch 3/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0240 - mean_absolute_error: 0.1200 - pearson_correlation_f: 0.9027 - val_loss: 0.0235 - val_mean_absolute_error: 0.1179 - val_pearson_correlation_f: 0.8843
Epoch 4/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0188 - mean_absolute_error: 0.1063 - pearson_correlation_f: 0.9233 - val_loss: 0.0229 - val_mean_absolute_error: 0.1174 - val_pearson_correlation_f: 0.8867
Epoch 5/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0154 - mean_absolute_error: 0.0961 - pearson_correlation_f: 0.9378 - val_loss: 0.0267 - val_mean_absolute_error: 0.1266 - val_pearson_correlation_f: 0.8849
Epoch 6/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0131 - mean_absolute_error: 0.0889 - pearson_correlation_f: 0.9468 - val_loss: 0.0278 - val_mean_absolute_error: 0.1279 - val_pearson_correlation_f: 0.8735
Epoch 7/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0111 - mean_absolute_error: 0.0819 - pearson_correlation_f: 0.9562 - val_loss: 0.0221 - val_mean_absolute_error: 0.1132 - val_pearson_correlation_f: 0.8802
Epoch 8/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0095 - mean_absolute_error: 0.0762 - pearson_correlation_f: 0.9640 - val_loss: 0.0235 - val_mean_absolute_error: 0.1160 - val_pearson_correlation_f: 0.8728
Epoch 9/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0082 - mean_absolute_error: 0.0704 - pearson_correlation_f: 0.9692 - val_loss: 0.0219 - val_mean_absolute_error: 0.1112 - val_pearson_correlation_f: 0.8728
Epoch 10/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0073 - mean_absolute_error: 0.0668 - pearson_correlation_f: 0.9732 - val_loss: 0.0285 - val_mean_absolute_error: 0.1305 - val_pearson_correlation_f: 0.8750
Epoch 11/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0063 - mean_absolute_error: 0.0626 - pearson_correlation_f: 0.9775 - val_loss: 0.0216 - val_mean_absolute_error: 0.1109 - val_pearson_correlation_f: 0.8762
Epoch 12/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0058 - mean_absolute_error: 0.0599 - pearson_correlation_f: 0.9793 - val_loss: 0.0429 - val_mean_absolute_error: 0.1641 - val_pearson_correlation_f: 0.8661
Epoch 13/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0053 - mean_absolute_error: 0.0575 - pearson_correlation_f: 0.9818 - val_loss: 0.0250 - val_mean_absolute_error: 0.1220 - val_pearson_correlation_f: 0.8672
Epoch 14/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0046 - mean_absolute_error: 0.0533 - pearson_correlation_f: 0.9833 - val_loss: 0.0240 - val_mean_absolute_error: 0.1181 - val_pearson_correlation_f: 0.8670
Epoch 15/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0045 - mean_absolute_error: 0.0527 - pearson_correlation_f: 0.9842 - val_loss: 0.0245 - val_mean_absolute_error: 0.1169 - val_pearson_correlation_f: 0.8593
Epoch 16/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0043 - mean_absolute_error: 0.0513 - pearson_correlation_f: 0.9855 - val_loss: 0.0283 - val_mean_absolute_error: 0.1293 - val_pearson_correlation_f: 0.8686
Epoch 17/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0039 - mean_absolute_error: 0.0493 - pearson_correlation_f: 0.9864 - val_loss: 0.0226 - val_mean_absolute_error: 0.1143 - val_pearson_correlation_f: 0.8696
Epoch 18/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0037 - mean_absolute_error: 0.0480 - pearson_correlation_f: 0.9873 - val_loss: 0.0225 - val_mean_absolute_error: 0.1129 - val_pearson_correlation_f: 0.8701
Epoch 19/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0035 - mean_absolute_error: 0.0470 - pearson_correlation_f: 0.9878 - val_loss: 0.0254 - val_mean_absolute_error: 0.1217 - val_pearson_correlation_f: 0.8648
Epoch 20/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0033 - mean_absolute_error: 0.0452 - pearson_correlation_f: 0.9885 - val_loss: 0.0335 - val_mean_absolute_error: 0.1438 - val_pearson_correlation_f: 0.8608
Epoch 21/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0031 - mean_absolute_error: 0.0435 - pearson_correlation_f: 0.9894 - val_loss: 0.0234 - val_mean_absolute_error: 0.1160 - val_pearson_correlation_f: 0.8640
Epoch 22/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0029 - mean_absolute_error: 0.0424 - pearson_correlation_f: 0.9897 - val_loss: 0.0269 - val_mean_absolute_error: 0.1257 - val_pearson_correlation_f: 0.8607
Epoch 23/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0028 - mean_absolute_error: 0.0414 - pearson_correlation_f: 0.9905 - val_loss: 0.0241 - val_mean_absolute_error: 0.1165 - val_pearson_correlation_f: 0.8615
Epoch 24/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0027 - mean_absolute_error: 0.0408 - pearson_correlation_f: 0.9906 - val_loss: 0.0260 - val_mean_absolute_error: 0.1242 - val_pearson_correlation_f: 0.8677
Epoch 25/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0025 - mean_absolute_error: 0.0397 - pearson_correlation_f: 0.9911 - val_loss: 0.0274 - val_mean_absolute_error: 0.1273 - val_pearson_correlation_f: 0.8665
Epoch 26/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0026 - mean_absolute_error: 0.0398 - pearson_correlation_f: 0.9912 - val_loss: 0.0256 - val_mean_absolute_error: 0.1204 - val_pearson_correlation_f: 0.8510
Epoch 27/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0024 - mean_absolute_error: 0.0386 - pearson_correlation_f: 0.9915 - val_loss: 0.0292 - val_mean_absolute_error: 0.1318 - val_pearson_correlation_f: 0.8613
Epoch 28/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0023 - mean_absolute_error: 0.0374 - pearson_correlation_f: 0.9919 - val_loss: 0.0232 - val_mean_absolute_error: 0.1153 - val_pearson_correlation_f: 0.8668
Epoch 29/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0022 - mean_absolute_error: 0.0373 - pearson_correlation_f: 0.9923 - val_loss: 0.0237 - val_mean_absolute_error: 0.1157 - val_pearson_correlation_f: 0.8616
Epoch 30/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0022 - mean_absolute_error: 0.0368 - pearson_correlation_f: 0.9924 - val_loss: 0.0244 - val_mean_absolute_error: 0.1182 - val_pearson_correlation_f: 0.8619
Epoch 31/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0021 - mean_absolute_error: 0.0362 - pearson_correlation_f: 0.9925 - val_loss: 0.0245 - val_mean_absolute_error: 0.1190 - val_pearson_correlation_f: 0.8614
Epoch 32/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0021 - mean_absolute_error: 0.0354 - pearson_correlation_f: 0.9928 - val_loss: 0.0240 - val_mean_absolute_error: 0.1177 - val_pearson_correlation_f: 0.8603
Epoch 33/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0020 - mean_absolute_error: 0.0349 - pearson_correlation_f: 0.9930 - val_loss: 0.0249 - val_mean_absolute_error: 0.1212 - val_pearson_correlation_f: 0.8655
Epoch 34/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0019 - mean_absolute_error: 0.0341 - pearson_correlation_f: 0.9934 - val_loss: 0.0250 - val_mean_absolute_error: 0.1178 - val_pearson_correlation_f: 0.8529
Epoch 35/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0018 - mean_absolute_error: 0.0333 - pearson_correlation_f: 0.9934 - val_loss: 0.0257 - val_mean_absolute_error: 0.1226 - val_pearson_correlation_f: 0.8637
Epoch 36/40
18900/18900 [==============================] - 1s 49us/step - loss: 0.0018 - mean_absolute_error: 0.0332 - pearson_correlation_f: 0.9937 - val_loss: 0.0265 - val_mean_absolute_error: 0.1243 - val_pearson_correlation_f: 0.8598
Epoch 37/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0018 - mean_absolute_error: 0.0331 - pearson_correlation_f: 0.9938 - val_loss: 0.0241 - val_mean_absolute_error: 0.1188 - val_pearson_correlation_f: 0.8611
Epoch 38/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0017 - mean_absolute_error: 0.0321 - pearson_correlation_f: 0.9940 - val_loss: 0.0240 - val_mean_absolute_error: 0.1163 - val_pearson_correlation_f: 0.8593
Epoch 39/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0017 - mean_absolute_error: 0.0319 - pearson_correlation_f: 0.9943 - val_loss: 0.0255 - val_mean_absolute_error: 0.1209 - val_pearson_correlation_f: 0.8589
Epoch 40/40
18900/18900 [==============================] - 1s 50us/step - loss: 0.0016 - mean_absolute_error: 0.0314 - pearson_correlation_f: 0.9944 - val_loss: 0.0238 - val_mean_absolute_error: 0.1192 - val_pearson_correlation_f: 0.8635
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 100)          9801100   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 126, 128)          38528     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 25, 128)           0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 23, 128)           49280     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 4, 128)            0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 2, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 9,954,829
Trainable params: 153,729
Non-trainable params: 9,801,100
_________________________________________________________________

* <2018-11-30 Fri> UAE, article based split

In [3]: data = prepare_data_using_use()
loading USE preprocessed stories ..
padding sequence ..
concatenating ..
splitting ..

In [4]: (x_train, y_train), (x_val, y_val) = data

In [5]: x_train.shape
Out[5]: (125515, 13, 512)

In [6]: y_train.shape
Out[6]: (125515,)

In [7]: x_val.shape
Out[7]: (13946, 13, 512)

In [8]: y_val.shape
Out[8]: (13946,)

In [9]: model = build_uae_model()

In [10]: train_model(model, data)
Train on 125515 samples, validate on 13946 samples
Epoch 1/40
2018-11-30 23:06:06.463017: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-30 23:06:06.566541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 23:06:06.567014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.63GiB
2018-11-30 23:06:06.567031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-30 23:06:06.723049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-30 23:06:06.723085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-30 23:06:06.723091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-30 23:06:06.723231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2330 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
125515/125515 [==============================] - 6s 45us/step - loss: 0.0594 - mean_absolute_error: 0.1892 - pearson_correlation_f: 0.6146 - val_loss: 0.0355 - val_mean_absolute_error: 0.1475 - val_pearson_correlation_f: 0.7873
Epoch 2/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0299 - mean_absolute_error: 0.1341 - pearson_correlation_f: 0.8311 - val_loss: 0.0336 - val_mean_absolute_error: 0.1410 - val_pearson_correlation_f: 0.8031
Epoch 3/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0258 - mean_absolute_error: 0.1237 - pearson_correlation_f: 0.8541 - val_loss: 0.0383 - val_mean_absolute_error: 0.1509 - val_pearson_correlation_f: 0.7949
Epoch 4/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0232 - mean_absolute_error: 0.1166 - pearson_correlation_f: 0.8690 - val_loss: 0.0369 - val_mean_absolute_error: 0.1473 - val_pearson_correlation_f: 0.7947
Epoch 5/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0213 - mean_absolute_error: 0.1112 - pearson_correlation_f: 0.8802 - val_loss: 0.0372 - val_mean_absolute_error: 0.1460 - val_pearson_correlation_f: 0.7842
Epoch 6/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0199 - mean_absolute_error: 0.1071 - pearson_correlation_f: 0.8883 - val_loss: 0.0371 - val_mean_absolute_error: 0.1455 - val_pearson_correlation_f: 0.7917
Epoch 7/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0186 - mean_absolute_error: 0.1032 - pearson_correlation_f: 0.8956 - val_loss: 0.0366 - val_mean_absolute_error: 0.1448 - val_pearson_correlation_f: 0.7890
Epoch 8/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0176 - mean_absolute_error: 0.1000 - pearson_correlation_f: 0.9017 - val_loss: 0.0370 - val_mean_absolute_error: 0.1452 - val_pearson_correlation_f: 0.7882
Epoch 9/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0167 - mean_absolute_error: 0.0973 - pearson_correlation_f: 0.9070 - val_loss: 0.0369 - val_mean_absolute_error: 0.1456 - val_pearson_correlation_f: 0.7892
Epoch 10/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0160 - mean_absolute_error: 0.0951 - pearson_correlation_f: 0.9109 - val_loss: 0.0367 - val_mean_absolute_error: 0.1452 - val_pearson_correlation_f: 0.7904
Epoch 11/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0153 - mean_absolute_error: 0.0930 - pearson_correlation_f: 0.9153 - val_loss: 0.0379 - val_mean_absolute_error: 0.1468 - val_pearson_correlation_f: 0.7852
Epoch 12/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0146 - mean_absolute_error: 0.0908 - pearson_correlation_f: 0.9184 - val_loss: 0.0373 - val_mean_absolute_error: 0.1455 - val_pearson_correlation_f: 0.7869
Epoch 13/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0141 - mean_absolute_error: 0.0890 - pearson_correlation_f: 0.9220 - val_loss: 0.0410 - val_mean_absolute_error: 0.1527 - val_pearson_correlation_f: 0.7765
Epoch 14/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0136 - mean_absolute_error: 0.0875 - pearson_correlation_f: 0.9245 - val_loss: 0.0369 - val_mean_absolute_error: 0.1456 - val_pearson_correlation_f: 0.7857
Epoch 15/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0131 - mean_absolute_error: 0.0858 - pearson_correlation_f: 0.9271 - val_loss: 0.0398 - val_mean_absolute_error: 0.1508 - val_pearson_correlation_f: 0.7826
Epoch 16/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0126 - mean_absolute_error: 0.0841 - pearson_correlation_f: 0.9300 - val_loss: 0.0401 - val_mean_absolute_error: 0.1531 - val_pearson_correlation_f: 0.7808
Epoch 17/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0123 - mean_absolute_error: 0.0828 - pearson_correlation_f: 0.9320 - val_loss: 0.0419 - val_mean_absolute_error: 0.1547 - val_pearson_correlation_f: 0.7825
Epoch 18/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0119 - mean_absolute_error: 0.0817 - pearson_correlation_f: 0.9339 - val_loss: 0.0384 - val_mean_absolute_error: 0.1475 - val_pearson_correlation_f: 0.7853
Epoch 19/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0116 - mean_absolute_error: 0.0804 - pearson_correlation_f: 0.9359 - val_loss: 0.0418 - val_mean_absolute_error: 0.1537 - val_pearson_correlation_f: 0.7783
Epoch 20/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0113 - mean_absolute_error: 0.0793 - pearson_correlation_f: 0.9376 - val_loss: 0.0389 - val_mean_absolute_error: 0.1477 - val_pearson_correlation_f: 0.7779
Epoch 21/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0110 - mean_absolute_error: 0.0781 - pearson_correlation_f: 0.9395 - val_loss: 0.0405 - val_mean_absolute_error: 0.1511 - val_pearson_correlation_f: 0.7767
Epoch 22/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0107 - mean_absolute_error: 0.0772 - pearson_correlation_f: 0.9409 - val_loss: 0.0412 - val_mean_absolute_error: 0.1520 - val_pearson_correlation_f: 0.7765
Epoch 23/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0104 - mean_absolute_error: 0.0761 - pearson_correlation_f: 0.9425 - val_loss: 0.0420 - val_mean_absolute_error: 0.1537 - val_pearson_correlation_f: 0.7698
Epoch 24/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0102 - mean_absolute_error: 0.0752 - pearson_correlation_f: 0.9438 - val_loss: 0.0414 - val_mean_absolute_error: 0.1529 - val_pearson_correlation_f: 0.7748
Epoch 25/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0100 - mean_absolute_error: 0.0742 - pearson_correlation_f: 0.9454 - val_loss: 0.0416 - val_mean_absolute_error: 0.1530 - val_pearson_correlation_f: 0.7712
Epoch 26/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0097 - mean_absolute_error: 0.0732 - pearson_correlation_f: 0.9467 - val_loss: 0.0422 - val_mean_absolute_error: 0.1547 - val_pearson_correlation_f: 0.7750
Epoch 27/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0095 - mean_absolute_error: 0.0725 - pearson_correlation_f: 0.9477 - val_loss: 0.0417 - val_mean_absolute_error: 0.1536 - val_pearson_correlation_f: 0.7704
Epoch 28/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0093 - mean_absolute_error: 0.0715 - pearson_correlation_f: 0.9492 - val_loss: 0.0425 - val_mean_absolute_error: 0.1543 - val_pearson_correlation_f: 0.7693
Epoch 29/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0091 - mean_absolute_error: 0.0709 - pearson_correlation_f: 0.9500 - val_loss: 0.0413 - val_mean_absolute_error: 0.1531 - val_pearson_correlation_f: 0.7720
Epoch 30/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0089 - mean_absolute_error: 0.0702 - pearson_correlation_f: 0.9513 - val_loss: 0.0407 - val_mean_absolute_error: 0.1522 - val_pearson_correlation_f: 0.7684
Epoch 31/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0087 - mean_absolute_error: 0.0693 - pearson_correlation_f: 0.9523 - val_loss: 0.0422 - val_mean_absolute_error: 0.1545 - val_pearson_correlation_f: 0.7663
Epoch 32/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0085 - mean_absolute_error: 0.0686 - pearson_correlation_f: 0.9533 - val_loss: 0.0432 - val_mean_absolute_error: 0.1554 - val_pearson_correlation_f: 0.7644
Epoch 33/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0084 - mean_absolute_error: 0.0680 - pearson_correlation_f: 0.9542 - val_loss: 0.0419 - val_mean_absolute_error: 0.1544 - val_pearson_correlation_f: 0.7673
Epoch 34/40
125515/125515 [==============================] - 4s 35us/step - loss: 0.0082 - mean_absolute_error: 0.0675 - pearson_correlation_f: 0.9550 - val_loss: 0.0416 - val_mean_absolute_error: 0.1533 - val_pearson_correlation_f: 0.7700
Epoch 35/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0081 - mean_absolute_error: 0.0667 - pearson_correlation_f: 0.9561 - val_loss: 0.0423 - val_mean_absolute_error: 0.1540 - val_pearson_correlation_f: 0.7654
Epoch 36/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0079 - mean_absolute_error: 0.0660 - pearson_correlation_f: 0.9569 - val_loss: 0.0436 - val_mean_absolute_error: 0.1564 - val_pearson_correlation_f: 0.7582
Epoch 37/40
125515/125515 [==============================] - 5s 37us/step - loss: 0.0077 - mean_absolute_error: 0.0654 - pearson_correlation_f: 0.9577 - val_loss: 0.0420 - val_mean_absolute_error: 0.1542 - val_pearson_correlation_f: 0.7628
Epoch 38/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0076 - mean_absolute_error: 0.0648 - pearson_correlation_f: 0.9586 - val_loss: 0.0431 - val_mean_absolute_error: 0.1557 - val_pearson_correlation_f: 0.7607
Epoch 39/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0075 - mean_absolute_error: 0.0642 - pearson_correlation_f: 0.9593 - val_loss: 0.0431 - val_mean_absolute_error: 0.1560 - val_pearson_correlation_f: 0.7595
Epoch 40/40
125515/125515 [==============================] - 5s 36us/step - loss: 0.0073 - mean_absolute_error: 0.0636 - pearson_correlation_f: 0.9600 - val_loss: 0.0451 - val_mean_absolute_error: 0.1592 - val_pearson_correlation_f: 0.7595
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________


* <2018-12-02 Sun> negative sampling (1 sample), 50000 samplings, binary glove model

#+BEGIN_EXAMPLE
Epoch 12/20
90000/90000 [==============================] - 27s 301us/step - loss: 0.0681 - acc: 0.9757 - pearson_correlation_f: 0.9626 - val_loss: 0.3353 - val_acc: 0.9112 - val_pearson_correlation_f: 0.8492
#+END_EXAMPLE


In [28]: model = build_binary_glove_model(embedding_layer)

In [29]: train_binary_model(model, data)
Train on 90000 samples, validate on 10000 samples
Epoch 1/20
2018-12-02 15:28:39.396719: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-02 15:28:39.529401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-02 15:28:39.529897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.51GiB
2018-12-02 15:28:39.529916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-02 15:28:40.317141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-02 15:28:40.317187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-02 15:28:40.317208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-02 15:28:40.318080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2210 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
89856/90000 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.6339 - pearson_correlation_f: 0.34272018-12-02 15:28:59.204528: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-02 15:28:59.213814: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.03GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
90000/90000 [==============================] - 22s 240us/step - loss: 0.6065 - acc: 0.6340 - pearson_correlation_f: 0.3432 - val_loss: 0.4789 - val_acc: 0.7672 - val_pearson_correlation_f: 0.6089
Epoch 2/20
90000/90000 [==============================] - 26s 285us/step - loss: 0.3733 - acc: 0.8346 - pearson_correlation_f: 0.7331 - val_loss: 0.5224 - val_acc: 0.7640 - val_pearson_correlation_f: 0.6455
Epoch 3/20
90000/90000 [==============================] - 27s 300us/step - loss: 0.2941 - acc: 0.8784 - pearson_correlation_f: 0.8035 - val_loss: 0.4729 - val_acc: 0.7829 - val_pearson_correlation_f: 0.6469
Epoch 4/20
90000/90000 [==============================] - 27s 303us/step - loss: 0.2537 - acc: 0.8987 - pearson_correlation_f: 0.8355 - val_loss: 0.2958 - val_acc: 0.8813 - val_pearson_correlation_f: 0.8058
Epoch 5/20
90000/90000 [==============================] - 27s 305us/step - loss: 0.2154 - acc: 0.9151 - pearson_correlation_f: 0.8653 - val_loss: 0.4473 - val_acc: 0.8158 - val_pearson_correlation_f: 0.7417
Epoch 6/20
90000/90000 [==============================] - 27s 296us/step - loss: 0.1787 - acc: 0.9315 - pearson_correlation_f: 0.8923 - val_loss: 0.4874 - val_acc: 0.8076 - val_pearson_correlation_f: 0.7342
Epoch 7/20
90000/90000 [==============================] - 26s 291us/step - loss: 0.1469 - acc: 0.9443 - pearson_correlation_f: 0.9131 - val_loss: 0.3367 - val_acc: 0.8770 - val_pearson_correlation_f: 0.7939
Epoch 8/20
90000/90000 [==============================] - 27s 302us/step - loss: 0.1230 - acc: 0.9547 - pearson_correlation_f: 0.9292 - val_loss: 0.2898 - val_acc: 0.9026 - val_pearson_correlation_f: 0.8395
Epoch 9/20
90000/90000 [==============================] - 29s 325us/step - loss: 0.1022 - acc: 0.9624 - pearson_correlation_f: 0.9417 - val_loss: 0.3313 - val_acc: 0.9051 - val_pearson_correlation_f: 0.8398
Epoch 10/20
90000/90000 [==============================] - 25s 278us/step - loss: 0.0868 - acc: 0.9677 - pearson_correlation_f: 0.9509 - val_loss: 0.3551 - val_acc: 0.8985 - val_pearson_correlation_f: 0.8268
Epoch 11/20
90000/90000 [==============================] - 27s 304us/step - loss: 0.0753 - acc: 0.9731 - pearson_correlation_f: 0.9581 - val_loss: 0.3836 - val_acc: 0.9047 - val_pearson_correlation_f: 0.8367
Epoch 12/20
90000/90000 [==============================] - 27s 301us/step - loss: 0.0681 - acc: 0.9757 - pearson_correlation_f: 0.9626 - val_loss: 0.3353 - val_acc: 0.9112 - val_pearson_correlation_f: 0.8492
Epoch 13/20
90000/90000 [==============================] - 27s 303us/step - loss: 0.0604 - acc: 0.9784 - pearson_correlation_f: 0.9671 - val_loss: 0.3706 - val_acc: 0.9070 - val_pearson_correlation_f: 0.8377
Epoch 14/20
90000/90000 [==============================] - 27s 304us/step - loss: 0.0566 - acc: 0.9804 - pearson_correlation_f: 0.9696 - val_loss: 0.4112 - val_acc: 0.9081 - val_pearson_correlation_f: 0.8372
Epoch 15/20
90000/90000 [==============================] - 27s 299us/step - loss: 0.0515 - acc: 0.9819 - pearson_correlation_f: 0.9723 - val_loss: 0.5291 - val_acc: 0.8928 - val_pearson_correlation_f: 0.8088
Epoch 16/20
90000/90000 [==============================] - 28s 308us/step - loss: 0.0493 - acc: 0.9833 - pearson_correlation_f: 0.9736 - val_loss: 0.4566 - val_acc: 0.9066 - val_pearson_correlation_f: 0.8352
Epoch 17/20
90000/90000 [==============================] - 28s 309us/step - loss: 0.0441 - acc: 0.9848 - pearson_correlation_f: 0.9762 - val_loss: 0.4413 - val_acc: 0.9069 - val_pearson_correlation_f: 0.8350
Epoch 18/20
90000/90000 [==============================] - 27s 295us/step - loss: 0.0432 - acc: 0.9851 - pearson_correlation_f: 0.9768 - val_loss: 0.4583 - val_acc: 0.9079 - val_pearson_correlation_f: 0.8374
Epoch 19/20
90000/90000 [==============================] - 27s 294us/step - loss: 0.0412 - acc: 0.9859 - pearson_correlation_f: 0.9780 - val_loss: 0.5008 - val_acc: 0.9085 - val_pearson_correlation_f: 0.8351
Epoch 20/20
90000/90000 [==============================] - 24s 269us/step - loss: 0.0410 - acc: 0.9861 - pearson_correlation_f: 0.9783 - val_loss: 0.4883 - val_acc: 0.9096 - val_pearson_correlation_f: 0.8386
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          18154000  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 18,398,865
Trainable params: 244,865
Non-trainable params: 18,154,000
_________________________________________________________________

* <2018-12-02 Sun> negative sampling (1 sample), 10000 samplings, binary glove model
In [15]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [16]: articles, summaries, labels = res

In [17]: articles.shape
Out[17]: (20000,)

In [18]: summaries.shape
Out[18]: (20000,)

In [19]: labels.shape
Out[19]: (20000,)

In [20]: group = fake_summaries.shape[1] + 1

In [21]: data = prepare_data_using_tokenizer(articles, summaries,
    ...:                                     labels, tokenizer, group=group)
article texts to sequences ..
padding ..
summary texts to sequences ..
padding ..
concatenating ..
shuffling ..
splitting ..

In [22]: (x_train, y_train), (x_val, y_val) = data

In [23]: x_train.shape
Out[23]: (18000, 640)

In [24]: y_train.shape
Out[24]: (18000,)

In [25]: x_val.shape
Out[25]: (2000, 640)

In [26]: y_val.shape
Out[26]: (2000,)

In [27]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [28]: model = build_binary_glove_model(embedding_layer)

In [29]: train_binary_model(model, data)
Train on 18000 samples, validate on 2000 samples
Epoch 1/20
2018-12-02 15:48:49.323382: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-02 15:48:49.422370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-02 15:48:49.422817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.53GiB
2018-12-02 15:48:49.422834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-02 15:48:49.581579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-02 15:48:49.581618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-02 15:48:49.581623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-02 15:48:49.581768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2222 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
17792/18000 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.5056 - pearson_correlation_f: 0.02732018-12-02 15:48:53.737719: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.30GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-02 15:48:53.754936: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18000/18000 [==============================] - 5s 258us/step - loss: 0.7006 - acc: 0.5058 - pearson_correlation_f: 0.0280 - val_loss: 0.6911 - val_acc: 0.5315 - val_pearson_correlation_f: 0.0938
Epoch 2/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.6865 - acc: 0.5451 - pearson_correlation_f: 0.1521 - val_loss: 0.7326 - val_acc: 0.5285 - val_pearson_correlation_f: 0.1548
Epoch 3/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.5864 - acc: 0.6751 - pearson_correlation_f: 0.4675 - val_loss: 0.5011 - val_acc: 0.7495 - val_pearson_correlation_f: 0.5793
Epoch 4/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.4835 - acc: 0.7597 - pearson_correlation_f: 0.6113 - val_loss: 0.5086 - val_acc: 0.7335 - val_pearson_correlation_f: 0.5664
Epoch 5/20
18000/18000 [==============================] - 5s 301us/step - loss: 0.4277 - acc: 0.7988 - pearson_correlation_f: 0.6740 - val_loss: 0.5458 - val_acc: 0.7225 - val_pearson_correlation_f: 0.5395
Epoch 6/20
18000/18000 [==============================] - 5s 261us/step - loss: 0.3828 - acc: 0.8269 - pearson_correlation_f: 0.7225 - val_loss: 0.5400 - val_acc: 0.7315 - val_pearson_correlation_f: 0.6064
Epoch 7/20
18000/18000 [==============================] - 6s 308us/step - loss: 0.3316 - acc: 0.8550 - pearson_correlation_f: 0.7729 - val_loss: 0.6567 - val_acc: 0.7025 - val_pearson_correlation_f: 0.5773
Epoch 8/20
18000/18000 [==============================] - 6s 314us/step - loss: 0.2723 - acc: 0.8858 - pearson_correlation_f: 0.8250 - val_loss: 0.4804 - val_acc: 0.7975 - val_pearson_correlation_f: 0.6587
Epoch 9/20
18000/18000 [==============================] - 6s 307us/step - loss: 0.2185 - acc: 0.9117 - pearson_correlation_f: 0.8674 - val_loss: 0.6591 - val_acc: 0.7585 - val_pearson_correlation_f: 0.5875
Epoch 10/20
18000/18000 [==============================] - 5s 266us/step - loss: 0.1697 - acc: 0.9349 - pearson_correlation_f: 0.9046 - val_loss: 0.5876 - val_acc: 0.7810 - val_pearson_correlation_f: 0.6209
Epoch 11/20
18000/18000 [==============================] - 5s 268us/step - loss: 0.1232 - acc: 0.9557 - pearson_correlation_f: 0.9332 - val_loss: 0.6852 - val_acc: 0.7870 - val_pearson_correlation_f: 0.6207
Epoch 12/20
18000/18000 [==============================] - 6s 313us/step - loss: 0.1033 - acc: 0.9644 - pearson_correlation_f: 0.9461 - val_loss: 0.8862 - val_acc: 0.7770 - val_pearson_correlation_f: 0.6211
Epoch 13/20
18000/18000 [==============================] - 6s 308us/step - loss: 0.0805 - acc: 0.9731 - pearson_correlation_f: 0.9590 - val_loss: 0.9851 - val_acc: 0.7340 - val_pearson_correlation_f: 0.5294
Epoch 14/20
18000/18000 [==============================] - 6s 315us/step - loss: 0.0707 - acc: 0.9768 - pearson_correlation_f: 0.9656 - val_loss: 0.8161 - val_acc: 0.7880 - val_pearson_correlation_f: 0.6117
Epoch 15/20
18000/18000 [==============================] - 5s 262us/step - loss: 0.0620 - acc: 0.9798 - pearson_correlation_f: 0.9690 - val_loss: 0.9477 - val_acc: 0.7620 - val_pearson_correlation_f: 0.5723
Epoch 16/20
18000/18000 [==============================] - 5s 270us/step - loss: 0.0505 - acc: 0.9832 - pearson_correlation_f: 0.9745 - val_loss: 1.0397 - val_acc: 0.7535 - val_pearson_correlation_f: 0.5663
Epoch 17/20
18000/18000 [==============================] - 6s 313us/step - loss: 0.0479 - acc: 0.9841 - pearson_correlation_f: 0.9759 - val_loss: 0.9719 - val_acc: 0.7890 - val_pearson_correlation_f: 0.6129
Epoch 18/20
18000/18000 [==============================] - 6s 306us/step - loss: 0.0472 - acc: 0.9854 - pearson_correlation_f: 0.9778 - val_loss: 0.9805 - val_acc: 0.7955 - val_pearson_correlation_f: 0.6130
Epoch 19/20
18000/18000 [==============================] - 6s 309us/step - loss: 0.0444 - acc: 0.9852 - pearson_correlation_f: 0.9778 - val_loss: 1.0149 - val_acc: 0.7870 - val_pearson_correlation_f: 0.6126
Epoch 20/20
18000/18000 [==============================] - 6s 317us/step - loss: 0.0396 - acc: 0.9879 - pearson_correlation_f: 0.9813 - val_loss: 1.1600 - val_acc: 0.7870 - val_pearson_correlation_f: 0.6021
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          8946400   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 9,191,265
Trainable params: 244,865
Non-trainable params: 8,946,400
_________________________________________________________________

* <2018-12-02 Sun> negative sampling (5 sample), 10000 samplings, binary glove model

In [21]: (x_train, y_train), (x_val, y_val) = data

In [22]: x_train.shape
Out[22]: (54000, 640)

In [23]: y_train.shape
Out[23]: (54000,)

In [24]: x_val.shape
Out[24]: (6000, 640)

In [25]: y_val.shape
Out[25]: (6000,)

In [26]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [27]: model = build_binary_glove_model(embedding_layer)

In [28]: train_binary_model(model, data)
Train on 54000 samples, validate on 6000 samples
Epoch 1/20
2018-12-02 15:52:24.082961: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-02 15:52:24.189833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-02 15:52:24.190279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.53GiB
2018-12-02 15:52:24.190301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-02 15:52:24.350198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-02 15:52:24.350236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-02 15:52:24.350241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-02 15:52:24.350385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2222 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
53632/54000 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8224 - pearson_correlation_f: 0.17392018-12-02 15:52:34.769420: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.81GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-02 15:52:34.791396: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.56GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
54000/54000 [==============================] - 11s 207us/step - loss: 0.4455 - acc: 0.8223 - pearson_correlation_f: 0.1752 - val_loss: 0.3933 - val_acc: 0.8197 - val_pearson_correlation_f: 0.3924
Epoch 2/20
54000/54000 [==============================] - 13s 245us/step - loss: 0.3160 - acc: 0.8470 - pearson_correlation_f: 0.5426 - val_loss: 0.2866 - val_acc: 0.8613 - val_pearson_correlation_f: 0.5889
Epoch 3/20
54000/54000 [==============================] - 16s 291us/step - loss: 0.2599 - acc: 0.8721 - pearson_correlation_f: 0.6418 - val_loss: 0.2716 - val_acc: 0.8598 - val_pearson_correlation_f: 0.6042
Epoch 4/20
54000/54000 [==============================] - 17s 313us/step - loss: 0.2243 - acc: 0.8921 - pearson_correlation_f: 0.7008 - val_loss: 0.2678 - val_acc: 0.8720 - val_pearson_correlation_f: 0.6449
Epoch 5/20
54000/54000 [==============================] - 15s 272us/step - loss: 0.1907 - acc: 0.9126 - pearson_correlation_f: 0.7564 - val_loss: 0.2673 - val_acc: 0.8743 - val_pearson_correlation_f: 0.6298
Epoch 6/20
54000/54000 [==============================] - 15s 285us/step - loss: 0.1591 - acc: 0.9306 - pearson_correlation_f: 0.8091 - val_loss: 0.3387 - val_acc: 0.8555 - val_pearson_correlation_f: 0.6048
Epoch 7/20
54000/54000 [==============================] - 16s 288us/step - loss: 0.1274 - acc: 0.9462 - pearson_correlation_f: 0.8501 - val_loss: 0.3151 - val_acc: 0.8762 - val_pearson_correlation_f: 0.6306
Epoch 8/20
54000/54000 [==============================] - 16s 291us/step - loss: 0.1025 - acc: 0.9588 - pearson_correlation_f: 0.8873 - val_loss: 0.3259 - val_acc: 0.8702 - val_pearson_correlation_f: 0.6352
Epoch 9/20
54000/54000 [==============================] - 15s 281us/step - loss: 0.0799 - acc: 0.9683 - pearson_correlation_f: 0.9139 - val_loss: 0.4987 - val_acc: 0.8645 - val_pearson_correlation_f: 0.5438
Epoch 10/20
54000/54000 [==============================] - 15s 269us/step - loss: 0.0668 - acc: 0.9746 - pearson_correlation_f: 0.9297 - val_loss: 0.4341 - val_acc: 0.8768 - val_pearson_correlation_f: 0.6131
Epoch 11/20
54000/54000 [==============================] - 16s 294us/step - loss: 0.0560 - acc: 0.9786 - pearson_correlation_f: 0.9413 - val_loss: 0.4988 - val_acc: 0.8672 - val_pearson_correlation_f: 0.5892
Epoch 12/20
54000/54000 [==============================] - 16s 288us/step - loss: 0.0505 - acc: 0.9813 - pearson_correlation_f: 0.9491 - val_loss: 0.5345 - val_acc: 0.8755 - val_pearson_correlation_f: 0.5877
Epoch 13/20
54000/54000 [==============================] - 16s 297us/step - loss: 0.0452 - acc: 0.9830 - pearson_correlation_f: 0.9534 - val_loss: 0.5621 - val_acc: 0.8772 - val_pearson_correlation_f: 0.5889
Epoch 14/20
54000/54000 [==============================] - 16s 288us/step - loss: 0.0407 - acc: 0.9849 - pearson_correlation_f: 0.9588 - val_loss: 0.6681 - val_acc: 0.8580 - val_pearson_correlation_f: 0.5955
Epoch 15/20
54000/54000 [==============================] - 16s 294us/step - loss: 0.0402 - acc: 0.9851 - pearson_correlation_f: 0.9598 - val_loss: 0.6777 - val_acc: 0.8718 - val_pearson_correlation_f: 0.5752
Epoch 16/20
54000/54000 [==============================] - 16s 304us/step - loss: 0.0381 - acc: 0.9871 - pearson_correlation_f: 0.9632 - val_loss: 0.6563 - val_acc: 0.8687 - val_pearson_correlation_f: 0.5987
Epoch 17/20
54000/54000 [==============================] - 16s 298us/step - loss: 0.0350 - acc: 0.9871 - pearson_correlation_f: 0.9650 - val_loss: 0.7151 - val_acc: 0.8740 - val_pearson_correlation_f: 0.5776
Epoch 18/20
54000/54000 [==============================] - 17s 314us/step - loss: 0.0345 - acc: 0.9883 - pearson_correlation_f: 0.9669 - val_loss: 0.6628 - val_acc: 0.8757 - val_pearson_correlation_f: 0.5960
Epoch 19/20
54000/54000 [==============================] - 15s 273us/step - loss: 0.0317 - acc: 0.9892 - pearson_correlation_f: 0.9692 - val_loss: 0.7058 - val_acc: 0.8788 - val_pearson_correlation_f: 0.5801
Epoch 20/20
54000/54000 [==============================] - 16s 304us/step - loss: 0.0301 - acc: 0.9893 - pearson_correlation_f: 0.9700 - val_loss: 0.7259 - val_acc: 0.8785 - val_pearson_correlation_f: 0.5848
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          8909600   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 9,154,465
Trainable params: 244,865
Non-trainable params: 8,909,600
_________________________________________________________________

* <2018-12-02 Sun> negative sampling (5 sample), 20000 samplings, binary glove model

In [27]: model = build_binary_glove_model(embedding_layer)

In [28]: train_binary_model(model, data)
Train on 108000 samples, validate on 12000 samples
Epoch 1/20
2018-12-02 20:12:22.231174: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-02 20:12:22.337353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-02 20:12:22.337805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.74GiB
2018-12-02 20:12:22.337821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-02 20:12:22.491983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-02 20:12:22.492021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-02 20:12:22.492027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-02 20:12:22.492172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2438 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
107648/108000 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8339 - pearson_correlation_f: 0.28242018-12-02 20:12:42.399338: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-02 20:12:42.418354: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
108000/108000 [==============================] - 21s 196us/step - loss: 0.4035 - acc: 0.8340 - pearson_correlation_f: 0.2828 - val_loss: 0.3305 - val_acc: 0.8466 - val_pearson_correlation_f: 0.5006
Epoch 2/20
108000/108000 [==============================] - 20s 183us/step - loss: 0.2869 - acc: 0.8612 - pearson_correlation_f: 0.5874 - val_loss: 0.2670 - val_acc: 0.8607 - val_pearson_correlation_f: 0.6600
Epoch 3/20
108000/108000 [==============================] - 20s 184us/step - loss: 0.2208 - acc: 0.8973 - pearson_correlation_f: 0.7080 - val_loss: 0.1974 - val_acc: 0.9104 - val_pearson_correlation_f: 0.7471
Epoch 4/20
108000/108000 [==============================] - 20s 185us/step - loss: 0.1769 - acc: 0.9215 - pearson_correlation_f: 0.7804 - val_loss: 0.1887 - val_acc: 0.9181 - val_pearson_correlation_f: 0.7695
Epoch 5/20
108000/108000 [==============================] - 21s 197us/step - loss: 0.1486 - acc: 0.9369 - pearson_correlation_f: 0.8210 - val_loss: 0.1872 - val_acc: 0.9196 - val_pearson_correlation_f: 0.7730
Epoch 6/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.1256 - acc: 0.9473 - pearson_correlation_f: 0.8530 - val_loss: 0.1838 - val_acc: 0.9197 - val_pearson_correlation_f: 0.7759
Epoch 7/20
108000/108000 [==============================] - 21s 195us/step - loss: 0.1038 - acc: 0.9576 - pearson_correlation_f: 0.8825 - val_loss: 0.1960 - val_acc: 0.9207 - val_pearson_correlation_f: 0.7708
Epoch 8/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0867 - acc: 0.9654 - pearson_correlation_f: 0.9048 - val_loss: 0.2344 - val_acc: 0.9163 - val_pearson_correlation_f: 0.7482
Epoch 9/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0726 - acc: 0.9717 - pearson_correlation_f: 0.9217 - val_loss: 0.2437 - val_acc: 0.9221 - val_pearson_correlation_f: 0.7584
Epoch 10/20
108000/108000 [==============================] - 21s 198us/step - loss: 0.0616 - acc: 0.9763 - pearson_correlation_f: 0.9344 - val_loss: 0.2690 - val_acc: 0.9223 - val_pearson_correlation_f: 0.7529
Epoch 11/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0542 - acc: 0.9795 - pearson_correlation_f: 0.9434 - val_loss: 0.2883 - val_acc: 0.9178 - val_pearson_correlation_f: 0.7487
Epoch 12/20
108000/108000 [==============================] - 21s 195us/step - loss: 0.0482 - acc: 0.9825 - pearson_correlation_f: 0.9503 - val_loss: 0.2997 - val_acc: 0.9177 - val_pearson_correlation_f: 0.7506
Epoch 13/20
108000/108000 [==============================] - 21s 197us/step - loss: 0.0450 - acc: 0.9833 - pearson_correlation_f: 0.9540 - val_loss: 0.3350 - val_acc: 0.9155 - val_pearson_correlation_f: 0.7385
Epoch 14/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0407 - acc: 0.9855 - pearson_correlation_f: 0.9591 - val_loss: 0.3946 - val_acc: 0.9193 - val_pearson_correlation_f: 0.7323
Epoch 15/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0389 - acc: 0.9858 - pearson_correlation_f: 0.9609 - val_loss: 0.3736 - val_acc: 0.9188 - val_pearson_correlation_f: 0.7414
Epoch 16/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0360 - acc: 0.9870 - pearson_correlation_f: 0.9637 - val_loss: 0.3809 - val_acc: 0.9212 - val_pearson_correlation_f: 0.7388
Epoch 17/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0349 - acc: 0.9879 - pearson_correlation_f: 0.9664 - val_loss: 0.4436 - val_acc: 0.9090 - val_pearson_correlation_f: 0.7329
Epoch 18/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0341 - acc: 0.9884 - pearson_correlation_f: 0.9667 - val_loss: 0.4024 - val_acc: 0.9200 - val_pearson_correlation_f: 0.7446
Epoch 19/20
108000/108000 [==============================] - 21s 196us/step - loss: 0.0326 - acc: 0.9892 - pearson_correlation_f: 0.9691 - val_loss: 0.4088 - val_acc: 0.9196 - val_pearson_correlation_f: 0.7471
Epoch 20/20
108000/108000 [==============================] - 21s 198us/step - loss: 0.0319 - acc: 0.9894 - pearson_correlation_f: 0.9702 - val_loss: 0.4222 - val_acc: 0.9193 - val_pearson_correlation_f: 0.7462
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          12164200  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 12,409,065
Trainable params: 244,865
Non-trainable params: 12,164,200
_________________________________________________________________

* <2018-12-02 Sun> negative sampling (1 sample), 20000 samplings, binary glove model
In [28]: model = build_binary_glove_model(embedding_layer)

In [29]: train_binary_model(model, data)
Train on 36000 samples, validate on 4000 samples
Epoch 1/20
2018-12-02 20:23:01.544442: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-02 20:23:01.644052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-02 20:23:01.644557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.62GiB
2018-12-02 20:23:01.644575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-02 20:23:01.801787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-02 20:23:01.801823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-02 20:23:01.801829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-02 20:23:01.801975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2318 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
35840/36000 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5254 - pearson_correlation_f: 0.09882018-12-02 20:23:09.115362: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
36000/36000 [==============================] - 8s 222us/step - loss: 0.6922 - acc: 0.5258 - pearson_correlation_f: 0.0998 - val_loss: 0.6541 - val_acc: 0.6058 - val_pearson_correlation_f: 0.3729
Epoch 2/20
36000/36000 [==============================] - 7s 183us/step - loss: 0.5538 - acc: 0.7036 - pearson_correlation_f: 0.5133 - val_loss: 0.5315 - val_acc: 0.7328 - val_pearson_correlation_f: 0.5424
Epoch 3/20
36000/36000 [==============================] - 7s 183us/step - loss: 0.4733 - acc: 0.7689 - pearson_correlation_f: 0.6165 - val_loss: 0.6313 - val_acc: 0.7067 - val_pearson_correlation_f: 0.5033
Epoch 4/20
36000/36000 [==============================] - 7s 183us/step - loss: 0.4182 - acc: 0.8064 - pearson_correlation_f: 0.6828 - val_loss: 0.4595 - val_acc: 0.7778 - val_pearson_correlation_f: 0.6443
Epoch 5/20
36000/36000 [==============================] - 7s 184us/step - loss: 0.3542 - acc: 0.8457 - pearson_correlation_f: 0.7517 - val_loss: 0.4215 - val_acc: 0.8117 - val_pearson_correlation_f: 0.6876
Epoch 6/20
36000/36000 [==============================] - 7s 184us/step - loss: 0.2806 - acc: 0.8849 - pearson_correlation_f: 0.8206 - val_loss: 0.3286 - val_acc: 0.8618 - val_pearson_correlation_f: 0.7715
Epoch 7/20
36000/36000 [==============================] - 7s 184us/step - loss: 0.2192 - acc: 0.9146 - pearson_correlation_f: 0.8669 - val_loss: 0.4369 - val_acc: 0.8485 - val_pearson_correlation_f: 0.7565
Epoch 8/20
36000/36000 [==============================] - 7s 186us/step - loss: 0.1672 - acc: 0.9358 - pearson_correlation_f: 0.9023 - val_loss: 0.3836 - val_acc: 0.8685 - val_pearson_correlation_f: 0.7805
Epoch 9/20
36000/36000 [==============================] - 7s 186us/step - loss: 0.1280 - acc: 0.9520 - pearson_correlation_f: 0.9270 - val_loss: 0.3836 - val_acc: 0.8678 - val_pearson_correlation_f: 0.7797
Epoch 10/20
36000/36000 [==============================] - 7s 190us/step - loss: 0.0932 - acc: 0.9664 - pearson_correlation_f: 0.9487 - val_loss: 0.5171 - val_acc: 0.8387 - val_pearson_correlation_f: 0.7358
Epoch 11/20
36000/36000 [==============================] - 7s 192us/step - loss: 0.0751 - acc: 0.9723 - pearson_correlation_f: 0.9586 - val_loss: 0.4910 - val_acc: 0.8735 - val_pearson_correlation_f: 0.7867
Epoch 12/20
36000/36000 [==============================] - 7s 193us/step - loss: 0.0612 - acc: 0.9787 - pearson_correlation_f: 0.9675 - val_loss: 0.5508 - val_acc: 0.8485 - val_pearson_correlation_f: 0.7395
Epoch 13/20
36000/36000 [==============================] - 7s 195us/step - loss: 0.0528 - acc: 0.9814 - pearson_correlation_f: 0.9718 - val_loss: 0.6608 - val_acc: 0.8450 - val_pearson_correlation_f: 0.7260
Epoch 14/20
36000/36000 [==============================] - 7s 194us/step - loss: 0.0459 - acc: 0.9840 - pearson_correlation_f: 0.9754 - val_loss: 0.5270 - val_acc: 0.8718 - val_pearson_correlation_f: 0.7768
Epoch 15/20
36000/36000 [==============================] - 7s 196us/step - loss: 0.0430 - acc: 0.9844 - pearson_correlation_f: 0.9769 - val_loss: 0.6758 - val_acc: 0.8460 - val_pearson_correlation_f: 0.7329
Epoch 16/20
36000/36000 [==============================] - 7s 196us/step - loss: 0.0397 - acc: 0.9859 - pearson_correlation_f: 0.9786 - val_loss: 0.5895 - val_acc: 0.8772 - val_pearson_correlation_f: 0.7842
Epoch 17/20
36000/36000 [==============================] - 7s 198us/step - loss: 0.0358 - acc: 0.9872 - pearson_correlation_f: 0.9810 - val_loss: 0.7172 - val_acc: 0.8745 - val_pearson_correlation_f: 0.7794
Epoch 18/20
36000/36000 [==============================] - 7s 199us/step - loss: 0.0343 - acc: 0.9886 - pearson_correlation_f: 0.9816 - val_loss: 0.6065 - val_acc: 0.8845 - val_pearson_correlation_f: 0.7945
Epoch 19/20
36000/36000 [==============================] - 7s 197us/step - loss: 0.0349 - acc: 0.9879 - pearson_correlation_f: 0.9818 - val_loss: 0.6541 - val_acc: 0.8775 - val_pearson_correlation_f: 0.7797
Epoch 20/20
36000/36000 [==============================] - 7s 194us/step - loss: 0.0354 - acc: 0.9874 - pearson_correlation_f: 0.9810 - val_loss: 0.8225 - val_acc: 0.8745 - val_pearson_correlation_f: 0.7744
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          12100100  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 12,344,965
Trainable params: 244,865
Non-trainable params: 12,100,100
_________________________________________________________________

* <2018-12-04 Tue> Negative glove, 1 negative sample, 4,7000 samples
In [4]: with open(os.path.join(USE_DAN_DIR, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)

In [5]: with open(os.path.join(USE_DAN_DIR, 'negative.pickle'), 'rb') as f:
   ...:     negatives = pickle.load(f)

In [6]: story_keys = set(stories.keys())

In [7]: negative_keys = set(negatives.keys())

In [8]: len(story_keys)
Out[8]: 47471

In [9]: len(negative_keys)
Out[9]: 92000

In [10]: intersect_keys = story_keys.intersection(negative_keys)

In [11]: len(intersect_keys)
Out[11]: 47471

In [12]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [13]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [14]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [15]: fake_summaries = fake_summaries[:,:1]

In [16]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [17]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [18]: articles.shape
Out[18]: (47471,)

In [19]: reference_summaries.shape
Out[19]: (47471,)

In [20]: reference_labels.shape
Out[20]: (47471,)

In [21]: fake_summaries.shape
Out[21]: (47471, 1)

In [22]: fake_labels.shape
Out[22]: (47471, 1)

In [23]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [24]: articles, summaries, labels = res
In [25]: articles.shape
Out[25]: (94942,)

In [26]: summaries.shape
Out[26]: (94942,)

In [27]: labels.shape
Out[27]: (94942,)

In [28]: group = fake_summaries.shape[1] + 1

In [29]: data = prepare_data_with_USE(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [30]: (x_train, y_train), (x_val, y_val) = data

In [31]: x_train.shape
Out[31]: (85448, 13, 512)

In [32]: y_train.shape
Out[32]: (85448,)

In [33]: x_val.shape
Out[33]: (9494, 13, 512)

In [34]: y_val.shape
Out[34]: (9494,)

In [35]: model = build_binary_USE_model()

In [36]: train_binary_model(model, data)
Train on 85448 samples, validate on 9494 samples
Epoch 1/20
2018-12-04 00:27:27.362349: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 00:27:27.578985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 00:27:27.579564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.56GiB
2018-12-04 00:27:27.579586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 00:27:28.524410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 00:27:28.524452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 00:27:28.524460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 00:27:28.525346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2262 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
85448/85448 [==============================] - 6s 73us/step - loss: 0.5877 - acc: 0.6412 - pearson_correlation_f: 0.3571 - val_loss: 0.4284 - val_acc: 0.7924 - val_pearson_correlation_f: 0.6773
Epoch 2/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.3763 - acc: 0.8275 - pearson_correlation_f: 0.7287 - val_loss: 0.3310 - val_acc: 0.8580 - val_pearson_correlation_f: 0.7664
Epoch 3/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.3000 - acc: 0.8710 - pearson_correlation_f: 0.7976 - val_loss: 0.3062 - val_acc: 0.8667 - val_pearson_correlation_f: 0.7899
Epoch 4/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.2478 - acc: 0.8980 - pearson_correlation_f: 0.8392 - val_loss: 0.2912 - val_acc: 0.8792 - val_pearson_correlation_f: 0.8074
Epoch 5/20
85448/85448 [==============================] - 3s 41us/step - loss: 0.2095 - acc: 0.9144 - pearson_correlation_f: 0.8677 - val_loss: 0.2864 - val_acc: 0.8828 - val_pearson_correlation_f: 0.8111
Epoch 6/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.1772 - acc: 0.9281 - pearson_correlation_f: 0.8902 - val_loss: 0.2980 - val_acc: 0.8837 - val_pearson_correlation_f: 0.8120
Epoch 7/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.1520 - acc: 0.9394 - pearson_correlation_f: 0.9078 - val_loss: 0.3402 - val_acc: 0.8760 - val_pearson_correlation_f: 0.8022
Epoch 8/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.1289 - acc: 0.9500 - pearson_correlation_f: 0.9232 - val_loss: 0.3212 - val_acc: 0.8842 - val_pearson_correlation_f: 0.8114
Epoch 9/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.1110 - acc: 0.9563 - pearson_correlation_f: 0.9339 - val_loss: 0.3601 - val_acc: 0.8850 - val_pearson_correlation_f: 0.8099
Epoch 10/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0959 - acc: 0.9630 - pearson_correlation_f: 0.9440 - val_loss: 0.3771 - val_acc: 0.8836 - val_pearson_correlation_f: 0.8017
Epoch 11/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0834 - acc: 0.9683 - pearson_correlation_f: 0.9514 - val_loss: 0.4167 - val_acc: 0.8821 - val_pearson_correlation_f: 0.7999
Epoch 12/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0728 - acc: 0.9731 - pearson_correlation_f: 0.9584 - val_loss: 0.4261 - val_acc: 0.8851 - val_pearson_correlation_f: 0.8017
Epoch 13/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0629 - acc: 0.9765 - pearson_correlation_f: 0.9642 - val_loss: 0.5327 - val_acc: 0.8782 - val_pearson_correlation_f: 0.7875
Epoch 14/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0575 - acc: 0.9786 - pearson_correlation_f: 0.9676 - val_loss: 0.5315 - val_acc: 0.8776 - val_pearson_correlation_f: 0.7873
Epoch 15/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0512 - acc: 0.9814 - pearson_correlation_f: 0.9717 - val_loss: 0.5529 - val_acc: 0.8821 - val_pearson_correlation_f: 0.7903
Epoch 16/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0472 - acc: 0.9827 - pearson_correlation_f: 0.9737 - val_loss: 0.5534 - val_acc: 0.8812 - val_pearson_correlation_f: 0.7907
Epoch 17/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0432 - acc: 0.9850 - pearson_correlation_f: 0.9766 - val_loss: 0.5762 - val_acc: 0.8818 - val_pearson_correlation_f: 0.7912
Epoch 18/20
85448/85448 [==============================] - 3s 41us/step - loss: 0.0402 - acc: 0.9855 - pearson_correlation_f: 0.9780 - val_loss: 0.6406 - val_acc: 0.8781 - val_pearson_correlation_f: 0.7852
Epoch 19/20
85448/85448 [==============================] - 4s 41us/step - loss: 0.0364 - acc: 0.9872 - pearson_correlation_f: 0.9802 - val_loss: 0.6560 - val_acc: 0.8786 - val_pearson_correlation_f: 0.7829
Epoch 20/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0337 - acc: 0.9882 - pearson_correlation_f: 0.9817 - val_loss: 0.6847 - val_acc: 0.8786 - val_pearson_correlation_f: 0.7815
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________

* <2018-12-04 Tue> Negative glove, 5 negative sample, 20,000 samples

In [3]: with open(os.path.join(USE_DAN_DIR, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)

In [4]: with open(os.path.join(USE_DAN_DIR, 'negative.pickle'), 'rb') as f:
   ...:     negatives = pickle.load(f)

In [5]: story_keys = set(stories.keys())

In [6]: negative_keys = set(negatives.keys())

In [7]: len(story_keys)
Out[7]: 47471

In [8]: len(negative_keys)
Out[8]: 92000

In [9]: intersect_keys = story_keys.intersection(negative_keys)

In [10]: len(intersect_keys)
Out[10]: 47471

In [11]: intersect_keys = set(random.sample(intersect_keys, 20000))

In [12]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [13]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [14]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [15]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [16]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [17]: articles.shape
Out[17]: (20000,)

In [18]: reference_summaries.shape
Out[18]: (20000,)

In [19]: reference_labels.shape
Out[19]: (20000,)

In [20]: fake_summaries.shape
Out[20]: (20000, 5)

In [21]: fake_labels.shape
Out[21]: (20000, 5)

In [22]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [23]: articles, summaries, labels = res

In [24]: articles.shape
Out[24]: (120000,)

In [25]: summaries.shape
Out[25]: (120000,)

In [26]: labels.shape
Out[26]: (120000,)

In [27]: group = fake_summaries.shape[1] + 1

In [28]: data = prepare_data_with_USE(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [29]: (x_train, y_train), (x_val, y_val) = data

In [30]: x_train.shape
Out[30]: (108000, 13, 512)

In [31]: y_train.shape
Out[31]: (108000,)

In [32]: x_val.shape
Out[32]: (12000, 13, 512)

In [33]: y_val.shape
Out[33]: (12000,)

In [34]: model = build_binary_USE_model()

In [35]: train_binary_model(model, data)
Train on 108000 samples, validate on 12000 samples
Epoch 1/20
2018-12-04 00:40:42.649297: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 00:40:43.062716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 00:40:43.063238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.57GiB
2018-12-04 00:40:43.063259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 00:40:44.508238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 00:40:44.508274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 00:40:44.508280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 00:40:44.509135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
108000/108000 [==============================] - 8s 75us/step - loss: 0.4040 - acc: 0.8441 - pearson_correlation_f: 0.3239 - val_loss: 0.2844 - val_acc: 0.8828 - val_pearson_correlation_f: 0.6184
Epoch 2/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.2610 - acc: 0.8881 - pearson_correlation_f: 0.6591 - val_loss: 0.2540 - val_acc: 0.8913 - val_pearson_correlation_f: 0.6770
Epoch 3/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.2138 - acc: 0.9097 - pearson_correlation_f: 0.7340 - val_loss: 0.2235 - val_acc: 0.9068 - val_pearson_correlation_f: 0.7184
Epoch 4/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.1792 - acc: 0.9252 - pearson_correlation_f: 0.7846 - val_loss: 0.2278 - val_acc: 0.9087 - val_pearson_correlation_f: 0.7253
Epoch 5/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.1525 - acc: 0.9370 - pearson_correlation_f: 0.8216 - val_loss: 0.2457 - val_acc: 0.8971 - val_pearson_correlation_f: 0.7237
Epoch 6/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.1315 - acc: 0.9458 - pearson_correlation_f: 0.8478 - val_loss: 0.2475 - val_acc: 0.9118 - val_pearson_correlation_f: 0.7306
Epoch 7/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.1130 - acc: 0.9542 - pearson_correlation_f: 0.8713 - val_loss: 0.2692 - val_acc: 0.9087 - val_pearson_correlation_f: 0.7190
Epoch 8/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0980 - acc: 0.9602 - pearson_correlation_f: 0.8904 - val_loss: 0.2474 - val_acc: 0.9145 - val_pearson_correlation_f: 0.7401
Epoch 9/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0841 - acc: 0.9668 - pearson_correlation_f: 0.9073 - val_loss: 0.2840 - val_acc: 0.9039 - val_pearson_correlation_f: 0.7324
Epoch 10/20
108000/108000 [==============================] - 5s 43us/step - loss: 0.0758 - acc: 0.9702 - pearson_correlation_f: 0.9178 - val_loss: 0.2881 - val_acc: 0.9131 - val_pearson_correlation_f: 0.7329
Epoch 11/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0657 - acc: 0.9742 - pearson_correlation_f: 0.9286 - val_loss: 0.3363 - val_acc: 0.9132 - val_pearson_correlation_f: 0.7225
Epoch 12/20
108000/108000 [==============================] - 5s 43us/step - loss: 0.0573 - acc: 0.9777 - pearson_correlation_f: 0.9390 - val_loss: 0.3440 - val_acc: 0.9057 - val_pearson_correlation_f: 0.7233
Epoch 13/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0525 - acc: 0.9800 - pearson_correlation_f: 0.9451 - val_loss: 0.3641 - val_acc: 0.9157 - val_pearson_correlation_f: 0.7264
Epoch 14/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0470 - acc: 0.9821 - pearson_correlation_f: 0.9510 - val_loss: 0.3640 - val_acc: 0.9068 - val_pearson_correlation_f: 0.7212
Epoch 15/20
108000/108000 [==============================] - 5s 43us/step - loss: 0.0427 - acc: 0.9840 - pearson_correlation_f: 0.9557 - val_loss: 0.4303 - val_acc: 0.9121 - val_pearson_correlation_f: 0.7147
Epoch 16/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0381 - acc: 0.9859 - pearson_correlation_f: 0.9609 - val_loss: 0.4009 - val_acc: 0.9115 - val_pearson_correlation_f: 0.7227
Epoch 17/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0360 - acc: 0.9870 - pearson_correlation_f: 0.9634 - val_loss: 0.5185 - val_acc: 0.9153 - val_pearson_correlation_f: 0.7074
Epoch 18/20
108000/108000 [==============================] - 5s 43us/step - loss: 0.0320 - acc: 0.9887 - pearson_correlation_f: 0.9679 - val_loss: 0.4969 - val_acc: 0.9120 - val_pearson_correlation_f: 0.7111
Epoch 19/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0304 - acc: 0.9889 - pearson_correlation_f: 0.9695 - val_loss: 0.4817 - val_acc: 0.9074 - val_pearson_correlation_f: 0.7177
Epoch 20/20
108000/108000 [==============================] - 5s 42us/step - loss: 0.0288 - acc: 0.9900 - pearson_correlation_f: 0.9719 - val_loss: 0.5302 - val_acc: 0.9134 - val_pearson_correlation_f: 0.7127
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________

* <2018-12-04 Tue> InferSent, 10,000 samples, 1 negative sample

In [3]: d = INFERSENT_DIR

In [4]: with open(os.path.join(d, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)
with open(os.path.join(d, 'negative.pickle'), 'rb') as f:
...:     negatives = pickle.load(f)

In [5]: story_keys = set(stories.keys())
negative_keys = set(negatives.keys())

In [6]: 
In [7]: 
In [8]: len(story_keys)
Out[8]: 10579

In [9]: len(negative_keys)
Out[9]: 30169

In [10]: intersect_keys = story_keys.intersection(negative_keys)

In [11]: len(intersect_keys)
Out[11]: 10579

In [12]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [13]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [14]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [15]: fake_summaries = fake_summaries[:,:1]

In [16]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [17]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [18]: articles.shape
Out[18]: (10579,)

In [19]: reference_summaries.shape
Out[19]: (10579,)

In [20]: reference_labels.shape
Out[20]: (10579,)

In [21]: fake_summaries.shape
Out[21]: (10579, 1)

In [22]: fake_labels.shape
Out[22]: (10579, 1)

In [23]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [24]: articles, summaries, labels = res

In [25]: articles.shape
Out[25]: (21158,)

In [26]: summaries.shape
Out[26]: (21158,)

In [27]: labels.shape
Out[27]: (21158,)

In [28]: group = fake_summaries.shape[1] + 1

In [29]: data = prepare_data_with_INFER(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [30]: (x_train, y_train), (x_val, y_val) = data

In [31]: x_train.shape
Out[31]: (19044, 13, 4096)

In [32]: y_train.shape
Out[32]: (19044,)

In [33]: x_val.shape
Out[33]: (2114, 13, 4096)

In [34]: y_val.shape
Out[34]: (2114,)

In [35]: model = build_binary_INFER_model()

In [36]: train_binary_model(model, data)
Train on 19044 samples, validate on 2114 samples
Epoch 1/20
2018-12-04 14:46:13.749944: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 14:46:14.067066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 14:46:14.067658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.64GiB
2018-12-04 14:46:14.067681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 14:46:19.946305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 14:46:19.946344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 14:46:19.946354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 14:46:19.947433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2338 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2018-12-04 14:46:24.789423: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
19044/19044 [==============================] - 14s 730us/step - loss: 0.6940 - acc: 0.4956 - pearson_correlation_f: -0.0112 - val_loss: 0.6958 - val_acc: 0.5000 - val_pearson_correlation_f: 0.0192
Epoch 2/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.6942 - acc: 0.5050 - pearson_correlation_f: 0.0403 - val_loss: 0.6844 - val_acc: 0.5416 - val_pearson_correlation_f: 0.1520
Epoch 3/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.6642 - acc: 0.5576 - pearson_correlation_f: 0.2435 - val_loss: 0.6383 - val_acc: 0.5790 - val_pearson_correlation_f: 0.3243
Epoch 4/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.6236 - acc: 0.5944 - pearson_correlation_f: 0.3452 - val_loss: 0.6058 - val_acc: 0.6041 - val_pearson_correlation_f: 0.3976
Epoch 5/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.5824 - acc: 0.6568 - pearson_correlation_f: 0.4363 - val_loss: 0.5731 - val_acc: 0.6712 - val_pearson_correlation_f: 0.4699
Epoch 6/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.5296 - acc: 0.7097 - pearson_correlation_f: 0.5293 - val_loss: 0.5395 - val_acc: 0.7100 - val_pearson_correlation_f: 0.5301
Epoch 7/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.4922 - acc: 0.7402 - pearson_correlation_f: 0.5839 - val_loss: 0.5024 - val_acc: 0.7379 - val_pearson_correlation_f: 0.5671
Epoch 8/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.4578 - acc: 0.7659 - pearson_correlation_f: 0.6258 - val_loss: 0.5233 - val_acc: 0.7460 - val_pearson_correlation_f: 0.5683
Epoch 9/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.4337 - acc: 0.7818 - pearson_correlation_f: 0.6572 - val_loss: 0.5206 - val_acc: 0.7394 - val_pearson_correlation_f: 0.5860
Epoch 10/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.3979 - acc: 0.8064 - pearson_correlation_f: 0.6944 - val_loss: 0.5451 - val_acc: 0.7304 - val_pearson_correlation_f: 0.5739
Epoch 11/20
19044/19044 [==============================] - 3s 138us/step - loss: 0.3650 - acc: 0.8276 - pearson_correlation_f: 0.7298 - val_loss: 0.5564 - val_acc: 0.7469 - val_pearson_correlation_f: 0.5765
Epoch 12/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.3344 - acc: 0.8445 - pearson_correlation_f: 0.7585 - val_loss: 0.5225 - val_acc: 0.7725 - val_pearson_correlation_f: 0.6071
Epoch 13/20
19044/19044 [==============================] - 3s 138us/step - loss: 0.3048 - acc: 0.8605 - pearson_correlation_f: 0.7855 - val_loss: 0.6383 - val_acc: 0.7337 - val_pearson_correlation_f: 0.5328
Epoch 14/20
19044/19044 [==============================] - 3s 139us/step - loss: 0.2757 - acc: 0.8748 - pearson_correlation_f: 0.8107 - val_loss: 0.6892 - val_acc: 0.7521 - val_pearson_correlation_f: 0.5630
Epoch 15/20
19044/19044 [==============================] - 3s 139us/step - loss: 0.2530 - acc: 0.8896 - pearson_correlation_f: 0.8324 - val_loss: 0.5990 - val_acc: 0.7640 - val_pearson_correlation_f: 0.5871
Epoch 16/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.2239 - acc: 0.9049 - pearson_correlation_f: 0.8546 - val_loss: 0.6621 - val_acc: 0.7517 - val_pearson_correlation_f: 0.5725
Epoch 17/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.2113 - acc: 0.9139 - pearson_correlation_f: 0.8668 - val_loss: 0.6527 - val_acc: 0.7512 - val_pearson_correlation_f: 0.5715
Epoch 18/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.1825 - acc: 0.9281 - pearson_correlation_f: 0.8895 - val_loss: 0.7453 - val_acc: 0.7545 - val_pearson_correlation_f: 0.5670
Epoch 19/20
19044/19044 [==============================] - 3s 137us/step - loss: 0.1675 - acc: 0.9328 - pearson_correlation_f: 0.8976 - val_loss: 0.7375 - val_acc: 0.7569 - val_pearson_correlation_f: 0.5801
Epoch 20/20
19044/19044 [==============================] - 3s 136us/step - loss: 0.1525 - acc: 0.9409 - pearson_correlation_f: 0.9092 - val_loss: 0.8256 - val_acc: 0.7649 - val_pearson_correlation_f: 0.5738
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 4096)          0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           1572992   
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 1,638,913
Trainable params: 1,638,913
Non-trainable params: 0
_________________________________________________________________

* <2018-12-04 Tue> USE, 10,000 samples, 1 negative sample
In [3]: d = USE_DAN_DIR

In [4]: with open(os.path.join(d, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)
with open(os.path.join(d, 'negative.pickle'), 'rb') as f:
...:     negatives = pickle.load(f)
story_keys = set(stories.keys())
negative_keys = set(negatives.keys())
len(story_keys)
len(negative_keys)

In [5]: intersect_keys = story_keys.intersection(negative_keys)
len(intersect_keys)

In [6]: 
In [7]: 
In [8]: Out[8]: 47471

In [9]: Out[9]: 92000

In [10]: 
In [11]: Out[11]: 47471

In [12]: intersect_keys = set(random.sample(intersect_keys, 10000))

In [13]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [14]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [15]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [16]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [17]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [18]: fake_summaries = fake_summaries[:,:1]

In [19]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [20]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [21]: articles.shape
Out[21]: (10000,)

In [22]: reference_summaries.shape
Out[22]: (10000,)

In [23]: reference_labels.shape
Out[23]: (10000,)

In [24]: fake_summaries.shape
Out[24]: (10000, 1)

In [25]: fake_labels.shape
Out[25]: (10000, 1)

In [26]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [27]: articles, summaries, labels = res

In [28]: articles.shape
Out[28]: (20000,)

In [29]: summaries.shape
Out[29]: (20000,)

In [30]: labels.shape
Out[30]: (20000,)

In [31]: group = fake_summaries.shape[1] + 1

In [32]: data = prepare_data_with_USE(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [33]: (x_train, y_train), (x_val, y_val) = data

In [34]: x_train.shape
Out[34]: (18000, 13, 512)

In [35]: y_train.shape
Out[35]: (18000,)

In [36]: x_val.shape
Out[36]: (2000, 13, 512)

In [37]: y_val.shape
Out[37]: (2000,)

In [38]: model = build_binary_USE_model()

In [39]: train_binary_model(model, data)
Train on 18000 samples, validate on 2000 samples
Epoch 1/20
2018-12-04 17:10:54.887903: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:10:54.988609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:10:54.989053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.62GiB
2018-12-04 17:10:54.989070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:10:55.190992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:10:55.191028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:10:55.191033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:10:55.191187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2323 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
18000/18000 [==============================] - 2s 108us/step - loss: 0.6934 - acc: 0.4959 - pearson_correlation_f: -0.0169 - val_loss: 0.6930 - val_acc: 0.5015 - val_pearson_correlation_f: 0.0181
Epoch 2/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.6889 - acc: 0.5192 - pearson_correlation_f: 0.0971 - val_loss: 0.6558 - val_acc: 0.6250 - val_pearson_correlation_f: 0.3172
Epoch 3/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.6001 - acc: 0.6643 - pearson_correlation_f: 0.4426 - val_loss: 0.6428 - val_acc: 0.6395 - val_pearson_correlation_f: 0.5223
Epoch 4/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.4857 - acc: 0.7581 - pearson_correlation_f: 0.6170 - val_loss: 0.4552 - val_acc: 0.7720 - val_pearson_correlation_f: 0.6322
Epoch 5/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.4079 - acc: 0.8062 - pearson_correlation_f: 0.7033 - val_loss: 0.4297 - val_acc: 0.7925 - val_pearson_correlation_f: 0.6650
Epoch 6/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.3486 - acc: 0.8447 - pearson_correlation_f: 0.7621 - val_loss: 0.4299 - val_acc: 0.7935 - val_pearson_correlation_f: 0.6745
Epoch 7/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.2984 - acc: 0.8711 - pearson_correlation_f: 0.8060 - val_loss: 0.4397 - val_acc: 0.7975 - val_pearson_correlation_f: 0.6841
Epoch 8/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.2524 - acc: 0.8926 - pearson_correlation_f: 0.8426 - val_loss: 0.4314 - val_acc: 0.8175 - val_pearson_correlation_f: 0.6970
Epoch 9/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.2106 - acc: 0.9141 - pearson_correlation_f: 0.8739 - val_loss: 0.4408 - val_acc: 0.8255 - val_pearson_correlation_f: 0.7095
Epoch 10/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.1767 - acc: 0.9296 - pearson_correlation_f: 0.8982 - val_loss: 0.5359 - val_acc: 0.8125 - val_pearson_correlation_f: 0.6967
Epoch 11/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.1483 - acc: 0.9412 - pearson_correlation_f: 0.9156 - val_loss: 0.5600 - val_acc: 0.8020 - val_pearson_correlation_f: 0.6747
Epoch 12/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.1196 - acc: 0.9549 - pearson_correlation_f: 0.9335 - val_loss: 0.6156 - val_acc: 0.8010 - val_pearson_correlation_f: 0.6755
Epoch 13/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.0947 - acc: 0.9648 - pearson_correlation_f: 0.9485 - val_loss: 0.7343 - val_acc: 0.8010 - val_pearson_correlation_f: 0.6660
Epoch 14/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.0859 - acc: 0.9695 - pearson_correlation_f: 0.9541 - val_loss: 0.9342 - val_acc: 0.7785 - val_pearson_correlation_f: 0.6268
Epoch 15/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.0698 - acc: 0.9762 - pearson_correlation_f: 0.9640 - val_loss: 0.7286 - val_acc: 0.8200 - val_pearson_correlation_f: 0.6817
Epoch 16/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.0619 - acc: 0.9785 - pearson_correlation_f: 0.9674 - val_loss: 0.7715 - val_acc: 0.8220 - val_pearson_correlation_f: 0.6812
Epoch 17/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.0529 - acc: 0.9822 - pearson_correlation_f: 0.9728 - val_loss: 0.8213 - val_acc: 0.8205 - val_pearson_correlation_f: 0.6765
Epoch 18/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.0473 - acc: 0.9843 - pearson_correlation_f: 0.9756 - val_loss: 0.8917 - val_acc: 0.8130 - val_pearson_correlation_f: 0.6671
Epoch 19/20
18000/18000 [==============================] - 1s 44us/step - loss: 0.0495 - acc: 0.9833 - pearson_correlation_f: 0.9747 - val_loss: 0.8534 - val_acc: 0.8150 - val_pearson_correlation_f: 0.6730
Epoch 20/20
18000/18000 [==============================] - 1s 43us/step - loss: 0.0449 - acc: 0.9840 - pearson_correlation_f: 0.9763 - val_loss: 0.9041 - val_acc: 0.8145 - val_pearson_correlation_f: 0.6611
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________

* <2018-12-04 Tue> InferSent, 10,000 samples, 1 negative sample
In [3]: d = INFERSENT_DIR

In [4]: with open(os.path.join(d, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)
with open(os.path.join(d, 'negative.pickle'), 'rb') as f:
...:     negatives = pickle.load(f)

In [5]: story_keys = set(stories.keys())
negative_keys = set(negatives.keys())
len(story_keys)
len(negative_keys)
intersect_keys = story_keys.intersection(negative_keys)
len(intersect_keys)

In [6]: 
In [7]: 
In [8]: Out[8]: 13074

In [9]: Out[9]: 30169

In [10]: 
In [11]: Out[11]: 13074

In [12]: intersect_keys = set(random.sample(intersect_keys, 10000))

In [13]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [14]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [15]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [16]: fake_summaries = fake_summaries[:,:1]

In [17]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [18]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [19]: articles.shape
Out[19]: (10000,)

In [20]: reference_summaries.shape
Out[20]: (10000,)

In [21]: reference_labels.shape
Out[21]: (10000,)

In [22]: fake_summaries.shape
Out[22]: (10000, 1)

In [23]: fake_labels.shape
Out[23]: (10000, 1)

In [24]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [25]: articles, summaries, labels = res

In [26]: articles.shape
Out[26]: (20000,)

In [27]: summaries.shape
Out[27]: (20000,)

In [28]: labels.shape
Out[28]: (20000,)

In [29]: group = fake_summaries.shape[1] + 1

In [30]: data = prepare_data_with_INFER(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [31]: (x_train, y_train), (x_val, y_val) = data

In [32]: x_train.shape
Out[32]: (18000, 13, 4096)

In [33]: y_train.shape
Out[33]: (18000,)

In [34]: x_val.shape
Out[34]: (2000, 13, 4096)

In [35]: y_val.shape
Out[35]: (2000,)

In [36]: model = build_binary_INFER_model()

In [37]: train_binary_model(model, data)
Train on 18000 samples, validate on 2000 samples
Epoch 1/20
2018-12-04 17:14:37.872288: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:14:38.110500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:14:38.111021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.62GiB
2018-12-04 17:14:38.111041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:14:43.889415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:14:43.889453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:14:43.889462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:14:43.890660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2323 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2018-12-04 17:14:48.719613: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18000/18000 [==============================] - 14s 751us/step - loss: 0.6935 - acc: 0.5068 - pearson_correlation_f: -0.0159 - val_loss: 0.6932 - val_acc: 0.5040 - val_pearson_correlation_f: 0.0160
Epoch 2/20
18000/18000 [==============================] - 2s 138us/step - loss: 0.6943 - acc: 0.5129 - pearson_correlation_f: 0.0343 - val_loss: 0.6885 - val_acc: 0.5365 - val_pearson_correlation_f: 0.0966
Epoch 3/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.6684 - acc: 0.5593 - pearson_correlation_f: 0.2280 - val_loss: 0.6457 - val_acc: 0.5825 - val_pearson_correlation_f: 0.3085
Epoch 4/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.6238 - acc: 0.6091 - pearson_correlation_f: 0.3532 - val_loss: 0.6313 - val_acc: 0.6225 - val_pearson_correlation_f: 0.3405
Epoch 5/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.5720 - acc: 0.6673 - pearson_correlation_f: 0.4632 - val_loss: 0.5526 - val_acc: 0.7065 - val_pearson_correlation_f: 0.4969
Epoch 6/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.5202 - acc: 0.7196 - pearson_correlation_f: 0.5467 - val_loss: 0.5854 - val_acc: 0.6820 - val_pearson_correlation_f: 0.4833
Epoch 7/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.4835 - acc: 0.7453 - pearson_correlation_f: 0.5961 - val_loss: 0.5590 - val_acc: 0.7060 - val_pearson_correlation_f: 0.5050
Epoch 8/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.4500 - acc: 0.7736 - pearson_correlation_f: 0.6382 - val_loss: 0.5729 - val_acc: 0.7005 - val_pearson_correlation_f: 0.5092
Epoch 9/20
18000/18000 [==============================] - 3s 140us/step - loss: 0.4192 - acc: 0.7934 - pearson_correlation_f: 0.6709 - val_loss: 0.5435 - val_acc: 0.7240 - val_pearson_correlation_f: 0.5327
Epoch 10/20
18000/18000 [==============================] - 3s 140us/step - loss: 0.3917 - acc: 0.8118 - pearson_correlation_f: 0.7024 - val_loss: 0.5417 - val_acc: 0.7380 - val_pearson_correlation_f: 0.5485
Epoch 11/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.3604 - acc: 0.8327 - pearson_correlation_f: 0.7346 - val_loss: 0.5989 - val_acc: 0.7305 - val_pearson_correlation_f: 0.5368
Epoch 12/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.3349 - acc: 0.8488 - pearson_correlation_f: 0.7595 - val_loss: 0.6078 - val_acc: 0.7330 - val_pearson_correlation_f: 0.5324
Epoch 13/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.3022 - acc: 0.8656 - pearson_correlation_f: 0.7891 - val_loss: 0.6183 - val_acc: 0.7405 - val_pearson_correlation_f: 0.5495
Epoch 14/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.2712 - acc: 0.8807 - pearson_correlation_f: 0.8154 - val_loss: 0.6175 - val_acc: 0.7350 - val_pearson_correlation_f: 0.5510
Epoch 15/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.2508 - acc: 0.8932 - pearson_correlation_f: 0.8336 - val_loss: 0.6963 - val_acc: 0.7260 - val_pearson_correlation_f: 0.5217
Epoch 16/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.2284 - acc: 0.9017 - pearson_correlation_f: 0.8514 - val_loss: 0.7044 - val_acc: 0.7355 - val_pearson_correlation_f: 0.5422
Epoch 17/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.2058 - acc: 0.9131 - pearson_correlation_f: 0.8674 - val_loss: 0.7780 - val_acc: 0.7240 - val_pearson_correlation_f: 0.5206
Epoch 18/20
18000/18000 [==============================] - 2s 139us/step - loss: 0.1887 - acc: 0.9238 - pearson_correlation_f: 0.8826 - val_loss: 0.8181 - val_acc: 0.7395 - val_pearson_correlation_f: 0.5332
Epoch 19/20
18000/18000 [==============================] - 3s 140us/step - loss: 0.1695 - acc: 0.9320 - pearson_correlation_f: 0.8949 - val_loss: 0.9062 - val_acc: 0.7200 - val_pearson_correlation_f: 0.5115
Epoch 20/20
18000/18000 [==============================] - 3s 139us/step - loss: 0.1564 - acc: 0.9395 - pearson_correlation_f: 0.9059 - val_loss: 0.9499 - val_acc: 0.7355 - val_pearson_correlation_f: 0.5120
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 4096)          0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           1572992   
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 1,638,913
Trainable params: 1,638,913
Non-trainable params: 0
_________________________________________________________________
* <2018-12-04 Tue> USE, 30,000 samples, 1 negative sample

In [3]: d = USE_DAN_DIR

In [4]: with open(os.path.join(d, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)
with open(os.path.join(d, 'negative.pickle'), 'rb') as f:
...:     negatives = pickle.load(f)
story_keys = set(stories.keys())
negative_keys = set(negatives.keys())
len(story_keys)
len(negative_keys)
intersect_keys = story_keys.intersection(negative_keys)
len(intersect_keys)

In [5]: 
In [6]: 
In [7]: 
In [8]: Out[8]: 47471

In [9]: Out[9]: 92000

In [10]: 
In [11]: Out[11]: 47471

In [12]: intersect_keys = set(random.sample(intersect_keys, 30000))

In [13]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [14]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [15]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [16]: fake_summaries = fake_summaries[:,:1]

In [17]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [18]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [19]: articles.shape
Out[19]: (30000,)

In [20]: reference_summaries.shape
Out[20]: (30000,)

In [21]: reference_labels.shape
Out[21]: (30000,)

In [22]: fake_summaries.shape
Out[22]: (30000, 1)

In [23]: fake_labels.shape
Out[23]: (30000, 1)

In [24]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [25]: articles, summaries, labels = res

In [26]: articles.shape
Out[26]: (60000,)

In [27]: summaries.shape
Out[27]: (60000,)

In [28]: labels.shape
Out[28]: (60000,)

In [29]: group = fake_summaries.shape[1] + 1

In [30]: data = prepare_data_with_USE(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [31]: (x_train, y_train), (x_val, y_val) = data

In [32]: x_train.shape
Out[32]: (54000, 13, 512)

In [33]: y_train.shape
Out[33]: (54000,)

In [34]: x_val.shape
Out[34]: (6000, 13, 512)

In [35]: y_val.shape
Out[35]: (6000,)

In [36]: model = build_binary_USE_model()

In [37]: train_binary_model(model, data)
Train on 54000 samples, validate on 6000 samples
Epoch 1/20
2018-12-04 17:17:43.802411: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:17:43.921462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:17:43.921936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.62GiB
2018-12-04 17:17:43.921953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:17:44.791943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:17:44.791978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:17:44.791985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:17:44.792758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2323 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
54000/54000 [==============================] - 5s 86us/step - loss: 0.6676 - acc: 0.5535 - pearson_correlation_f: 0.1546 - val_loss: 0.5371 - val_acc: 0.7137 - val_pearson_correlation_f: 0.5511
Epoch 2/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.4585 - acc: 0.7741 - pearson_correlation_f: 0.6441 - val_loss: 0.3908 - val_acc: 0.8263 - val_pearson_correlation_f: 0.7117
Epoch 3/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.3577 - acc: 0.8380 - pearson_correlation_f: 0.7482 - val_loss: 0.3538 - val_acc: 0.8422 - val_pearson_correlation_f: 0.7457
Epoch 4/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.2959 - acc: 0.8736 - pearson_correlation_f: 0.8037 - val_loss: 0.3440 - val_acc: 0.8490 - val_pearson_correlation_f: 0.7596
Epoch 5/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.2458 - acc: 0.8977 - pearson_correlation_f: 0.8417 - val_loss: 0.3243 - val_acc: 0.8628 - val_pearson_correlation_f: 0.7779
Epoch 6/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.2090 - acc: 0.9156 - pearson_correlation_f: 0.8701 - val_loss: 0.3663 - val_acc: 0.8537 - val_pearson_correlation_f: 0.7686
Epoch 7/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.1744 - acc: 0.9298 - pearson_correlation_f: 0.8933 - val_loss: 0.3548 - val_acc: 0.8645 - val_pearson_correlation_f: 0.7786
Epoch 8/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.1494 - acc: 0.9403 - pearson_correlation_f: 0.9104 - val_loss: 0.4808 - val_acc: 0.8458 - val_pearson_correlation_f: 0.7489
Epoch 9/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.1245 - acc: 0.9514 - pearson_correlation_f: 0.9265 - val_loss: 0.4058 - val_acc: 0.8653 - val_pearson_correlation_f: 0.7710
Epoch 10/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.1073 - acc: 0.9590 - pearson_correlation_f: 0.9375 - val_loss: 0.4114 - val_acc: 0.8702 - val_pearson_correlation_f: 0.7793
Epoch 11/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.0917 - acc: 0.9645 - pearson_correlation_f: 0.9467 - val_loss: 0.4780 - val_acc: 0.8607 - val_pearson_correlation_f: 0.7646
Epoch 12/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0784 - acc: 0.9704 - pearson_correlation_f: 0.9554 - val_loss: 0.5158 - val_acc: 0.8582 - val_pearson_correlation_f: 0.7576
Epoch 13/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.0710 - acc: 0.9728 - pearson_correlation_f: 0.9598 - val_loss: 0.6112 - val_acc: 0.8547 - val_pearson_correlation_f: 0.7513
Epoch 14/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0622 - acc: 0.9765 - pearson_correlation_f: 0.9647 - val_loss: 0.5577 - val_acc: 0.8647 - val_pearson_correlation_f: 0.7644
Epoch 15/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0559 - acc: 0.9795 - pearson_correlation_f: 0.9689 - val_loss: 0.6155 - val_acc: 0.8627 - val_pearson_correlation_f: 0.7593
Epoch 16/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0475 - acc: 0.9826 - pearson_correlation_f: 0.9736 - val_loss: 0.6531 - val_acc: 0.8627 - val_pearson_correlation_f: 0.7551
Epoch 17/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0451 - acc: 0.9839 - pearson_correlation_f: 0.9755 - val_loss: 0.7014 - val_acc: 0.8640 - val_pearson_correlation_f: 0.7572
Epoch 18/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0427 - acc: 0.9846 - pearson_correlation_f: 0.9765 - val_loss: 0.6794 - val_acc: 0.8618 - val_pearson_correlation_f: 0.7558
Epoch 19/20
54000/54000 [==============================] - 2s 42us/step - loss: 0.0388 - acc: 0.9863 - pearson_correlation_f: 0.9789 - val_loss: 0.7218 - val_acc: 0.8663 - val_pearson_correlation_f: 0.7580
Epoch 20/20
54000/54000 [==============================] - 2s 41us/step - loss: 0.0351 - acc: 0.9879 - pearson_correlation_f: 0.9814 - val_loss: 0.7701 - val_acc: 0.8662 - val_pearson_correlation_f: 0.7579
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________
* <2018-12-04 Tue> USE, 47,000 samples, 1 negative sample

In [3]: d = USE_DAN_DIR

In [4]: with open(os.path.join(d, 'story.pickle'), 'rb') as f:
   ...:     stories = pickle.load(f)

In [5]: with open(os.path.join(d, 'negative.pickle'), 'rb') as f:
   ...:     negatives = pickle.load(f)

In [6]: story_keys = set(stories.keys())

In [7]: negative_keys = set(negatives.keys())

In [8]: len(story_keys)
Out[8]: 47471

In [9]: len(negative_keys)
Out[9]: 92000

In [10]: intersect_keys = story_keys.intersection(negative_keys)

In [11]: len(intersect_keys)
Out[11]: 47471

In [12]: articles = np.array([stories[key]['article'] for key in intersect_keys])

In [13]: reference_summaries = np.array([stories[key]['summary'] for key in
    ...:                                 intersect_keys])

In [14]: fake_summaries = np.array([negatives[key] for key in intersect_keys])

In [15]: fake_summaries = fake_summaries[:,:1]

In [16]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [17]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [18]: articles.shape
Out[18]: (47471,)

In [19]: reference_summaries.shape
Out[19]: (47471,)

In [20]: reference_labels.shape
Out[20]: (47471,)

In [21]: fake_summaries.shape
Out[21]: (47471, 1)

In [22]: fake_labels.shape
Out[22]: (47471, 1)

In [23]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [24]: articles, summaries, labels = res

In [25]: articles.shape
Out[25]: (94942,)

In [26]: summaries.shape
Out[26]: (94942,)

In [27]: labels.shape
Out[27]: (94942,)

In [28]: group = fake_summaries.shape[1] + 1

In [29]: data = prepare_data_with_USE(articles, summaries,
    ...:                              labels, group=group)
padding sequence ..
concatenating ..
shuffling ..
splitting ..

In [30]: (x_train, y_train), (x_val, y_val) = data

In [31]: x_train.shape
Out[31]: (85448, 13, 512)

In [32]: y_train.shape
Out[32]: (85448,)

In [33]: x_val.shape
Out[33]: (9494, 13, 512)

In [34]: y_val.shape
Out[34]: (9494,)

In [35]: model = build_binary_USE_model()

In [36]: train_binary_model(model, data)
Train on 85448 samples, validate on 9494 samples
Epoch 1/20
2018-12-04 17:20:11.997939: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:20:12.189691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:20:12.190193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.59GiB
2018-12-04 17:20:12.190212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:20:13.071660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:20:13.071697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:20:13.071704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:20:13.072486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2292 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
85448/85448 [==============================] - 6s 72us/step - loss: 0.5874 - acc: 0.6421 - pearson_correlation_f: 0.3595 - val_loss: 0.4202 - val_acc: 0.8042 - val_pearson_correlation_f: 0.6804
Epoch 2/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.3733 - acc: 0.8299 - pearson_correlation_f: 0.7341 - val_loss: 0.3301 - val_acc: 0.8599 - val_pearson_correlation_f: 0.7686
Epoch 3/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.2920 - acc: 0.8752 - pearson_correlation_f: 0.8050 - val_loss: 0.3426 - val_acc: 0.8560 - val_pearson_correlation_f: 0.7741
Epoch 4/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.2420 - acc: 0.8988 - pearson_correlation_f: 0.8444 - val_loss: 0.3006 - val_acc: 0.8748 - val_pearson_correlation_f: 0.8001
Epoch 5/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.2041 - acc: 0.9163 - pearson_correlation_f: 0.8717 - val_loss: 0.3160 - val_acc: 0.8689 - val_pearson_correlation_f: 0.7954
Epoch 6/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.1745 - acc: 0.9304 - pearson_correlation_f: 0.8929 - val_loss: 0.3041 - val_acc: 0.8764 - val_pearson_correlation_f: 0.8014
Epoch 7/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.1476 - acc: 0.9420 - pearson_correlation_f: 0.9108 - val_loss: 0.3236 - val_acc: 0.8770 - val_pearson_correlation_f: 0.8014
Epoch 8/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.1269 - acc: 0.9501 - pearson_correlation_f: 0.9241 - val_loss: 0.3856 - val_acc: 0.8747 - val_pearson_correlation_f: 0.7898
Epoch 9/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.1100 - acc: 0.9575 - pearson_correlation_f: 0.9348 - val_loss: 0.3703 - val_acc: 0.8791 - val_pearson_correlation_f: 0.7987
Epoch 10/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0963 - acc: 0.9634 - pearson_correlation_f: 0.9441 - val_loss: 0.4396 - val_acc: 0.8794 - val_pearson_correlation_f: 0.7931
Epoch 11/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0828 - acc: 0.9682 - pearson_correlation_f: 0.9518 - val_loss: 0.4509 - val_acc: 0.8751 - val_pearson_correlation_f: 0.7884
Epoch 12/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0726 - acc: 0.9726 - pearson_correlation_f: 0.9586 - val_loss: 0.4952 - val_acc: 0.8746 - val_pearson_correlation_f: 0.7858
Epoch 13/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0658 - acc: 0.9754 - pearson_correlation_f: 0.9627 - val_loss: 0.5065 - val_acc: 0.8784 - val_pearson_correlation_f: 0.7868
Epoch 14/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0572 - acc: 0.9791 - pearson_correlation_f: 0.9679 - val_loss: 0.5334 - val_acc: 0.8779 - val_pearson_correlation_f: 0.7857
Epoch 15/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0527 - acc: 0.9807 - pearson_correlation_f: 0.9708 - val_loss: 0.6052 - val_acc: 0.8783 - val_pearson_correlation_f: 0.7840
Epoch 16/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0474 - acc: 0.9823 - pearson_correlation_f: 0.9732 - val_loss: 0.5885 - val_acc: 0.8773 - val_pearson_correlation_f: 0.7833
Epoch 17/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0450 - acc: 0.9840 - pearson_correlation_f: 0.9754 - val_loss: 0.6557 - val_acc: 0.8755 - val_pearson_correlation_f: 0.7795
Epoch 18/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0406 - acc: 0.9854 - pearson_correlation_f: 0.9776 - val_loss: 0.6543 - val_acc: 0.8802 - val_pearson_correlation_f: 0.7827
Epoch 19/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0368 - acc: 0.9871 - pearson_correlation_f: 0.9800 - val_loss: 0.6788 - val_acc: 0.8753 - val_pearson_correlation_f: 0.7794
Epoch 20/20
85448/85448 [==============================] - 4s 42us/step - loss: 0.0355 - acc: 0.9870 - pearson_correlation_f: 0.9804 - val_loss: 0.7133 - val_acc: 0.8760 - val_pearson_correlation_f: 0.7778
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 13, 512)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 11, 128)           196736    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 3, 128)            49280     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 262,657
Trainable params: 262,657
Non-trainable params: 0
_________________________________________________________________

* <2018-12-04 Tue> binary glove, neg, 92,000 samples, 1 negative sample

In [3]: keys = load_story_keys()

In [4]: tokenizer = create_tokenizer_by_key(keys)

In [5]: len(keys)
Out[5]: 92579

In [6]: fake_summaries = load_negative_sampling_data(keys)

In [7]: fake_summaries = fake_summaries[:,:1]

In [8]: fake_summaries.shape
Out[8]: (92579, 1)

In [9]: articles, reference_summaries = load_article_and_summary_data(keys)

In [10]: reference_labels = np.ones_like(reference_summaries, dtype=int)

In [11]: fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [12]: articles.shape
Out[12]: (92579,)

In [13]: reference_summaries.shape
Out[13]: (92579,)

In [14]: fake_summaries.shape
Out[14]: (92579, 1)

In [15]: fake_labels.shape
Out[15]: (92579, 1)

In [16]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [17]: articles, summaries, labels = res

articles.shape
In [18]: Out[18]: (185158,)

In [19]: summaries.shape
Out[19]: (185158,)

In [20]: labels.shape
Out[20]: (185158,)

In [21]: group = fake_summaries.shape[1] + 1

In [22]: data = prepare_data_using_tokenizer(articles, summaries,
    ...:                                     labels, tokenizer, group=group)
article texts to sequences ..
padding ..
summary texts to sequences ..
padding ..
concatenating ..
shuffling ..
splitting ..

In [23]: (x_train, y_train), (x_val, y_val) = data

In [24]: x_train.shape
Out[24]: (166644, 640)

In [25]: y_train.shape
Out[25]: (166644,)

In [26]: x_val.shape
Out[26]: (18514, 640)

In [27]: y_val.shape
Out[27]: (18514,)

In [28]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [29]: model = build_binary_glove_model(embedding_layer)

In [30]: train_binary_model(model, data)
Train on 166644 samples, validate on 18514 samples
Epoch 1/20
2018-12-04 17:28:12.438233: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:28:12.592967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:28:12.593482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.59GiB
2018-12-04 17:28:12.593503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:28:13.430967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:28:13.431009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:28:13.431017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:28:13.431891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2292 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2018-12-04 17:28:13.550941: W tensorflow/core/framework/allocator.cc:122] Allocation of 80000400 exceeds 10% of system memory.
2018-12-04 17:28:13.579758: W tensorflow/core/framework/allocator.cc:122] Allocation of 80000400 exceeds 10% of system memory.
2018-12-04 17:28:13.687887: W tensorflow/core/framework/allocator.cc:122] Allocation of 80000400 exceeds 10% of system memory.
2018-12-04 17:28:13.713480: W tensorflow/core/framework/allocator.cc:122] Allocation of 80000400 exceeds 10% of system memory.
2018-12-04 17:28:13.737782: W tensorflow/core/framework/allocator.cc:122] Allocation of 80000400 exceeds 10% of system memory.
166272/166644 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.7337 - pearson_correlation_f: 0.53712018-12-04 17:28:44.713824: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.87GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-04 17:28:44.738471: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.69GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-04 17:28:46.225453: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
166644/166644 [==============================] - 34s 203us/step - loss: 0.4939 - acc: 0.7340 - pearson_correlation_f: 0.5376 - val_loss: 0.3108 - val_acc: 0.8684 - val_pearson_correlation_f: 0.7845
Epoch 2/20
166644/166644 [==============================] - 31s 185us/step - loss: 0.2993 - acc: 0.8766 - pearson_correlation_f: 0.7995 - val_loss: 0.2846 - val_acc: 0.8859 - val_pearson_correlation_f: 0.8194
Epoch 3/20
166644/166644 [==============================] - 32s 192us/step - loss: 0.2219 - acc: 0.9134 - pearson_correlation_f: 0.8618 - val_loss: 0.2271 - val_acc: 0.9115 - val_pearson_correlation_f: 0.8579
Epoch 4/20
166644/166644 [==============================] - 33s 196us/step - loss: 0.1841 - acc: 0.9307 - pearson_correlation_f: 0.8894 - val_loss: 0.1929 - val_acc: 0.9283 - val_pearson_correlation_f: 0.8842
Epoch 5/20
166644/166644 [==============================] - 33s 198us/step - loss: 0.1582 - acc: 0.9406 - pearson_correlation_f: 0.9063 - val_loss: 0.2182 - val_acc: 0.9177 - val_pearson_correlation_f: 0.8681
Epoch 6/20
166644/166644 [==============================] - 33s 197us/step - loss: 0.1345 - acc: 0.9507 - pearson_correlation_f: 0.9220 - val_loss: 0.1938 - val_acc: 0.9301 - val_pearson_correlation_f: 0.8865
Epoch 7/20
166644/166644 [==============================] - 33s 196us/step - loss: 0.1165 - acc: 0.9573 - pearson_correlation_f: 0.9331 - val_loss: 0.2098 - val_acc: 0.9263 - val_pearson_correlation_f: 0.8794
Epoch 8/20
166644/166644 [==============================] - 33s 199us/step - loss: 0.1001 - acc: 0.9639 - pearson_correlation_f: 0.9431 - val_loss: 0.2181 - val_acc: 0.9258 - val_pearson_correlation_f: 0.8780
Epoch 9/20
166644/166644 [==============================] - 33s 197us/step - loss: 0.0879 - acc: 0.9685 - pearson_correlation_f: 0.9510 - val_loss: 0.2273 - val_acc: 0.9285 - val_pearson_correlation_f: 0.8817
Epoch 10/20
166644/166644 [==============================] - 33s 197us/step - loss: 0.0791 - acc: 0.9718 - pearson_correlation_f: 0.9560 - val_loss: 0.2353 - val_acc: 0.9270 - val_pearson_correlation_f: 0.8769
Epoch 11/20
166644/166644 [==============================] - 33s 200us/step - loss: 0.0706 - acc: 0.9749 - pearson_correlation_f: 0.9608 - val_loss: 0.2704 - val_acc: 0.9205 - val_pearson_correlation_f: 0.8688
Epoch 12/20
166644/166644 [==============================] - 33s 201us/step - loss: 0.0647 - acc: 0.9773 - pearson_correlation_f: 0.9646 - val_loss: 0.2729 - val_acc: 0.9265 - val_pearson_correlation_f: 0.8751
Epoch 13/20
166644/166644 [==============================] - 33s 201us/step - loss: 0.0595 - acc: 0.9795 - pearson_correlation_f: 0.9679 - val_loss: 0.2637 - val_acc: 0.9294 - val_pearson_correlation_f: 0.8800
Epoch 14/20
166644/166644 [==============================] - 34s 202us/step - loss: 0.0548 - acc: 0.9805 - pearson_correlation_f: 0.9700 - val_loss: 0.3394 - val_acc: 0.9199 - val_pearson_correlation_f: 0.8589
Epoch 15/20
166644/166644 [==============================] - 34s 202us/step - loss: 0.0508 - acc: 0.9823 - pearson_correlation_f: 0.9726 - val_loss: 0.3387 - val_acc: 0.9257 - val_pearson_correlation_f: 0.8704
Epoch 16/20
166644/166644 [==============================] - 34s 203us/step - loss: 0.0469 - acc: 0.9837 - pearson_correlation_f: 0.9748 - val_loss: 0.3223 - val_acc: 0.9310 - val_pearson_correlation_f: 0.8793
Epoch 17/20
166644/166644 [==============================] - 33s 201us/step - loss: 0.0447 - acc: 0.9847 - pearson_correlation_f: 0.9763 - val_loss: 0.3317 - val_acc: 0.9254 - val_pearson_correlation_f: 0.8701
Epoch 18/20
166644/166644 [==============================] - 34s 203us/step - loss: 0.0434 - acc: 0.9853 - pearson_correlation_f: 0.9770 - val_loss: 0.3654 - val_acc: 0.9297 - val_pearson_correlation_f: 0.8759
Epoch 19/20
166644/166644 [==============================] - 34s 205us/step - loss: 0.0405 - acc: 0.9864 - pearson_correlation_f: 0.9787 - val_loss: 0.3573 - val_acc: 0.9257 - val_pearson_correlation_f: 0.8689
Epoch 20/20
166644/166644 [==============================] - 33s 200us/step - loss: 0.0401 - acc: 0.9865 - pearson_correlation_f: 0.9790 - val_loss: 0.3736 - val_acc: 0.9282 - val_pearson_correlation_f: 0.8710
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          20000100  
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 20,244,965
Trainable params: 244,865
Non-trainable params: 20,000,100
_________________________________________________________________
* <2018-12-04 Tue> negative sampling, 10,000 samples, 1 negative sample, binary gloe

In [3]: keys = load_story_keys(10000)

In [4]: len(keys)
Out[4]: 10000

In [5]: tokenizer = create_tokenizer_by_key(keys)

In [6]: fake_summaries = load_negative_sampling_data(keys)

In [7]: fake_summaries = fake_summaries[:,:1]

In [8]: fake_summaries.shape
Out[8]: (10000, 1)

In [9]: articles, reference_summaries = load_article_and_summary_data(keys)
reference_labels = np.ones_like(reference_summaries, dtype=int)
fake_labels = np.zeros_like(fake_summaries, dtype=int)

In [10]: articles.shape

In [11]: 
In [12]: Out[12]: (10000,)

In [13]: reference_summaries.shape
Out[13]: (10000,)

In [14]: fake_summaries.shape
Out[14]: (10000, 1)

In [15]: fake_labels.shape
Out[15]: (10000, 1)

In [16]: res = concatenate_data(articles, reference_summaries,
    ...:                        reference_labels,
    ...:                        fake_summaries,
    ...:                        fake_labels)

In [17]: articles, summaries, labels = res

In [18]: articles.shape
Out[18]: (20000,)

In [19]: summaries.shape
Out[19]: (20000,)

In [20]: labels.shape
Out[20]: (20000,)

In [21]: group = fake_summaries.shape[1] + 1

In [22]: data = prepare_data_using_tokenizer(articles, summaries,
    ...:                                     labels, tokenizer, group=group)
article texts to sequences ..
padding ..
summary texts to sequences ..
padding ..
concatenating ..
shuffling ..
splitting ..

In [23]: (x_train, y_train), (x_val, y_val) = data

In [24]: x_train.shape
Out[24]: (18000, 640)

In [25]: y_train.shape
Out[25]: (18000,)

In [26]: x_val.shape
Out[26]: (2000, 640)

In [27]: y_val.shape
Out[27]: (2000,)

In [28]: embedding_layer = load_embedding(tokenizer)
Indexing word vectors.
Found 400000 word vectors.

In [29]: model = build_binary_glove_model(embedding_layer)

In [30]: plot_model(model, to_file='model.png')
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-30-b950b8cba428> in <module>()
----> 1 plot_model(model, to_file='model.png')

NameError: name 'plot_model' is not defined

In [31]: from keras.utils import plot_model

In [32]: plot_model(model, to_file='model.png')

In [33]: plot_model(model, to_file='model.png', show_shapes=True)

In [34]: train_binary_model(model, data)
Train on 18000 samples, validate on 2000 samples
Epoch 1/20
2018-12-04 17:45:27.562552: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-04 17:45:27.675518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-04 17:45:27.676214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:0b:00.0
totalMemory: 2.95GiB freeMemory: 2.61GiB
2018-12-04 17:45:27.676234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-04 17:45:28.499190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-04 17:45:28.499231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-04 17:45:28.499240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-04 17:45:28.499894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2306 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:0b:00.0, compute capability: 6.1)
17664/18000 [============================>.] - ETA: 0s - loss: 0.6976 - acc: 0.5042 - pearson_correlation_f: 0.02142018-12-04 17:45:33.200661: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.30GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-12-04 17:45:33.218522: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
18000/18000 [==============================] - 6s 325us/step - loss: 0.6975 - acc: 0.5037 - pearson_correlation_f: 0.0215 - val_loss: 0.7040 - val_acc: 0.5000 - val_pearson_correlation_f: 0.1152
Epoch 2/20
18000/18000 [==============================] - 3s 181us/step - loss: 0.6841 - acc: 0.5458 - pearson_correlation_f: 0.1874 - val_loss: 0.6555 - val_acc: 0.5625 - val_pearson_correlation_f: 0.3424
Epoch 3/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.5654 - acc: 0.6969 - pearson_correlation_f: 0.5005 - val_loss: 0.5229 - val_acc: 0.7300 - val_pearson_correlation_f: 0.5461
Epoch 4/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.4955 - acc: 0.7523 - pearson_correlation_f: 0.5908 - val_loss: 0.4995 - val_acc: 0.7565 - val_pearson_correlation_f: 0.5928
Epoch 5/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.4466 - acc: 0.7842 - pearson_correlation_f: 0.6493 - val_loss: 0.5589 - val_acc: 0.7215 - val_pearson_correlation_f: 0.5112
Epoch 6/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.4065 - acc: 0.8126 - pearson_correlation_f: 0.6963 - val_loss: 0.4890 - val_acc: 0.7655 - val_pearson_correlation_f: 0.5967
Epoch 7/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.3492 - acc: 0.8460 - pearson_correlation_f: 0.7556 - val_loss: 0.5010 - val_acc: 0.7605 - val_pearson_correlation_f: 0.6016
Epoch 8/20
18000/18000 [==============================] - 3s 182us/step - loss: 0.2924 - acc: 0.8759 - pearson_correlation_f: 0.8086 - val_loss: 0.5175 - val_acc: 0.7655 - val_pearson_correlation_f: 0.6014
Epoch 9/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.2327 - acc: 0.9085 - pearson_correlation_f: 0.8588 - val_loss: 0.5272 - val_acc: 0.7815 - val_pearson_correlation_f: 0.6192
Epoch 10/20
18000/18000 [==============================] - 3s 184us/step - loss: 0.1768 - acc: 0.9321 - pearson_correlation_f: 0.8979 - val_loss: 0.5610 - val_acc: 0.7690 - val_pearson_correlation_f: 0.6085
Epoch 11/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.1276 - acc: 0.9524 - pearson_correlation_f: 0.9283 - val_loss: 0.6628 - val_acc: 0.7825 - val_pearson_correlation_f: 0.6148
Epoch 12/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.0994 - acc: 0.9646 - pearson_correlation_f: 0.9466 - val_loss: 0.7528 - val_acc: 0.7735 - val_pearson_correlation_f: 0.5993
Epoch 13/20
18000/18000 [==============================] - 3s 183us/step - loss: 0.0845 - acc: 0.9708 - pearson_correlation_f: 0.9559 - val_loss: 0.7209 - val_acc: 0.7855 - val_pearson_correlation_f: 0.6192
Epoch 14/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0685 - acc: 0.9748 - pearson_correlation_f: 0.9635 - val_loss: 0.7640 - val_acc: 0.7885 - val_pearson_correlation_f: 0.6106
Epoch 15/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0600 - acc: 0.9808 - pearson_correlation_f: 0.9693 - val_loss: 0.8997 - val_acc: 0.7865 - val_pearson_correlation_f: 0.6165
Epoch 16/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0540 - acc: 0.9815 - pearson_correlation_f: 0.9720 - val_loss: 0.8670 - val_acc: 0.8040 - val_pearson_correlation_f: 0.6261
Epoch 17/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0505 - acc: 0.9837 - pearson_correlation_f: 0.9754 - val_loss: 0.9137 - val_acc: 0.7655 - val_pearson_correlation_f: 0.5824
Epoch 18/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0448 - acc: 0.9856 - pearson_correlation_f: 0.9789 - val_loss: 1.1742 - val_acc: 0.7470 - val_pearson_correlation_f: 0.5403
Epoch 19/20
18000/18000 [==============================] - 3s 185us/step - loss: 0.0387 - acc: 0.9872 - pearson_correlation_f: 0.9804 - val_loss: 1.2460 - val_acc: 0.7915 - val_pearson_correlation_f: 0.6307
Epoch 20/20
18000/18000 [==============================] - 3s 187us/step - loss: 0.0414 - acc: 0.9872 - pearson_correlation_f: 0.9802 - val_loss: 1.0004 - val_acc: 0.8010 - val_pearson_correlation_f: 0.6299
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 640)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 640, 100)          9005000   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 636, 128)          64128     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 127, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 123, 128)          82048     
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 24, 128)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 20, 128)           82048     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               16512     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129       
=================================================================
Total params: 9,249,865
Trainable params: 244,865
Non-trainable params: 9,005,000
_________________________________________________________________
