#+TITLE: Dataset Design Doc
* TODOs

** TODO test tf.data.Dataset shuffle performance for tfrec data
** TODO integrate the sentence embedding into the model?
** TODO integrate the negative sampling into the model?
** TODO integrate the mutation into the model?
** TODO integrate the negative sampling into the batch generation?

* TODO sequential embedding data serialization
with index file probably

The old way is to use dictionary as data structure for storing the
data files, including stories, mutations, and sentence embedding
results. The hash of the story (the filename) is used as the key. 
1. It is easy for referencing the same stories
2. Easy to track which stories are processed. This is very useful when
   my machine cannot process everything at once.
3. The order of the stories need not be matched among stories,
   mutations, and different embedding schema. In case some stories
   cannot be processed, this is very helpful.


However, the downside is also obvious.
1. I have to load everything to know what are the processed stories
2. Since it is dict, I cannot really make the serialization serial.

I need to use a list for this, and use TFRecord to load the
files. Hopefully this method will read the data on-demand, and close
it on-time so that I don't enter memory issues. The dataset is huge.

Thus I need to pay special attention to the order of the stories to be
exactly the same.

** Format of TFREC
Each file would have the following structure:
story:
#+BEGIN_EXAMPLE
'key': _bytes_feature(key.encode('utf-8')),
'article': _bytes_feature(article.encode('utf-8')),
'summary': _bytes_feature(summary.encode('utf-8'))
#+END_EXAMPLE

where each field:
- key: bytes string
- article: bytes string
- summary: bytes string

sentence embedding:
#+BEGIN_EXAMPLE
'key': _bytes_feature(key),
'article': _bytes_feature(pickle.dumps(article)),
'summary': _bytes_feature(pickle.dumps(summary))
#+END_EXAMPLE

where each article and summary is:
- article: pickle.dumps(numpy of shape (#sent, 512/4096))
- article: pickle.dumps(numpy of shape (#sent, 512/4096))

(HEBI: UPDATE) I'm not going to use tf.data.TFRecordDateset because:
1. Tf 2.0 removed iterator, thus force the user of dataset API to use
   eager evaluation. I'm against it.
2. The dataset API can only serialize bytes, int, float. It cannot
   serialize numpy array directly. To serialize it, I have to use
   pickle to dump to a string, convert to byte string, and embed into
   the tfrec data. That is not the worst part. The worst is, when
   reading it back, I have to get the byte string (which incur an
   evaluation of the tensor), and then load back using pickle. I
   cannot simply do a mapping of the dataset. This loses pretty much
   all the attractions of it.
3. Although the TFRecord dataset can be read from hard disk quite
   nicely, the full dataset shuffle still takes up all memory and
   freeze the machine.
4. I cannot get any information about the dataset, including its length.

In a word, the encoding and decoding of protobuffer is really painful,
and it does not work well with numpy array. Thus I would just use
pickle and manipulate it as needed.

(OR: Can I reshape it?)


** Story shuffle and splitting

The first step is to read the stories, shuffle them, and split them
into training, testing, validating. It must be done first, because I
don't want any potential overlapping impact among the training and
testing set.

#+BEGIN_EXAMPLE
proto/story/train/1.tfrec
proto/story/validate/1.tfrec
proto/story/test/1.tfrec
#+END_EXAMPLE

(HEBI: TODO) I will probably need to make sure the shuffle API of
tf.data.Dataset works for tfrec data. But this is not that important,
as the stories can be hold in memory, and I only need to shuffle the
stories.

** Sentence Embedding

File System Hierarchy. I would really want to maintain the file
correspondence of splits. I'm testing 1,000 stories per file here, but
eventually I will use 10,000.

Note: InferSent embedding is significantly larger (10x).

#+BEGIN_EXAMPLE
proto/USE/train/1.tfrec
proto/USE-Large/validate/1.tfrec
proto/InferSent/test/1.tfrec
#+END_EXAMPLE

** negative sampling

I basically have three approaches to generate negative samples:
1. pre-generate all negative samples. This is also the previous
   approach. However this approach has significant drawbacks:
   - It is not scalable.
   - The negative samples are fixed across epochs. This is not
     general enough.
2. generate negative samples when providing the batch. This is easy to
   implement, and if the entire dataset is shuffled every epochs, it
   can achieve pretty good generalization.
3. In the model, or in the loss function. This is what
   =tf.nn.nce_loss= is doing for word2vec.  However, =tf.nn.nce_loss=
   is not suitable either, it is special for processing word. As far
   as I can see, it expects the words to be indexed, and a vocabulary
   size (say 50000) is give, and sampling is done in the range of
   [0,50000]. This cannot be used for sentence.

** TODO Mutation
This is tricky. I would mutate the article and summary on text
level. After that, I would need to ...



mutation.pickle (I'm going to drop this because I need to perform
separate sentence embedding for it.)

| key          | add          | add label     | delete & label | replace & label |
|--------------+--------------+---------------+----------------+-----------------|
| 0001d1afc246 | [10] of text | [10] of float | [10]           | [10]            |
| ...          |              |               |                |                 |

negative.pickle (I probably don't need this at all.)

| key | neg |
|-----+-----|
|     |     |

** Data Consuming

1. load data depending on the task
   - story (100,000)
   - USE/Large embedding
   - InferSent embedding
   Each contains three fields:
   - key
   - article
   - summary
2. shuffle the entire dataset for each epoch (HEBI: test the performance)
2. fetch 10 batches (100 each):
   - pair each batch with other 9 batches as negative samples (10x9)
   - plus each batch, we got (10x9+10) batch size data
   - concatenate together as one batch
3. train on batch, and continue

** NEW data consuming from pickle files
Create the model first.

1. load all data files
2. divide into training, testing, validating
3. For each epoch:
   1. shuffle training data files
   2. read file_batch_size number of files, concatenate data together
   3. shuffle the data
   4. loop through each of them, and create 5 negative samples for
      each data point
   5. feed into the model

** TODO Experiments
- [X] different neg_size
- [ ] stemming (simpler) or lemmatization
- [ ] create more challenging negative samples
- [ ] debug InferSent and glove models
- [X] fixed negative sampling (by setting random seed for each epoch)
- [ ] DailyMail data
- [ ] word embedding vs. sentence embedding
- [ ] DUC/TAC
- [ ] NYC
- [ ] Transfer
- [ ] Compare with "[1]"
- [ ] Compare with ROUGE
- [ ] evaluation summarization techniques using our brand-new score

** TODO Implementation
- [ ] add command line interface

** TAC/DUC



Filesystem hierarchy:
#+BEGIN_EXAMPLE
DUC_OUT/baseline
DUC_OUT/manual
DUC_OUT/system
#+END_EXAMPLE

Or store the documents/abs by a unique ID:
#+BEGIN_EXAMPLE
DUC_OUT/texts/xxxx.txt
#+END_EXAMPLE

The ID is:
#+BEGIN_EXAMPLE
docID
absID = docID + absID
#+END_EXAMPLE


(HEBI: TODO) I just need to make sure that the docID is unique.

Since the DUC data are split into sentence already, I'll add an empty
line between each sentences.


The TAC/DUC data need to use pairwise ranking comparison.

1. directly uses TAC data to train a regression model (Pearson 0.53, Spearman 0.5)
2. directly uses TAC manual written summaries to do negative sampling
3. use classification model trained on CNN/DM/NYC
   1. directly use and do the regression. compare the ranking
      performance. the activation value after sigmoid is used as the
      probability (remove the classification layer)
   2. fine tune the comparison layer:
      - change the input to accept two pairs of (article, summary)
      - the network structure and weight are shared
      - use a pairwise loss function: the difference between the two
   3. retrain the comparison layer
      1. remove the classification layer
      2. retrain a regression model
      3. the loss function should be a regular regression 








* Email

** Experiment Design

First and foremost, I'm adding the following three comparisons:
1. with ROUGE score as baseline
2. with human judgement
3. with [1], which is kind of the manual feature engineering counterpart
of our approach

We will use DUC/TAC data for it, as it has ground truth (human judgement label).

However, there are two potential problems of using DUC and TAC data:

1. The DUC/TA tasks are for extractive summarization, which works well
with ROUGE. There seems to be no such human data for extractive
summarization. Conducting human study ourselves would seem to be too
expensive for now.

2. This dataset is much smaller (thousands of samples each year), because
the summarizations are manually assessed. This may not be sufficient for
a deep learning based training.

To handle the small data set size problem, there's actually an
interesting transfer-learning experiment: training on CNN/DM and NYT
dataset and fine-tune the model on DUC/TAC.

Finally, in terms of other datasets, previous experiment used 30000
stories from CNN/DM. That's 1/10 of the total data. I can use all of
them. I also have New York time corpus in hand, thus adding that as
well.

I would prioritize the experiments as follows, with descending importance:
1. comparison with ROUGE, Human Judgement, and [1]
2. transfer-learning experiment
3. All CNN/DM data and NYT corpus
4. human survey for collecting abstractive summarization assessment data

Let me know if you have any suggestions.


[1] Louis, Annie, and Ani Nenkova. "Automatically assessing machine
summary content without a gold standard." Computational Linguistics 39.2
(2013): 267-300.

** Directly use embedding coverage

I think the reviewers' comments and your suggestion of improvements
makes a lot of sense to me.

But at another direction, I am also very interested in if we can also
use some unsupervised method to measure the summary quality. Say, the
goal of a summary is to preserve the semantic information from a
document. Can we use the sentence embedding from document and sentence
embeddings from abstract, to measure the semantic coverage. (PS. ROUGE
is basiclly a word coverage)

If we can propose two approaches, one supervised method and one
unsupervised, I think the story is complete. what do you think?

On Sat, Feb 23, 2019 at 2:25 AM yinfei yang <yangyin7@gmail.com> wrote: 

 Can we use the sentence embedding from document and sentence
 embeddings from abstract, to measure the semantic
 coverage. (PS. ROUGE is basiclly a word coverage)

Isn't this what we are doing in the paper now? And to measure the
quality (in terms of semantic coverage) of the summary, we use the
ratio of mutation and negative sampling.

I meant to use the unsupervised approach, without any training. 

I remember our current approach includes a training procedure, do we ?
I may have a wrong memory.

Yes, we do have a training procedure. Current approach basically uses
sentence embedding as pretraining, and train a discriminative model on
top.

That's a good idea. I'll do an unsupervised coverage test on the
embedding directly.



* Appendix
DUC2002 result table:

#+BEGIN_EXAMPLE
Document set number (Dnnn) (HEBI: We care only abstracts which are evaluated manually, extracts are evaluated by machine)
|    Summary type (M = multi-doc, P= single-doc) (HEBI: I'm going to use only "P" for single document.)
|    | Base TREC document id (HEBI: The ID here should points to the Document. ****)
|    | |               Summary target size (10,50,100,200) (HEBI: this is used in the filename)
|    | |               |    Peer size (whitespace-delimited tokens)
|    | |               |    |  Document selector code (A-J)
|    | |               |    |  |
|    | |               |    |  |   Model summarizer code (A-J) (HEBI: this is the person who created the model summary)
|    | |               |    |  |   | Assessor code (A-C,E-J) (HEBI: this is typically the same person as above)
|    | |               |    |  |   | |  Peer summarizer code (baseline[1-3], manual[A-J]submission, or system submission[15-31])
(HEBI: Summary. baseline 1 [no 2 and 3] is the first 100 words selected from article. What is manual submission? We care about 15-31. ****)
|    | |               |    |  |   | |  |      Count of quality questions with non-0 answers  (HEBI: these count questions are not used) 
|    | |               |    |  |   | |  |      |  ######### 12 peer quality questions ask for counts of ERRORS 
|    | |               |    |  |   | |  |      |  Q1        (0 = 0, 1 = 1-5, 2 = 6-10, 3 = 11 or more)
|    | |               |    |  |   | |  |      |  | Q2      (No questions asked on 10-word summaries)
|    | |               |    |  |   | |  |      |  | | Q3
|    | |               |    |  |   | |  |      |  | | | Q4
|    | |               |    |  |   | |  |      |  | | | | Q5
|    | |               |    |  |   | |  |      |  | | | | |   Q6
|    | |               |    |  |   | |  |      |  | | | | |   | Q7
|    | |               |    |  |   | |  |      |  | | | | |   | | Q8 
|    | |               |    |  |   | |  |      |  | | | | |   | | | Q9
|    | |               |    |  |   | |  |      |  | | | | |   | | | | Q10
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | Q11
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | Q12
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   Fraction of unmarked peer units at least related to the model's subject
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        Number of peer units 
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   Number of marked peer units
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   Number of unmarked peer units
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   Number of model units
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   Mean coverage (HEBI: I'm going to use this score ****)
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     Median coverage
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     |     Sample standard deviation of coverage scores
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     |     |       Mean length-adjusted coverage
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     |     |       |     Median length-adjusted coverage
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     |     |       |     |     Sample standard deviation of adjusted coverage scores
|    | |               |    |  |   | |  |      |  | | | | |   | | | | | | |   |        |   |   |   |   |     |     |       |     |     |

D061 M --------------- 010   9 J   I I  16     0  - - - - -   - - - - - - -   0.00     1   1   0   1   0.600 0.600 0.000   0.433 0.433 0.000  
D061 M --------------- 010  10 J   I I  19     0  - - - - -   - - - - - - -   0.00     1   1   0   1   0.600 0.600 0.000   0.400 0.400 0.000  
D061 M --------------- 010   3 J   I I  20     0  - - - - -   - - - - - - -   1.00     1   0   1   1   0.000 0.000 0.000   0.233 0.233 0.000  
D061 M --------------- 200 161 J   I I  29     3  0 0 0 0 0   0 1 0 0 0 1 2   1.00     7   5   2  25   0.136 0.000 0.269   0.156 0.065 0.179  
D061 M --------------- 200 171 J   I I   3     2  0 0 0 0 0   0 0 0 1 0 1 0   0.00     6   6   0  25   0.440 0.400 0.432   0.342 0.315 0.288  
D061 M --------------- 200 204 J   I I   B     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00    10   9   1  25   0.656 1.000 0.414   0.437 0.667 0.276  
D061 P   AP880911-0016 100 100 J   I I   1     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     5   3   2  10   0.340 0.100 0.462   0.227 0.067 0.308  
D061 P   AP880912-0095 100  92 J   I I   1     2  0 0 0 0 0   0 0 1 0 0 1 0   1.00     6   4   2   9   0.400 0.000 0.480   0.293 0.027 0.320  
D061 P   AP880912-0137 100 104 J   I I   1     1  0 0 0 0 0   0 0 0 0 0 1 0   1.00     5   4   1   8   0.350 0.000 0.487   0.233 0.000 0.325  
D061 P   AP880915-0003 100  92 J   I I   1     0  0 0 0 0 0   0 0 0 0 0 0 0   0.00     5   5   0  10   0.500 0.500 0.435   0.360 0.360 0.290  
D061 P  WSJ880912-0064 100 100 J   I I   1     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     5   4   1  11   0.455 0.000 0.522   0.303 0.000 0.348  
D061 P   AP880911-0016 100 101 J   I I  15     1  0 0 0 0 0   0 0 1 0 0 0 0   0.00     5   5   0  10   0.700 1.000 0.424   0.467 0.667 0.283  
D061 P   AP880912-0095 100 102 J   I I  15     0  0 0 0 0 0   0 0 0 0 0 0 0   0.00     3   3   0   9   0.267 0.000 0.412   0.178 0.000 0.275  
D061 P   AP880912-0137 100  95 J   I I  15     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     4   3   1   8   0.400 0.300 0.441   0.283 0.217 0.294  
D061 P   AP880915-0003 100 111 J   I I  15     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     4   2   2  10   0.340 0.200 0.401   0.227 0.133 0.267  
D061 P  WSJ880912-0064 100  97 J   I I  15     2  0 0 0 0 0   0 0 0 0 0 1 1   1.00     5   4   1  11   0.400 0.000 0.490   0.277 0.010 0.327  
D061 P   AP880911-0016 100  96 J   I I  16     0  0 0 0 0 0   0 0 0 0 0 0 0   0.00     3   3   0  10   0.460 0.400 0.490   0.320 0.280 0.327  
D061 P   AP880912-0095 100  95 J   I I  16     2  1 0 0 0 1   0 0 0 0 0 0 0   0.00     3   3   0   9   0.178 0.000 0.273   0.135 0.017 0.182  
D061 P   AP880912-0137 100  98 J   I I  16     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     3   2   1   8   0.300 0.000 0.414   0.207 0.007 0.276  
D061 P   AP880915-0003 100  93 J   I I  16     0  0 0 0 0 0   0 0 0 0 0 0 0   1.00     3   1   2  10   0.240 0.000 0.350   0.183 0.023 0.233  
D061 P  WSJ880912-0064 100  98 J   I I  16     1  1 0 0 0 0   0 0 0 0 0 0 0   0.00     3   3   0  11   0.255 0.000 0.380   0.176 0.007 0.254  
#+END_EXAMPLE
